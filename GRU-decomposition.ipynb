{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5075eb04",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8551110e7f3394cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T23:54:45.842905500Z",
     "start_time": "2024-10-01T23:54:41.703643600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "directory_path = os.getcwd() + \"\\\\Data\\\\sorting_event_volumes_2023.csv\"\n",
    "\n",
    "df = pd.read_csv(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9f920",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c121f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T23:54:49.277348400Z",
     "start_time": "2024-10-01T23:54:45.844905600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows original dataset is: 8949721\n",
      "Number of rows cleaned dataset is: 188628\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "print(\"Number of rows original dataset is: \" + str(df.shape[0]))\n",
    "\n",
    "df = df.loc[df[\"event_type\"] == \"LAJ\", :]\n",
    "df.drop(['event_location', 'input_belt', 'position'], axis=1, inplace = True)\n",
    "df.dropna(inplace = True)\n",
    "df['output_belt'] = df['output_belt'].astype(int)\n",
    "df = df.groupby(['sorting_center_name', 'scanning_date', 'output_belt'], as_index = False)['no_of_events'].sum()\n",
    "df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "\n",
    "print(\"Number of rows cleaned dataset is: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579550c6",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21528160",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T23:54:49.327560700Z",
     "start_time": "2024-10-01T23:54:49.271282100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VANTAA       44006\n",
       "TAMPERE      41481\n",
       "LIETO        35434\n",
       "OULU         31037\n",
       "KUOPIO       27888\n",
       "SEINÄJOKI     8782\n",
       "Name: sorting_center_name, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preparation\n",
    "df['day'] = df['scanning_date'].dt.day\n",
    "df['month'] = df['scanning_date'].dt.month\n",
    "df['weekday'] = df['scanning_date'].dt.day_of_week + 1\n",
    "df['week'] = df['scanning_date'].dt.day_of_year // 7 + 1\n",
    "df['yearday'] = df['scanning_date'].dt.day_of_year\n",
    "df['yearday_sin'] = np.sin(df['yearday'] / 7 * 2 * np.pi)\n",
    "df['yearday_cos'] = np.cos(df['yearday'] / 7 * 2 * np.pi)\n",
    "\n",
    "sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "df[\"sorting_center_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3077866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T23:54:49.364963400Z",
     "start_time": "2024-10-01T23:54:49.322044600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scanning_date</th>\n",
       "      <th>sorting_center_name</th>\n",
       "      <th>no_of_events</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>week</th>\n",
       "      <th>yearday</th>\n",
       "      <th>yearday_sin</th>\n",
       "      <th>yearday_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>LIETO</td>\n",
       "      <td>3650</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>OULU</td>\n",
       "      <td>1441</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>TAMPERE</td>\n",
       "      <td>1458</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>KUOPIO</td>\n",
       "      <td>23812</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9.749279e-01</td>\n",
       "      <td>-0.222521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>LIETO</td>\n",
       "      <td>44598</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9.749279e-01</td>\n",
       "      <td>-0.222521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>OULU</td>\n",
       "      <td>19831</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>363</td>\n",
       "      <td>-7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>SEINÄJOKI</td>\n",
       "      <td>15100</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>363</td>\n",
       "      <td>-7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>TAMPERE</td>\n",
       "      <td>34499</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>363</td>\n",
       "      <td>-7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>VANTAA</td>\n",
       "      <td>129349</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>363</td>\n",
       "      <td>-7.818315e-01</td>\n",
       "      <td>0.623490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>VANTAA</td>\n",
       "      <td>41895</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>364</td>\n",
       "      <td>1.568538e-14</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1881 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     scanning_date sorting_center_name  no_of_events  day  month  weekday  \\\n",
       "0       2023-01-01               LIETO          3650    1      1        7   \n",
       "1       2023-01-01                OULU          1441    1      1        7   \n",
       "2       2023-01-01             TAMPERE          1458    1      1        7   \n",
       "3       2023-01-02              KUOPIO         23812    2      1        1   \n",
       "4       2023-01-02               LIETO         44598    2      1        1   \n",
       "...            ...                 ...           ...  ...    ...      ...   \n",
       "1876    2023-12-29                OULU         19831   29     12        5   \n",
       "1877    2023-12-29           SEINÄJOKI         15100   29     12        5   \n",
       "1878    2023-12-29             TAMPERE         34499   29     12        5   \n",
       "1879    2023-12-29              VANTAA        129349   29     12        5   \n",
       "1880    2023-12-30              VANTAA         41895   30     12        6   \n",
       "\n",
       "      week  yearday   yearday_sin  yearday_cos  \n",
       "0        1        1  7.818315e-01     0.623490  \n",
       "1        1        1  7.818315e-01     0.623490  \n",
       "2        1        1  7.818315e-01     0.623490  \n",
       "3        1        2  9.749279e-01    -0.222521  \n",
       "4        1        2  9.749279e-01    -0.222521  \n",
       "...    ...      ...           ...          ...  \n",
       "1876    52      363 -7.818315e-01     0.623490  \n",
       "1877    52      363 -7.818315e-01     0.623490  \n",
       "1878    52      363 -7.818315e-01     0.623490  \n",
       "1879    52      363 -7.818315e-01     0.623490  \n",
       "1880    53      364  1.568538e-14     1.000000  \n",
       "\n",
       "[1881 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregating data\n",
    "adf = df.groupby(['scanning_date','sorting_center_name'])['no_of_events'].sum().reset_index()\n",
    "adf['day'] = adf['scanning_date'].dt.day\n",
    "adf['month'] = adf['scanning_date'].dt.month\n",
    "adf['weekday'] = adf['scanning_date'].dt.day_of_week + 1\n",
    "adf['week'] = adf['scanning_date'].dt.day_of_year // 7 + 1\n",
    "adf['yearday'] = adf['scanning_date'].dt.day_of_year\n",
    "adf['yearday_sin'] = np.sin(adf['yearday'] / 7 * 2 * np.pi)\n",
    "adf['yearday_cos'] = np.cos(adf['yearday'] / 7 * 2 * np.pi)\n",
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d969e92b37a0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T23:54:54.096711Z",
     "start_time": "2024-10-01T23:54:52.419744800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Device 0: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())  # 查看有多少个 CUDA 设备\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3b51bdd10a04aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T23:54:54.097711Z",
     "start_time": "2024-10-01T23:54:54.085437900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot should be difference between predicted and reality\n",
    "# I'm not sure if scaling on the number of events is good, because you are trying to predict that\n",
    "# Do LSTMs work per row or not?\n",
    "# Split the data by date, don't use random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9979c0f5-c2eb-411f-b3df-a0d9f6398e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In this project, we used Optuna to automatically find the best set of hyperparameters that optimize the performance of our LSTM model. \n",
    "#Hyperparameters such as the hidden size, number of LSTM layers, learning rate, and sequence length have a significant impact on how well the model performs.\n",
    "#Instead of manually tuning these parameters, which is time-consuming and often inefficient, Optuna allows us to automate this process and find the optimal combination.\n",
    "#Optuna works by conducting multiple trials where it suggests different hyperparameter values and evaluates the model's performance based on a target metric—in our case, the root mean square error (RMSE). \n",
    "#The process begins with defining an objective function, which includes training the model using the hyperparameters suggested by Optuna. \n",
    "#After training, the function computes the RMSE on the validation set, which is then passed back to Optuna. Optuna uses this feedback to refine its suggestions, gradually improving its search for the best parameters.\n",
    "#It utilizes a technique called Tree-structured Parzen Estimator (TPE), which is more efficient than traditional methods like grid search or random search because it focuses on the most promising areas of the hyperparameter space. \n",
    "#After running multiple trials, Optuna identifies the best-performing set of hyperparameters, which we can then use to retrain the model for final evaluation. This method saves time, improves model performance, and removes much of the guesswork from manual tuning, making it a powerful tool for optimizing machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd65dcc7-a7f8-4362-8804-78716660fe92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sun Dexin\\AppData\\Local\\Temp\\ipykernel_35220\\1919305335.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.loc[:, feature_columns] = train_df[feature_columns].fillna(train_df[feature_columns].median(numeric_only=True))\n",
      "C:\\Users\\Sun Dexin\\AppData\\Local\\Temp\\ipykernel_35220\\1919305335.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.loc[:, feature_columns] = test_df[feature_columns].fillna(test_df[feature_columns].median(numeric_only=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Center: VANTAA, Epoch [1/50], Loss: 247705.0548\n",
      "Sorting Center: VANTAA, Epoch [2/50], Loss: 170393.1472\n",
      "Sorting Center: VANTAA, Epoch [3/50], Loss: 167241.2654\n",
      "Sorting Center: VANTAA, Epoch [4/50], Loss: 166170.9721\n",
      "Sorting Center: VANTAA, Epoch [5/50], Loss: 165510.0107\n",
      "Sorting Center: VANTAA, Epoch [6/50], Loss: 164100.8055\n",
      "Sorting Center: VANTAA, Epoch [7/50], Loss: 162548.3946\n",
      "Sorting Center: VANTAA, Epoch [8/50], Loss: 161976.5683\n",
      "Sorting Center: VANTAA, Epoch [9/50], Loss: 161027.6242\n",
      "Sorting Center: VANTAA, Epoch [10/50], Loss: 160752.4679\n",
      "Sorting Center: VANTAA, Epoch [11/50], Loss: 159360.5493\n",
      "Sorting Center: VANTAA, Epoch [12/50], Loss: 159761.1351\n",
      "Sorting Center: VANTAA, Epoch [13/50], Loss: 158576.2099\n",
      "Sorting Center: VANTAA, Epoch [14/50], Loss: 157945.4815\n",
      "Sorting Center: VANTAA, Epoch [15/50], Loss: 157428.3375\n",
      "Sorting Center: VANTAA, Epoch [16/50], Loss: 157204.5810\n",
      "Sorting Center: VANTAA, Epoch [17/50], Loss: 156943.7621\n",
      "Sorting Center: VANTAA, Epoch [18/50], Loss: 156558.1463\n",
      "Sorting Center: VANTAA, Epoch [19/50], Loss: 156021.5324\n",
      "Sorting Center: VANTAA, Epoch [20/50], Loss: 155371.6248\n",
      "Sorting Center: VANTAA, Epoch [21/50], Loss: 155266.9399\n",
      "Sorting Center: VANTAA, Epoch [22/50], Loss: 154650.3603\n",
      "Sorting Center: VANTAA, Epoch [23/50], Loss: 154773.1566\n",
      "Sorting Center: VANTAA, Epoch [24/50], Loss: 154262.5752\n",
      "Sorting Center: VANTAA, Epoch [25/50], Loss: 153769.6791\n",
      "Sorting Center: VANTAA, Epoch [26/50], Loss: 153366.2147\n",
      "Sorting Center: VANTAA, Epoch [27/50], Loss: 152962.8574\n",
      "Sorting Center: VANTAA, Epoch [28/50], Loss: 152714.3168\n",
      "Sorting Center: VANTAA, Epoch [29/50], Loss: 152352.1436\n",
      "Sorting Center: VANTAA, Epoch [30/50], Loss: 151922.3356\n",
      "Sorting Center: VANTAA, Epoch [31/50], Loss: 152128.6624\n",
      "Sorting Center: VANTAA, Epoch [32/50], Loss: 151757.5483\n",
      "Sorting Center: VANTAA, Epoch [33/50], Loss: 151284.6461\n",
      "Sorting Center: VANTAA, Epoch [34/50], Loss: 151013.8173\n",
      "Sorting Center: VANTAA, Epoch [35/50], Loss: 151097.4227\n",
      "Sorting Center: VANTAA, Epoch [36/50], Loss: 150608.5566\n",
      "Sorting Center: VANTAA, Epoch [37/50], Loss: 150876.6858\n",
      "Sorting Center: VANTAA, Epoch [38/50], Loss: 150160.4256\n",
      "Sorting Center: VANTAA, Epoch [39/50], Loss: 150059.5596\n",
      "Sorting Center: VANTAA, Epoch [40/50], Loss: 150141.2264\n",
      "Sorting Center: VANTAA, Epoch [41/50], Loss: 150139.0146\n",
      "Sorting Center: VANTAA, Epoch [42/50], Loss: 149732.7220\n",
      "Sorting Center: VANTAA, Epoch [43/50], Loss: 149497.9806\n",
      "Sorting Center: VANTAA, Epoch [44/50], Loss: 149738.3352\n",
      "Sorting Center: VANTAA, Epoch [45/50], Loss: 149397.4305\n",
      "Sorting Center: VANTAA, Epoch [46/50], Loss: 149633.0650\n",
      "Sorting Center: VANTAA, Epoch [47/50], Loss: 149502.8920\n",
      "Sorting Center: VANTAA, Epoch [48/50], Loss: 149391.8626\n",
      "Sorting Center: VANTAA, Epoch [49/50], Loss: 149222.1546\n",
      "Sorting Center: VANTAA, Epoch [50/50], Loss: 149302.0750\n",
      "Sorting Center: OULU, Epoch [1/50], Loss: 93941.9226\n",
      "Sorting Center: OULU, Epoch [2/50], Loss: 86906.0175\n",
      "Sorting Center: OULU, Epoch [3/50], Loss: 84779.5604\n",
      "Sorting Center: OULU, Epoch [4/50], Loss: 84089.4753\n",
      "Sorting Center: OULU, Epoch [5/50], Loss: 82972.0826\n",
      "Sorting Center: OULU, Epoch [6/50], Loss: 82099.2365\n",
      "Sorting Center: OULU, Epoch [7/50], Loss: 81020.4446\n",
      "Sorting Center: OULU, Epoch [8/50], Loss: 80475.5019\n",
      "Sorting Center: OULU, Epoch [9/50], Loss: 80046.8594\n",
      "Sorting Center: OULU, Epoch [10/50], Loss: 79462.4065\n",
      "Sorting Center: OULU, Epoch [11/50], Loss: 78828.2695\n",
      "Sorting Center: OULU, Epoch [12/50], Loss: 78582.0913\n",
      "Sorting Center: OULU, Epoch [13/50], Loss: 77879.5290\n",
      "Sorting Center: OULU, Epoch [14/50], Loss: 77864.2187\n",
      "Sorting Center: OULU, Epoch [15/50], Loss: 77536.3349\n",
      "Sorting Center: OULU, Epoch [16/50], Loss: 77283.3613\n",
      "Sorting Center: OULU, Epoch [17/50], Loss: 76675.9516\n",
      "Sorting Center: OULU, Epoch [18/50], Loss: 76109.1520\n",
      "Sorting Center: OULU, Epoch [19/50], Loss: 76031.2939\n",
      "Sorting Center: OULU, Epoch [20/50], Loss: 75563.0648\n",
      "Sorting Center: OULU, Epoch [21/50], Loss: 75268.3540\n",
      "Sorting Center: OULU, Epoch [22/50], Loss: 74788.1838\n",
      "Sorting Center: OULU, Epoch [23/50], Loss: 74709.7668\n",
      "Sorting Center: OULU, Epoch [24/50], Loss: 74852.0669\n",
      "Sorting Center: OULU, Epoch [25/50], Loss: 74429.5961\n",
      "Sorting Center: OULU, Epoch [26/50], Loss: 73944.9160\n",
      "Sorting Center: OULU, Epoch [27/50], Loss: 73970.7132\n",
      "Sorting Center: OULU, Epoch [28/50], Loss: 73724.6833\n",
      "Sorting Center: OULU, Epoch [29/50], Loss: 73163.5500\n",
      "Sorting Center: OULU, Epoch [30/50], Loss: 73186.0386\n",
      "Sorting Center: OULU, Epoch [31/50], Loss: 72985.3175\n",
      "Sorting Center: OULU, Epoch [32/50], Loss: 73078.0216\n",
      "Sorting Center: OULU, Epoch [33/50], Loss: 72613.0011\n",
      "Sorting Center: OULU, Epoch [34/50], Loss: 72249.6017\n",
      "Sorting Center: OULU, Epoch [35/50], Loss: 72244.6051\n",
      "Sorting Center: OULU, Epoch [36/50], Loss: 72326.7686\n",
      "Sorting Center: OULU, Epoch [37/50], Loss: 71913.6719\n",
      "Sorting Center: OULU, Epoch [38/50], Loss: 71868.3379\n",
      "Sorting Center: OULU, Epoch [39/50], Loss: 71715.0766\n",
      "Sorting Center: OULU, Epoch [40/50], Loss: 71837.7382\n",
      "Sorting Center: OULU, Epoch [41/50], Loss: 71727.8783\n",
      "Sorting Center: OULU, Epoch [42/50], Loss: 71634.1339\n",
      "Sorting Center: OULU, Epoch [43/50], Loss: 71664.5040\n",
      "Sorting Center: OULU, Epoch [44/50], Loss: 71389.2665\n",
      "Sorting Center: OULU, Epoch [45/50], Loss: 71346.6132\n",
      "Sorting Center: OULU, Epoch [46/50], Loss: 71467.2767\n",
      "Sorting Center: OULU, Epoch [47/50], Loss: 71426.5706\n",
      "Sorting Center: OULU, Epoch [48/50], Loss: 71177.4547\n",
      "Sorting Center: OULU, Epoch [49/50], Loss: 71444.6717\n",
      "Sorting Center: OULU, Epoch [50/50], Loss: 71198.8654\n",
      "Sorting Center: LIETO, Epoch [1/50], Loss: 113321.1726\n",
      "Sorting Center: LIETO, Epoch [2/50], Loss: 77280.7359\n",
      "Sorting Center: LIETO, Epoch [3/50], Loss: 74839.8253\n",
      "Sorting Center: LIETO, Epoch [4/50], Loss: 73614.7827\n",
      "Sorting Center: LIETO, Epoch [5/50], Loss: 71977.0385\n",
      "Sorting Center: LIETO, Epoch [6/50], Loss: 71851.1525\n",
      "Sorting Center: LIETO, Epoch [7/50], Loss: 71349.4484\n",
      "Sorting Center: LIETO, Epoch [8/50], Loss: 70232.7365\n",
      "Sorting Center: LIETO, Epoch [9/50], Loss: 70303.8034\n",
      "Sorting Center: LIETO, Epoch [10/50], Loss: 69529.7488\n",
      "Sorting Center: LIETO, Epoch [11/50], Loss: 68960.1427\n",
      "Sorting Center: LIETO, Epoch [12/50], Loss: 69136.2008\n",
      "Sorting Center: LIETO, Epoch [13/50], Loss: 68488.4952\n",
      "Sorting Center: LIETO, Epoch [14/50], Loss: 68400.8399\n",
      "Sorting Center: LIETO, Epoch [15/50], Loss: 67739.6937\n",
      "Sorting Center: LIETO, Epoch [16/50], Loss: 67807.3631\n",
      "Sorting Center: LIETO, Epoch [17/50], Loss: 67338.7791\n",
      "Sorting Center: LIETO, Epoch [18/50], Loss: 67408.4896\n",
      "Sorting Center: LIETO, Epoch [19/50], Loss: 66774.7921\n",
      "Sorting Center: LIETO, Epoch [20/50], Loss: 66558.1060\n",
      "Sorting Center: LIETO, Epoch [21/50], Loss: 66481.6576\n",
      "Sorting Center: LIETO, Epoch [22/50], Loss: 66469.3497\n",
      "Sorting Center: LIETO, Epoch [23/50], Loss: 65922.1807\n",
      "Sorting Center: LIETO, Epoch [24/50], Loss: 65780.5212\n",
      "Sorting Center: LIETO, Epoch [25/50], Loss: 65789.6500\n",
      "Sorting Center: LIETO, Epoch [26/50], Loss: 65681.9956\n",
      "Sorting Center: LIETO, Epoch [27/50], Loss: 65433.3972\n",
      "Sorting Center: LIETO, Epoch [28/50], Loss: 65275.1757\n",
      "Sorting Center: LIETO, Epoch [29/50], Loss: 64878.0176\n",
      "Sorting Center: LIETO, Epoch [30/50], Loss: 64893.3571\n",
      "Sorting Center: LIETO, Epoch [31/50], Loss: 64784.6026\n",
      "Sorting Center: LIETO, Epoch [32/50], Loss: 64709.7269\n",
      "Sorting Center: LIETO, Epoch [33/50], Loss: 64635.7709\n",
      "Sorting Center: LIETO, Epoch [34/50], Loss: 64633.8982\n",
      "Sorting Center: LIETO, Epoch [35/50], Loss: 64287.0524\n",
      "Sorting Center: LIETO, Epoch [36/50], Loss: 64457.7733\n",
      "Sorting Center: LIETO, Epoch [37/50], Loss: 64143.0786\n",
      "Sorting Center: LIETO, Epoch [38/50], Loss: 64003.6559\n",
      "Sorting Center: LIETO, Epoch [39/50], Loss: 64034.3674\n",
      "Sorting Center: LIETO, Epoch [40/50], Loss: 64027.1728\n",
      "Sorting Center: LIETO, Epoch [41/50], Loss: 63981.1780\n",
      "Sorting Center: LIETO, Epoch [42/50], Loss: 63712.6127\n",
      "Sorting Center: LIETO, Epoch [43/50], Loss: 63827.4181\n",
      "Sorting Center: LIETO, Epoch [44/50], Loss: 63792.7732\n",
      "Sorting Center: LIETO, Epoch [45/50], Loss: 63751.7143\n",
      "Sorting Center: LIETO, Epoch [46/50], Loss: 63732.5782\n",
      "Sorting Center: LIETO, Epoch [47/50], Loss: 63722.9891\n",
      "Sorting Center: LIETO, Epoch [48/50], Loss: 63609.9951\n",
      "Sorting Center: LIETO, Epoch [49/50], Loss: 63599.5363\n",
      "Sorting Center: LIETO, Epoch [50/50], Loss: 63618.6949\n",
      "Sorting Center: TAMPERE, Epoch [1/50], Loss: 1364446.6272\n",
      "Sorting Center: TAMPERE, Epoch [2/50], Loss: 1025177.2215\n",
      "Sorting Center: TAMPERE, Epoch [3/50], Loss: 1006342.6500\n",
      "Sorting Center: TAMPERE, Epoch [4/50], Loss: 992675.6370\n",
      "Sorting Center: TAMPERE, Epoch [5/50], Loss: 979831.1173\n",
      "Sorting Center: TAMPERE, Epoch [6/50], Loss: 957647.9004\n",
      "Sorting Center: TAMPERE, Epoch [7/50], Loss: 952983.9741\n",
      "Sorting Center: TAMPERE, Epoch [8/50], Loss: 940971.0780\n",
      "Sorting Center: TAMPERE, Epoch [9/50], Loss: 935634.7039\n",
      "Sorting Center: TAMPERE, Epoch [10/50], Loss: 922669.4084\n",
      "Sorting Center: TAMPERE, Epoch [11/50], Loss: 916753.9344\n",
      "Sorting Center: TAMPERE, Epoch [12/50], Loss: 911096.6232\n",
      "Sorting Center: TAMPERE, Epoch [13/50], Loss: 899150.5436\n",
      "Sorting Center: TAMPERE, Epoch [14/50], Loss: 887818.0744\n",
      "Sorting Center: TAMPERE, Epoch [15/50], Loss: 880318.2139\n",
      "Sorting Center: TAMPERE, Epoch [16/50], Loss: 886475.0000\n",
      "Sorting Center: TAMPERE, Epoch [17/50], Loss: 861802.0762\n",
      "Sorting Center: TAMPERE, Epoch [18/50], Loss: 865605.8887\n",
      "Sorting Center: TAMPERE, Epoch [19/50], Loss: 847176.8578\n",
      "Sorting Center: TAMPERE, Epoch [20/50], Loss: 839560.5528\n",
      "Sorting Center: TAMPERE, Epoch [21/50], Loss: 827355.7113\n",
      "Sorting Center: TAMPERE, Epoch [22/50], Loss: 824437.1637\n",
      "Sorting Center: TAMPERE, Epoch [23/50], Loss: 809379.4191\n",
      "Sorting Center: TAMPERE, Epoch [24/50], Loss: 809302.2522\n",
      "Sorting Center: TAMPERE, Epoch [25/50], Loss: 802176.0973\n",
      "Sorting Center: TAMPERE, Epoch [26/50], Loss: 802725.6213\n",
      "Sorting Center: TAMPERE, Epoch [27/50], Loss: 786400.1165\n",
      "Sorting Center: TAMPERE, Epoch [28/50], Loss: 788047.7380\n",
      "Sorting Center: TAMPERE, Epoch [29/50], Loss: 779720.1876\n",
      "Sorting Center: TAMPERE, Epoch [30/50], Loss: 786830.7804\n",
      "Sorting Center: TAMPERE, Epoch [31/50], Loss: 753177.6763\n",
      "Sorting Center: TAMPERE, Epoch [32/50], Loss: 763100.8871\n",
      "Sorting Center: TAMPERE, Epoch [33/50], Loss: 745599.3765\n",
      "Sorting Center: TAMPERE, Epoch [34/50], Loss: 750143.2632\n",
      "Sorting Center: TAMPERE, Epoch [35/50], Loss: 745069.1148\n",
      "Sorting Center: TAMPERE, Epoch [36/50], Loss: 738042.3369\n",
      "Sorting Center: TAMPERE, Epoch [37/50], Loss: 731607.5021\n",
      "Sorting Center: TAMPERE, Epoch [38/50], Loss: 738641.3278\n",
      "Sorting Center: TAMPERE, Epoch [39/50], Loss: 726325.1324\n",
      "Sorting Center: TAMPERE, Epoch [40/50], Loss: 726419.1461\n",
      "Sorting Center: TAMPERE, Epoch [41/50], Loss: 720340.9193\n",
      "Sorting Center: TAMPERE, Epoch [42/50], Loss: 711085.6253\n",
      "Sorting Center: TAMPERE, Epoch [43/50], Loss: 722247.4338\n",
      "Sorting Center: TAMPERE, Epoch [44/50], Loss: 720994.1833\n",
      "Sorting Center: TAMPERE, Epoch [45/50], Loss: 712686.5511\n",
      "Sorting Center: TAMPERE, Epoch [46/50], Loss: 718675.9310\n",
      "Sorting Center: TAMPERE, Epoch [47/50], Loss: 710118.3822\n",
      "Sorting Center: TAMPERE, Epoch [48/50], Loss: 730792.3738\n",
      "Sorting Center: TAMPERE, Epoch [49/50], Loss: 715063.9313\n",
      "Sorting Center: TAMPERE, Epoch [50/50], Loss: 705565.3008\n",
      "Sorting Center: SEINÄJOKI, Epoch [1/50], Loss: 142875.2245\n",
      "Sorting Center: SEINÄJOKI, Epoch [2/50], Loss: 123300.6695\n",
      "Sorting Center: SEINÄJOKI, Epoch [3/50], Loss: 119210.5948\n",
      "Sorting Center: SEINÄJOKI, Epoch [4/50], Loss: 117706.4152\n",
      "Sorting Center: SEINÄJOKI, Epoch [5/50], Loss: 116660.9699\n",
      "Sorting Center: SEINÄJOKI, Epoch [6/50], Loss: 115873.9257\n",
      "Sorting Center: SEINÄJOKI, Epoch [7/50], Loss: 115590.2041\n",
      "Sorting Center: SEINÄJOKI, Epoch [8/50], Loss: 114107.7575\n",
      "Sorting Center: SEINÄJOKI, Epoch [9/50], Loss: 113090.6398\n",
      "Sorting Center: SEINÄJOKI, Epoch [10/50], Loss: 112921.2569\n",
      "Sorting Center: SEINÄJOKI, Epoch [11/50], Loss: 111829.5239\n",
      "Sorting Center: SEINÄJOKI, Epoch [12/50], Loss: 111599.1376\n",
      "Sorting Center: SEINÄJOKI, Epoch [13/50], Loss: 110778.7011\n",
      "Sorting Center: SEINÄJOKI, Epoch [14/50], Loss: 110455.2528\n",
      "Sorting Center: SEINÄJOKI, Epoch [15/50], Loss: 109959.8635\n",
      "Sorting Center: SEINÄJOKI, Epoch [16/50], Loss: 109422.5316\n",
      "Sorting Center: SEINÄJOKI, Epoch [17/50], Loss: 109342.6146\n",
      "Sorting Center: SEINÄJOKI, Epoch [18/50], Loss: 108530.3186\n",
      "Sorting Center: SEINÄJOKI, Epoch [19/50], Loss: 107876.3914\n",
      "Sorting Center: SEINÄJOKI, Epoch [20/50], Loss: 107133.6232\n",
      "Sorting Center: SEINÄJOKI, Epoch [21/50], Loss: 107178.1715\n",
      "Sorting Center: SEINÄJOKI, Epoch [22/50], Loss: 106483.4197\n",
      "Sorting Center: SEINÄJOKI, Epoch [23/50], Loss: 106396.6128\n",
      "Sorting Center: SEINÄJOKI, Epoch [24/50], Loss: 106527.6809\n",
      "Sorting Center: SEINÄJOKI, Epoch [25/50], Loss: 105759.4570\n",
      "Sorting Center: SEINÄJOKI, Epoch [26/50], Loss: 105399.4688\n",
      "Sorting Center: SEINÄJOKI, Epoch [27/50], Loss: 104833.2183\n",
      "Sorting Center: SEINÄJOKI, Epoch [28/50], Loss: 104711.6409\n",
      "Sorting Center: SEINÄJOKI, Epoch [29/50], Loss: 104527.9746\n",
      "Sorting Center: SEINÄJOKI, Epoch [30/50], Loss: 103468.2391\n",
      "Sorting Center: SEINÄJOKI, Epoch [31/50], Loss: 103706.4819\n",
      "Sorting Center: SEINÄJOKI, Epoch [32/50], Loss: 103355.4237\n",
      "Sorting Center: SEINÄJOKI, Epoch [33/50], Loss: 102556.2541\n",
      "Sorting Center: SEINÄJOKI, Epoch [34/50], Loss: 102440.4712\n",
      "Sorting Center: SEINÄJOKI, Epoch [35/50], Loss: 102455.3139\n",
      "Sorting Center: SEINÄJOKI, Epoch [36/50], Loss: 102535.6426\n",
      "Sorting Center: SEINÄJOKI, Epoch [37/50], Loss: 101811.2808\n",
      "Sorting Center: SEINÄJOKI, Epoch [38/50], Loss: 102225.2994\n",
      "Sorting Center: SEINÄJOKI, Epoch [39/50], Loss: 101364.8326\n",
      "Sorting Center: SEINÄJOKI, Epoch [40/50], Loss: 101601.8614\n",
      "Sorting Center: SEINÄJOKI, Epoch [41/50], Loss: 101813.3497\n",
      "Sorting Center: SEINÄJOKI, Epoch [42/50], Loss: 101488.5291\n",
      "Sorting Center: SEINÄJOKI, Epoch [43/50], Loss: 101119.3187\n",
      "Sorting Center: SEINÄJOKI, Epoch [44/50], Loss: 101185.5721\n",
      "Sorting Center: SEINÄJOKI, Epoch [45/50], Loss: 101098.9958\n",
      "Sorting Center: SEINÄJOKI, Epoch [46/50], Loss: 101003.8042\n",
      "Sorting Center: SEINÄJOKI, Epoch [47/50], Loss: 101111.9476\n",
      "Sorting Center: SEINÄJOKI, Epoch [48/50], Loss: 101046.7875\n",
      "Sorting Center: SEINÄJOKI, Epoch [49/50], Loss: 100714.6518\n",
      "Sorting Center: SEINÄJOKI, Epoch [50/50], Loss: 100998.9718\n",
      "Sorting Center: KUOPIO, Epoch [1/50], Loss: 151595.3356\n",
      "Sorting Center: KUOPIO, Epoch [2/50], Loss: 76333.7007\n",
      "Sorting Center: KUOPIO, Epoch [3/50], Loss: 73350.6334\n",
      "Sorting Center: KUOPIO, Epoch [4/50], Loss: 72036.4990\n",
      "Sorting Center: KUOPIO, Epoch [5/50], Loss: 71423.1383\n",
      "Sorting Center: KUOPIO, Epoch [6/50], Loss: 70391.1431\n",
      "Sorting Center: KUOPIO, Epoch [7/50], Loss: 69991.0205\n",
      "Sorting Center: KUOPIO, Epoch [8/50], Loss: 69425.1780\n",
      "Sorting Center: KUOPIO, Epoch [9/50], Loss: 68675.4493\n",
      "Sorting Center: KUOPIO, Epoch [10/50], Loss: 68363.7482\n",
      "Sorting Center: KUOPIO, Epoch [11/50], Loss: 67750.1642\n",
      "Sorting Center: KUOPIO, Epoch [12/50], Loss: 67611.3669\n",
      "Sorting Center: KUOPIO, Epoch [13/50], Loss: 66971.3733\n",
      "Sorting Center: KUOPIO, Epoch [14/50], Loss: 66723.0294\n",
      "Sorting Center: KUOPIO, Epoch [15/50], Loss: 66194.0200\n",
      "Sorting Center: KUOPIO, Epoch [16/50], Loss: 65878.3057\n",
      "Sorting Center: KUOPIO, Epoch [17/50], Loss: 65633.0058\n",
      "Sorting Center: KUOPIO, Epoch [18/50], Loss: 65355.3456\n",
      "Sorting Center: KUOPIO, Epoch [19/50], Loss: 65221.8116\n",
      "Sorting Center: KUOPIO, Epoch [20/50], Loss: 64808.9606\n",
      "Sorting Center: KUOPIO, Epoch [21/50], Loss: 64553.1635\n",
      "Sorting Center: KUOPIO, Epoch [22/50], Loss: 64547.1855\n",
      "Sorting Center: KUOPIO, Epoch [23/50], Loss: 64192.5752\n",
      "Sorting Center: KUOPIO, Epoch [24/50], Loss: 63848.4845\n",
      "Sorting Center: KUOPIO, Epoch [25/50], Loss: 63964.2926\n",
      "Sorting Center: KUOPIO, Epoch [26/50], Loss: 63491.8120\n",
      "Sorting Center: KUOPIO, Epoch [27/50], Loss: 63267.7745\n",
      "Sorting Center: KUOPIO, Epoch [28/50], Loss: 63123.9087\n",
      "Sorting Center: KUOPIO, Epoch [29/50], Loss: 63018.7743\n",
      "Sorting Center: KUOPIO, Epoch [30/50], Loss: 63075.5335\n",
      "Sorting Center: KUOPIO, Epoch [31/50], Loss: 62824.0401\n",
      "Sorting Center: KUOPIO, Epoch [32/50], Loss: 62396.3706\n",
      "Sorting Center: KUOPIO, Epoch [33/50], Loss: 62398.1004\n",
      "Sorting Center: KUOPIO, Epoch [34/50], Loss: 62067.5066\n",
      "Sorting Center: KUOPIO, Epoch [35/50], Loss: 61877.3294\n",
      "Sorting Center: KUOPIO, Epoch [36/50], Loss: 61836.1396\n",
      "Sorting Center: KUOPIO, Epoch [37/50], Loss: 61813.8052\n",
      "Sorting Center: KUOPIO, Epoch [38/50], Loss: 61691.2085\n",
      "Sorting Center: KUOPIO, Epoch [39/50], Loss: 61512.2004\n",
      "Sorting Center: KUOPIO, Epoch [40/50], Loss: 61638.0948\n",
      "Sorting Center: KUOPIO, Epoch [41/50], Loss: 61512.0353\n",
      "Sorting Center: KUOPIO, Epoch [42/50], Loss: 61233.9189\n",
      "Sorting Center: KUOPIO, Epoch [43/50], Loss: 61500.5345\n",
      "Sorting Center: KUOPIO, Epoch [44/50], Loss: 61180.6571\n",
      "Sorting Center: KUOPIO, Epoch [45/50], Loss: 61138.1297\n",
      "Sorting Center: KUOPIO, Epoch [46/50], Loss: 61343.0349\n",
      "Sorting Center: KUOPIO, Epoch [47/50], Loss: 61379.8692\n",
      "Sorting Center: KUOPIO, Epoch [48/50], Loss: 61308.7947\n",
      "Sorting Center: KUOPIO, Epoch [49/50], Loss: 60949.0014\n",
      "Sorting Center: KUOPIO, Epoch [50/50], Loss: 61220.0294\n",
      "\n",
      "Overall Metrics for Each Sorting Center:\n",
      "Sorting Center VANTAA: MSE = 366589.0845, VSE = 129374801608.4995\n",
      "Sorting Center OULU: MSE = 62087.9348, VSE = 894128918.6466\n",
      "Sorting Center LIETO: MSE = 52131.9382, VSE = 998716621.1026\n",
      "Sorting Center TAMPERE: MSE = 435776.1865, VSE = 221915038199.2953\n",
      "Sorting Center SEINÄJOKI: MSE = 79329.6572, VSE = 2139603136.8194\n",
      "Sorting Center KUOPIO: MSE = 61778.2659, VSE = 559503776.8282\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import pywt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Define hyperparameters\n",
    "sequence_length = 15\n",
    "num_blocks = 3\n",
    "hidden_channels = 32\n",
    "kernel_size = 3\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Create GRU input sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        Y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Wavelet-based decomposition function (GPU-friendly version)\n",
    "def decompose_signal_wavelet(signal, wavelet='db1', level=None):\n",
    "    if level is None:\n",
    "        level = min(3, int(np.log2(len(signal))))\n",
    "    coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "    coeffs = [torch.tensor(c, dtype=torch.float32).to(device) for c in coeffs]\n",
    "    return coeffs\n",
    "\n",
    "# ResNet-like block for time series\n",
    "class ResNetBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dropout_rate=0.3):\n",
    "        super(ResNetBlock1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.skip is not None:\n",
    "            identity = self.skip(identity)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Full ResNet model for time series\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, input_channels, num_blocks, hidden_channels, kernel_size, output_size):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(ResNetBlock1D(in_channels, hidden_channels, kernel_size))\n",
    "            in_channels = hidden_channels\n",
    "        self.resnet_blocks = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(hidden_channels, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.resnet_blocks(x)\n",
    "        out = self.global_avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Data augmentation for sparse data\n",
    "def augment_data(signal, num_augments=10, noise_level=0.02):\n",
    "    augmented_signals = [signal]\n",
    "    for _ in range(num_augments):\n",
    "        noise = noise_level * np.random.randn(len(signal))\n",
    "        shifted_signal = np.roll(signal, shift=np.random.randint(-5, 5))\n",
    "        augmented_signal = shifted_signal + noise\n",
    "        augmented_signals.append(augmented_signal)\n",
    "    return np.array(augmented_signals)\n",
    "\n",
    "# Define function to map belt ID to sorting center\n",
    "# Updated to ensure sorting center names are consistent with actual data\n",
    "def get_sorting_center(belt_id):\n",
    "    sorting_center_mapping = {\n",
    "        0: \"VANTAA\",\n",
    "        1: \"TAMPERE\",\n",
    "        2: \"LIETO\",\n",
    "        3: \"OULU\",\n",
    "        4: \"KUOPIO\",\n",
    "        5: \"SEINÄJOKI\"\n",
    "    }\n",
    "    return sorting_center_mapping.get(belt_id % len(sorting_center_mapping), \"UNKNOWN\")\n",
    "\n",
    "# Initialize model weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "df = df.sort_values(by='scanning_date').reset_index(drop=True)\n",
    "\n",
    "# Train/Test split\n",
    "train_size = int(0.75 * len(df))\n",
    "test_size = int(0.05 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:train_size + test_size]\n",
    "\n",
    "# Data preprocessing\n",
    "feature_columns = ['day', 'month', 'weekday', 'week', 'yearday_sin', 'yearday_cos']\n",
    "\n",
    "# Fill missing values with median values for robustness\n",
    "train_df.loc[:, feature_columns] = train_df[feature_columns].fillna(train_df[feature_columns].median(numeric_only=True))\n",
    "test_df.loc[:, feature_columns] = test_df[feature_columns].fillna(test_df[feature_columns].median(numeric_only=True))\n",
    "\n",
    "# Fit scalers with valid data only\n",
    "scaler_features = RobustScaler()\n",
    "train_features = scaler_features.fit_transform(train_df[feature_columns])\n",
    "test_features = scaler_features.transform(test_df[feature_columns])\n",
    "\n",
    "scaler_target = StandardScaler()\n",
    "train_targets = scaler_target.fit_transform(train_df['no_of_events'].values.reshape(-1, 1))\n",
    "test_targets = scaler_target.transform(test_df['no_of_events'].values.reshape(-1, 1))\n",
    "\n",
    "# Decompose each output belt's signal using Wavelet-based method\n",
    "output_belt_ids = train_df['output_belt'].unique()\n",
    "imfs_dict = {}\n",
    "for belt in output_belt_ids:\n",
    "    belt_signal = train_df[train_df['output_belt'] == belt]['no_of_events'].values\n",
    "    if len(belt_signal) == 0:\n",
    "        continue\n",
    "    augmented_signals = augment_data(belt_signal)\n",
    "    imfs = []\n",
    "    for augmented_signal in augmented_signals:\n",
    "        imfs.extend(decompose_signal_wavelet(augmented_signal))\n",
    "    imfs_dict[belt] = imfs\n",
    "\n",
    "# Assign sorting centers to belts and create models for each sorting center\n",
    "sorting_center_models = {}\n",
    "sorting_center_belts = {}\n",
    "for belt in output_belt_ids:\n",
    "    sorting_center = get_sorting_center(belt)\n",
    "    if sorting_center not in sorting_center_belts:\n",
    "        sorting_center_belts[sorting_center] = []\n",
    "    sorting_center_belts[sorting_center].append(belt)\n",
    "    \n",
    "    # Create a model for each sorting center\n",
    "    if sorting_center not in sorting_center_models:\n",
    "        model = ResNet1D(input_channels=1, num_blocks=num_blocks, hidden_channels=hidden_channels, kernel_size=kernel_size, output_size=1).to(device)\n",
    "        model.apply(init_weights)\n",
    "        sorting_center_models[sorting_center] = model\n",
    "\n",
    "# Training loop for each sorting center model\n",
    "belt_predictions_actuals = {}\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    model = sorting_center_models[sorting_center]\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)  # Changed to SGD with momentum\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)  # Cosine annealing scheduler to dynamically adjust the learning rate\n",
    "    \n",
    "    all_X_train, all_Y_train = [], []\n",
    "    belt_losses = {}\n",
    "    for belt in belts:\n",
    "        imfs = imfs_dict[belt]\n",
    "        belt_losses[belt] = []  # Ensure initialization of belt_losses before using it\n",
    "        for imf in imfs:\n",
    "            X_train, Y_train = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "            if len(X_train) > 0:\n",
    "                all_X_train.append(X_train)\n",
    "                all_Y_train.append(Y_train)\n",
    "    \n",
    "    if len(all_X_train) > 0:\n",
    "        X_train_tensor = torch.tensor(np.concatenate(all_X_train), dtype=torch.float32).unsqueeze(1)\n",
    "        Y_train_tensor = torch.tensor(np.concatenate(all_Y_train), dtype=torch.float32).unsqueeze(1)\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_Y in train_loader:\n",
    "                batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = nn.MSELoss()(outputs.view(-1), batch_Y.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Track loss per belt\n",
    "                for belt in belts:\n",
    "                    belt_losses[belt].append(loss.item())\n",
    "\n",
    "            # Step scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Sorting Center: {sorting_center}, Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "            # Early stopping condition could be added here if required\n",
    "\n",
    "        # Store predictions and actuals for each belt\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for belt in belts:\n",
    "                imfs = imfs_dict[belt]\n",
    "                belt_predictions = []\n",
    "                belt_actuals = []\n",
    "\n",
    "                for imf in imfs:\n",
    "                    X_test, Y_test = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "                    if len(X_test) > 0:\n",
    "                        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                        Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "                        \n",
    "                        predictions = model(X_test_tensor).cpu().numpy().flatten()\n",
    "                        actuals = Y_test_tensor.cpu().numpy().flatten()\n",
    "                        \n",
    "                        belt_predictions.extend(predictions)\n",
    "                        belt_actuals.extend(actuals)\n",
    "\n",
    "                # Store for KPI calculation\n",
    "                belt_predictions_actuals[belt] = {'predictions': belt_predictions, 'actuals': belt_actuals}\n",
    "\n",
    "# Calculate KPIs for each sorting center\n",
    "sorting_center_metrics = {}\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    daily_errors = {}\n",
    "    for belt in belts:\n",
    "        predictions = belt_predictions_actuals[belt]['predictions']\n",
    "        actuals = belt_predictions_actuals[belt]['actuals']\n",
    "        scanning_dates = test_df['scanning_date'][sequence_length:sequence_length + len(predictions)]\n",
    "\n",
    "        # Calculate squared deviation for each day\n",
    "        for date, actual, predicted in zip(scanning_dates, actuals, predictions):\n",
    "            squared_deviation = (actual - predicted) ** 2\n",
    "            if date not in daily_errors:\n",
    "                daily_errors[date] = []\n",
    "            daily_errors[date].append(squared_deviation)\n",
    "\n",
    "    # Calculate daily average squared error across belts\n",
    "    daily_mse = {date: np.mean(errors) for date, errors in daily_errors.items() if len(errors) > 0}\n",
    "    mse = np.mean(list(daily_mse.values())) if len(daily_mse) > 0 else np.nan\n",
    "    vse = np.var(list(daily_mse.values()), ddof=1) if len(daily_mse) > 1 else np.nan\n",
    "\n",
    "    sorting_center_metrics[sorting_center] = {'MSE': mse, 'VSE': vse}\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nOverall Metrics for Each Sorting Center:\")\n",
    "for sorting_center, metrics in sorting_center_metrics.items():\n",
    "    mse = metrics['MSE']\n",
    "    vse = metrics['VSE']\n",
    "    if not np.isnan(mse) and not np.isnan(vse):\n",
    "        print(f\"Sorting Center {sorting_center}: MSE = {mse:.4f}, VSE = {vse:.4f}\")\n",
    "    else:\n",
    "        print(f\"Sorting Center {sorting_center}: No valid predictions available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be2c3e4-c6cf-477e-b1a5-465abf9af3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Sorting Center: KUOPIO, Epoch [1/20], Loss: 26198.2852\n",
      "Sorting Center: KUOPIO, Epoch [2/20], Loss: 25478.2021\n",
      "Sorting Center: KUOPIO, Epoch [3/20], Loss: 25144.9563\n",
      "Sorting Center: KUOPIO, Epoch [4/20], Loss: 24909.9939\n",
      "Sorting Center: KUOPIO, Epoch [5/20], Loss: 24692.6867\n",
      "Sorting Center: KUOPIO, Epoch [6/20], Loss: 24459.6826\n",
      "Sorting Center: KUOPIO, Epoch [7/20], Loss: 24269.4436\n",
      "Sorting Center: KUOPIO, Epoch [8/20], Loss: 24105.4111\n",
      "Sorting Center: KUOPIO, Epoch [9/20], Loss: 23957.1353\n",
      "Sorting Center: KUOPIO, Epoch [10/20], Loss: 23893.0572\n",
      "Sorting Center: KUOPIO, Epoch [11/20], Loss: 23808.8379\n",
      "Sorting Center: KUOPIO, Epoch [12/20], Loss: 23700.2117\n",
      "Sorting Center: KUOPIO, Epoch [13/20], Loss: 23645.4987\n",
      "Sorting Center: KUOPIO, Epoch [14/20], Loss: 23599.2139\n",
      "Sorting Center: KUOPIO, Epoch [15/20], Loss: 23579.5194\n",
      "Sorting Center: KUOPIO, Epoch [16/20], Loss: 23555.6343\n",
      "Sorting Center: KUOPIO, Epoch [17/20], Loss: 23527.7475\n",
      "Sorting Center: KUOPIO, Epoch [18/20], Loss: 23499.9443\n",
      "Sorting Center: KUOPIO, Epoch [19/20], Loss: 23533.3790\n",
      "Sorting Center: KUOPIO, Epoch [20/20], Loss: 23488.6608\n",
      "Sorting Center: LIETO, Epoch [1/20], Loss: 378921.5976\n",
      "Sorting Center: LIETO, Epoch [2/20], Loss: 290043.1161\n",
      "Sorting Center: LIETO, Epoch [3/20], Loss: 288163.1826\n",
      "Sorting Center: LIETO, Epoch [4/20], Loss: 287078.0151\n",
      "Sorting Center: LIETO, Epoch [5/20], Loss: 286403.7462\n",
      "Sorting Center: LIETO, Epoch [6/20], Loss: 285004.0183\n",
      "Sorting Center: LIETO, Epoch [7/20], Loss: 283734.6176\n",
      "Sorting Center: LIETO, Epoch [8/20], Loss: 283050.2291\n",
      "Sorting Center: LIETO, Epoch [9/20], Loss: 280409.8559\n",
      "Sorting Center: LIETO, Epoch [10/20], Loss: 278009.5053\n",
      "Sorting Center: LIETO, Epoch [11/20], Loss: 277503.8193\n",
      "Sorting Center: LIETO, Epoch [12/20], Loss: 275970.0495\n",
      "Sorting Center: LIETO, Epoch [13/20], Loss: 276057.1635\n",
      "Sorting Center: LIETO, Epoch [14/20], Loss: 275431.3228\n",
      "Sorting Center: LIETO, Epoch [15/20], Loss: 275590.4853\n",
      "Sorting Center: LIETO, Epoch [16/20], Loss: 273207.9948\n",
      "Sorting Center: LIETO, Epoch [17/20], Loss: 274439.5215\n",
      "Sorting Center: LIETO, Epoch [18/20], Loss: 271886.0577\n",
      "Sorting Center: LIETO, Epoch [19/20], Loss: 273339.0417\n",
      "Sorting Center: LIETO, Epoch [20/20], Loss: 272171.2610\n",
      "Sorting Center: OULU, Epoch [1/20], Loss: 27060.7628\n",
      "Sorting Center: OULU, Epoch [2/20], Loss: 26186.6995\n",
      "Sorting Center: OULU, Epoch [3/20], Loss: 25840.8416\n",
      "Sorting Center: OULU, Epoch [4/20], Loss: 25485.2551\n",
      "Sorting Center: OULU, Epoch [5/20], Loss: 25248.9243\n",
      "Sorting Center: OULU, Epoch [6/20], Loss: 25029.3928\n",
      "Sorting Center: OULU, Epoch [7/20], Loss: 24845.1665\n",
      "Sorting Center: OULU, Epoch [8/20], Loss: 24759.5181\n",
      "Sorting Center: OULU, Epoch [9/20], Loss: 24635.9550\n",
      "Sorting Center: OULU, Epoch [10/20], Loss: 24563.3363\n",
      "Sorting Center: OULU, Epoch [11/20], Loss: 24506.1459\n",
      "Sorting Center: OULU, Epoch [12/20], Loss: 24456.6251\n",
      "Sorting Center: OULU, Epoch [13/20], Loss: 24375.5627\n",
      "Sorting Center: OULU, Epoch [14/20], Loss: 24334.1681\n",
      "Sorting Center: OULU, Epoch [15/20], Loss: 24302.6730\n",
      "Sorting Center: OULU, Epoch [16/20], Loss: 24262.9606\n",
      "Sorting Center: OULU, Epoch [17/20], Loss: 24292.2112\n",
      "Sorting Center: OULU, Epoch [18/20], Loss: 24261.8438\n",
      "Sorting Center: OULU, Epoch [19/20], Loss: 24243.8125\n",
      "Sorting Center: OULU, Epoch [20/20], Loss: 24255.5279\n",
      "Sorting Center: SEINÄJOKI, Epoch [1/20], Loss: 240051.6852\n",
      "Sorting Center: SEINÄJOKI, Epoch [2/20], Loss: 218309.1010\n",
      "Sorting Center: SEINÄJOKI, Epoch [3/20], Loss: 216224.4246\n",
      "Sorting Center: SEINÄJOKI, Epoch [4/20], Loss: 214708.5533\n",
      "Sorting Center: SEINÄJOKI, Epoch [5/20], Loss: 213701.0256\n",
      "Sorting Center: SEINÄJOKI, Epoch [6/20], Loss: 212434.2798\n",
      "Sorting Center: SEINÄJOKI, Epoch [7/20], Loss: 211523.8948\n",
      "Sorting Center: SEINÄJOKI, Epoch [8/20], Loss: 210467.0099\n",
      "Sorting Center: SEINÄJOKI, Epoch [9/20], Loss: 209779.4036\n",
      "Sorting Center: SEINÄJOKI, Epoch [10/20], Loss: 209097.9321\n",
      "Sorting Center: SEINÄJOKI, Epoch [11/20], Loss: 208203.8696\n",
      "Sorting Center: SEINÄJOKI, Epoch [12/20], Loss: 207712.6393\n",
      "Sorting Center: SEINÄJOKI, Epoch [13/20], Loss: 207129.8578\n",
      "Sorting Center: SEINÄJOKI, Epoch [14/20], Loss: 206936.4332\n",
      "Sorting Center: SEINÄJOKI, Epoch [15/20], Loss: 206938.6621\n",
      "Sorting Center: SEINÄJOKI, Epoch [16/20], Loss: 206585.9742\n",
      "Sorting Center: SEINÄJOKI, Epoch [17/20], Loss: 206306.2344\n",
      "Sorting Center: SEINÄJOKI, Epoch [18/20], Loss: 206352.8051\n",
      "Sorting Center: SEINÄJOKI, Epoch [19/20], Loss: 206172.4569\n",
      "Sorting Center: SEINÄJOKI, Epoch [20/20], Loss: 206161.3312\n",
      "Sorting Center: TAMPERE, Epoch [1/20], Loss: 23684.4738\n",
      "Sorting Center: TAMPERE, Epoch [2/20], Loss: 23215.7396\n",
      "Sorting Center: TAMPERE, Epoch [3/20], Loss: 22925.8348\n",
      "Sorting Center: TAMPERE, Epoch [4/20], Loss: 22674.3818\n",
      "Sorting Center: TAMPERE, Epoch [5/20], Loss: 22403.6731\n",
      "Sorting Center: TAMPERE, Epoch [6/20], Loss: 22226.0831\n",
      "Sorting Center: TAMPERE, Epoch [7/20], Loss: 22048.2351\n",
      "Sorting Center: TAMPERE, Epoch [8/20], Loss: 21885.8981\n",
      "Sorting Center: TAMPERE, Epoch [9/20], Loss: 21774.8066\n",
      "Sorting Center: TAMPERE, Epoch [10/20], Loss: 21642.9528\n",
      "Sorting Center: TAMPERE, Epoch [11/20], Loss: 21583.6885\n",
      "Sorting Center: TAMPERE, Epoch [12/20], Loss: 21508.0491\n",
      "Sorting Center: TAMPERE, Epoch [13/20], Loss: 21446.5727\n",
      "Sorting Center: TAMPERE, Epoch [14/20], Loss: 21379.5662\n",
      "Sorting Center: TAMPERE, Epoch [15/20], Loss: 21363.0405\n",
      "Sorting Center: TAMPERE, Epoch [16/20], Loss: 21342.4329\n",
      "Sorting Center: TAMPERE, Epoch [17/20], Loss: 21319.3578\n",
      "Sorting Center: TAMPERE, Epoch [18/20], Loss: 21299.0363\n",
      "Sorting Center: TAMPERE, Epoch [19/20], Loss: 21292.8885\n",
      "Sorting Center: TAMPERE, Epoch [20/20], Loss: 21309.2115\n",
      "Sorting Center: VANTAA, Epoch [1/20], Loss: 329522.3856\n",
      "Sorting Center: VANTAA, Epoch [2/20], Loss: 316116.1174\n",
      "Sorting Center: VANTAA, Epoch [3/20], Loss: 312534.5009\n",
      "Sorting Center: VANTAA, Epoch [4/20], Loss: 309711.3298\n",
      "Sorting Center: VANTAA, Epoch [5/20], Loss: 307335.1349\n",
      "Sorting Center: VANTAA, Epoch [6/20], Loss: 305383.8068\n",
      "Sorting Center: VANTAA, Epoch [7/20], Loss: 302517.6969\n",
      "Sorting Center: VANTAA, Epoch [8/20], Loss: 301081.4206\n",
      "Sorting Center: VANTAA, Epoch [9/20], Loss: 298976.9238\n",
      "Sorting Center: VANTAA, Epoch [10/20], Loss: 297466.9985\n",
      "Sorting Center: VANTAA, Epoch [11/20], Loss: 295804.5063\n",
      "Sorting Center: VANTAA, Epoch [12/20], Loss: 294570.8532\n",
      "Sorting Center: VANTAA, Epoch [13/20], Loss: 293734.9409\n",
      "Sorting Center: VANTAA, Epoch [14/20], Loss: 293809.5771\n",
      "Sorting Center: VANTAA, Epoch [15/20], Loss: 292543.3218\n",
      "Sorting Center: VANTAA, Epoch [16/20], Loss: 292633.0613\n",
      "Sorting Center: VANTAA, Epoch [17/20], Loss: 291289.1531\n",
      "Sorting Center: VANTAA, Epoch [18/20], Loss: 291773.1047\n",
      "Sorting Center: VANTAA, Epoch [19/20], Loss: 290846.5170\n",
      "Sorting Center: VANTAA, Epoch [20/20], Loss: 292014.3900\n",
      "\n",
      "Overall Metrics for Each Sorting Center:\n",
      "Sorting Center KUOPIO: MSE = 14457.9875, VSE = 84307121.0375\n",
      "Sorting Center LIETO: MSE = 174475.0168, VSE = 29529345633.2543\n",
      "Sorting Center OULU: MSE = 19842.7683, VSE = 109039960.0742\n",
      "Sorting Center SEINÄJOKI: MSE = 254798.8845, VSE = 35905850362.3168\n",
      "Sorting Center TAMPERE: MSE = 14855.8503, VSE = 54804243.8427\n",
      "Sorting Center VANTAA: MSE = 203083.9359, VSE = 25667683132.8492\n"
     ]
    }
   ],
   "source": [
    "from PyEMD import EMD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pywt\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Define hyperparameters\n",
    "sequence_length = 15\n",
    "num_blocks = 2  # 减少网络的深度\n",
    "hidden_channels = 16  # 减少隐藏层通道数\n",
    "kernel_size = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Create GRU input sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        Y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Wavelet-based decomposition function (GPU-friendly)\n",
    "\n",
    "\n",
    "def decompose_signal_wavelet(signal, wavelets=['db1', 'haar', 'sym5'], level=None):\n",
    "    if level is None:\n",
    "        level = min(1, int(np.log2(len(signal))))  # 动态选择合适的分解级别\n",
    "    coeffs_list = []\n",
    "    for wavelet in wavelets:\n",
    "        coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "        coeffs = [torch.tensor(c, dtype=torch.float32).to(device) for c in coeffs]\n",
    "        coeffs_list.extend(coeffs)\n",
    "    return coeffs_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Empirical Mode Decomposition (EMD) based decomposition\n",
    "def decompose_signal_emd(signal):\n",
    "    emd = EMD()  # 初始化 EMD 类\n",
    "    imfs = emd.emd(signal)  # 对信号进行 EMD 分解\n",
    "    imfs = [torch.tensor(imf, dtype=torch.float32).to(device) for imf in imfs]\n",
    "    return imfs\n",
    "\n",
    "# ResNet-like block for time series (Reduced complexity)\n",
    "class ResNetBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dropout_rate=0.3):\n",
    "        super(ResNetBlock1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.skip is not None:\n",
    "            identity = self.skip(identity)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Full ResNet model for time series (Reduced complexity)\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, input_channels, num_blocks, hidden_channels, kernel_size, output_size):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(ResNetBlock1D(in_channels, hidden_channels, kernel_size))\n",
    "            in_channels = hidden_channels\n",
    "        self.resnet_blocks = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(hidden_channels, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.resnet_blocks(x)\n",
    "        out = self.global_avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Data augmentation for sparse data\n",
    "def augment_data(signal, num_augments=10, noise_level=0.02):\n",
    "    augmented_signals = [signal]\n",
    "    for _ in range(num_augments):\n",
    "        noise = noise_level * np.random.randn(len(signal))\n",
    "        shifted_signal = np.roll(signal, shift=np.random.randint(-5, 5))\n",
    "        augmented_signal = shifted_signal + noise\n",
    "        augmented_signals.append(augmented_signal)\n",
    "    return np.array(augmented_signals)\n",
    "\n",
    "# Initialize model weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "df = df.sort_values(by='scanning_date').reset_index(drop=True)\n",
    "\n",
    "# Train/Test split\n",
    "train_size = int(0.75 * len(df))\n",
    "test_size = int(0.05 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:train_size + test_size]\n",
    "\n",
    "# Data preprocessing\n",
    "feature_columns = ['day', 'month', 'weekday', 'week', 'yearday_sin', 'yearday_cos']\n",
    "\n",
    "# Fill missing values with median values for robustness\n",
    "\n",
    "\n",
    "\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "train_df[feature_columns] = train_df[feature_columns].fillna(train_df[feature_columns].median(numeric_only=True))\n",
    "test_df[feature_columns] = test_df[feature_columns].fillna(test_df[feature_columns].median(numeric_only=True))\n",
    "\n",
    "\n",
    "\n",
    "# Fit scalers with valid data only\n",
    "scaler_features = RobustScaler()\n",
    "train_features = scaler_features.fit_transform(train_df[feature_columns])\n",
    "test_features = scaler_features.transform(test_df[feature_columns])\n",
    "\n",
    "scaler_target = StandardScaler()\n",
    "train_targets = scaler_target.fit_transform(train_df['no_of_events'].values.reshape(-1, 1))\n",
    "test_targets = scaler_target.transform(test_df['no_of_events'].values.reshape(-1, 1))\n",
    "\n",
    "# Decompose each output belt's signal using Wavelet and EMD-based method\n",
    "output_belt_ids = train_df['output_belt'].unique()\n",
    "imfs_dict = {}\n",
    "for belt in output_belt_ids:\n",
    "    belt_signal = train_df[train_df['output_belt'] == belt]['no_of_events'].values\n",
    "    if len(belt_signal) == 0:\n",
    "        continue\n",
    "    augmented_signals = augment_data(belt_signal)\n",
    "    imfs = []\n",
    "    for augmented_signal in augmented_signals:\n",
    "        # Apply both Wavelet and EMD decomposition\n",
    "        wavelet_coeffs = decompose_signal_wavelet(augmented_signal)\n",
    "        emd_imfs = decompose_signal_emd(augmented_signal)\n",
    "        imfs.extend(wavelet_coeffs)\n",
    "        imfs.extend(emd_imfs)\n",
    "    imfs_dict[belt] = imfs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assign sorting centers to belts and create models for each sorting center\n",
    "sorting_center_models = {}\n",
    "sorting_center_belts = train_df.groupby('sorting_center_name')['output_belt'].unique().to_dict()\n",
    "\n",
    "# Create models for each sorting center\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    if sorting_center not in sorting_center_models:\n",
    "        model = ResNet1D(input_channels=1, num_blocks=num_blocks, hidden_channels=hidden_channels, kernel_size=kernel_size, output_size=1).to(device)\n",
    "        model.apply(init_weights)\n",
    "        sorting_center_models[sorting_center] = model\n",
    "\n",
    "# Training loop for each sorting center model\n",
    "belt_predictions_actuals = {}\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    model = sorting_center_models[sorting_center]\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    all_X_train, all_Y_train = [], []\n",
    "    belt_losses = {}\n",
    "    for belt in belts:\n",
    "        imfs = imfs_dict.get(belt, [])\n",
    "        belt_losses[belt] = []  # Ensure initialization of belt_losses before using it\n",
    "        for imf in imfs:\n",
    "            X_train, Y_train = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "            if len(X_train) > 0:\n",
    "                all_X_train.append(X_train)\n",
    "                all_Y_train.append(Y_train)\n",
    "\n",
    "    if len(all_X_train) > 0:\n",
    "        X_train_tensor = torch.tensor(np.concatenate(all_X_train), dtype=torch.float32).unsqueeze(1)\n",
    "        Y_train_tensor = torch.tensor(np.concatenate(all_Y_train), dtype=torch.float32).unsqueeze(1)\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_Y in train_loader:\n",
    "                batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = nn.MSELoss()(outputs.view(-1), batch_Y.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Track loss per belt\n",
    "                for belt in belts:\n",
    "                    belt_losses[belt].append(loss.item())\n",
    "\n",
    "            # Step scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Sorting Center: {sorting_center}, Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "           \n",
    "\n",
    "        # Store predictions and actuals for each belt\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for belt in belts:\n",
    "                imfs = imfs_dict.get(belt, [])\n",
    "                belt_predictions = []\n",
    "                belt_actuals = []\n",
    "\n",
    "                for imf in imfs:\n",
    "                    X_test, Y_test = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "                    if len(X_test) > 0:\n",
    "                        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                        Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                        predictions = model(X_test_tensor).cpu().numpy().flatten()\n",
    "                        actuals = Y_test_tensor.cpu().numpy().flatten()\n",
    "\n",
    "                        belt_predictions.extend(predictions)\n",
    "                        belt_actuals.extend(actuals)\n",
    "\n",
    "                # Store for KPI calculation\n",
    "                belt_predictions_actuals[belt] = {'predictions': belt_predictions, 'actuals': belt_actuals}\n",
    "\n",
    "# Calculate KPIs for each sorting center\n",
    "sorting_center_metrics = {}\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    daily_errors = {}\n",
    "    for belt in belts:\n",
    "        if belt not in belt_predictions_actuals:\n",
    "            continue\n",
    "        predictions = belt_predictions_actuals[belt]['predictions']\n",
    "        actuals = belt_predictions_actuals[belt]['actuals']\n",
    "        scanning_dates = test_df['scanning_date'][sequence_length:sequence_length + len(predictions)]\n",
    "\n",
    "        # Calculate squared deviation for each day\n",
    "        for date, actual, predicted in zip(scanning_dates, actuals, predictions):\n",
    "            squared_deviation = (actual - predicted) ** 2\n",
    "            if date not in daily_errors:\n",
    "                daily_errors[date] = []\n",
    "            daily_errors[date].append(squared_deviation)\n",
    "\n",
    "    # Calculate daily average squared error across belts\n",
    "    daily_mse = {date: np.mean(errors) for date, errors in daily_errors.items() if len(errors) > 0}\n",
    "    mse = np.mean(list(daily_mse.values())) if len(daily_mse) > 0 else np.nan\n",
    "    vse = np.var(list(daily_mse.values()), ddof=1) if len(daily_mse) > 1 else np.nan\n",
    "\n",
    "    sorting_center_metrics[sorting_center] = {'MSE': mse, 'VSE': vse}\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nOverall Metrics for Each Sorting Center:\")\n",
    "for sorting_center, metrics in sorting_center_metrics.items():\n",
    "    mse = metrics['MSE']\n",
    "    vse = metrics['VSE']\n",
    "    if not np.isnan(mse) and not np.isnan(vse):\n",
    "        print(f\"Sorting Center {sorting_center}: MSE = {mse:.4f}, VSE = {vse:.4f}\")\n",
    "    else:\n",
    "        print(f\"Sorting Center {sorting_center}: No valid predictions available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe0a8f-a927-46ff-92df-a4660c45ab20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
