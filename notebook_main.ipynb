{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5075eb04",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8551110e7f3394cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:18.271259200Z",
     "start_time": "2024-10-06T20:21:14.977256400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and packages\n",
    "import csv\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import folium\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pmdarima as pm\n",
    "from pmdarima import auto_arima\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "directory_path = os.getcwd() + \"\\\\Data\\\\sorting_event_volumes_2023.csv\"\n",
    "\n",
    "df = pd.read_csv(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9f920",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859069b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_events(df):\n",
    "    df_filled_list = []\n",
    "    \n",
    "    for center in df['sorting_center_name'].unique():\n",
    "        df_center = df[df['sorting_center_name'] == center]\n",
    "        output_belts = df_center['output_belt'].unique()\n",
    "        \n",
    "        min_date = df_center['scanning_date'].min()\n",
    "        max_date = df_center['scanning_date'].max()\n",
    "\n",
    "        all_dates = pd.date_range(start=pd.Timestamp(year=min_date.year, month=1, day=1), end=pd.Timestamp(year=max_date.year, month=max_date.month, day=1) + pd.offsets.MonthEnd(0))\n",
    "        \n",
    "        all_combinations = pd.MultiIndex.from_product(\n",
    "            [[center], all_dates, output_belts],\n",
    "            names=['sorting_center_name', 'scanning_date', 'output_belt']\n",
    "        )\n",
    "        \n",
    "        all_combinations_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "        df_filled_center = pd.merge(all_combinations_df, df_center, \n",
    "                                    on=['sorting_center_name', 'scanning_date', 'output_belt'], \n",
    "                                    how='left')\n",
    "        \n",
    "        df_filled_center['no_of_events'] = df_filled_center['no_of_events'].fillna(0.0001)\n",
    "        df_filled_list.append(df_filled_center)\n",
    "    \n",
    "    df_filled = pd.concat(df_filled_list, ignore_index=True)\n",
    "    \n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579550c6",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ef5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    # Data cleaning\n",
    "    print(\"Number of rows original dataset is: \" + str(df.shape[0]))\n",
    "\n",
    "    df = df.loc[df[\"event_type\"] == \"LAJ\", :]\n",
    "    df.drop(['event_location', 'input_belt', 'position'], axis=1, inplace = True)\n",
    "    df.dropna(inplace = True)\n",
    "    df['output_belt'] = df['output_belt'].astype(int)\n",
    "    df = df.groupby(['sorting_center_name', 'scanning_date', 'output_belt'], as_index = False)['no_of_events'].sum()\n",
    "    df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "\n",
    "    # We add 0 events for all dates without orders \n",
    "    df = fill_missing_events(df)\n",
    "\n",
    "    print(\"Number of rows cleaned dataset is: \" + str(df.shape[0]))\n",
    "\n",
    "    # Data preparation\n",
    "    df['day'] = df['scanning_date'].dt.day\n",
    "    df['month'] = df['scanning_date'].dt.month\n",
    "    df['weekday'] = df['scanning_date'].dt.dayofweek + 1\n",
    "    df['week'] = df['scanning_date'].dt.isocalendar().week\n",
    "    df['week_of_month'] = (df['day'] - 1) // 7 + 1\n",
    "    df['yearday'] = df['scanning_date'].dt.day_of_year\n",
    "    df['weekday_sin'] = np.sin(df['weekday'] / 7 * 2 * np.pi)\n",
    "    df['weekday_cos'] = np.cos(df['weekday'] / 7 * 2 * np.pi)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b921e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c4b96",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25388a4a",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "We construct a correlation matrix to investigate which features correlate most with the number of events. This could give an indication with features would add most to prediction the number of events in the future. We observe that the correlation with number of events the day before is the strongest. Next the correlation is the strongest with output belt, weekday and yearday cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "# Add no_of_events previous day\n",
    "df_prev_yearday = df.copy()\n",
    "df_prev_yearday['yearday_yesterday'] = df_prev_yearday['yearday'] + 1  # Adjusting to match yearday - 1\n",
    "\n",
    "df_correlations = pd.merge(\n",
    "    df, \n",
    "    df_prev_yearday[['sorting_center_name', 'output_belt', 'yearday_yesterday', 'no_of_events']],  # only keep necessary columns from right\n",
    "    left_on=['sorting_center_name', 'output_belt', 'yearday'],  # match on the current 'yearday' from the left df\n",
    "    right_on=['sorting_center_name', 'output_belt', 'yearday_yesterday'],  # match on the 'yearday_yesterday' from the right df\n",
    "    how='left',  # Perform a left join to keep all rows from the left dataframe\n",
    "    suffixes=('', '_yesterday')\n",
    ")\n",
    "\n",
    "df_correlations = df_correlations.drop(columns = [\"yearday_yesterday\"])\n",
    "\n",
    "# Create correlation matrix\n",
    "target_covariance_matrix = df_correlations.drop(columns = [\"sorting_center_name\"]).corr().round(2)\n",
    "#target_covariance_matrix_events = df_correlations.drop(columns = [\"sorting_center_name\"]).corr()[[\"no_of_events\"]]\n",
    "sns.heatmap(target_covariance_matrix,annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac13f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analysis exclude the null events\n",
    "filtered_df = df[df['no_of_events'] >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596e7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:24.351015800Z",
     "start_time": "2024-10-06T20:21:21.728507200Z"
    }
   },
   "outputs": [],
   "source": [
    "event_counts = filtered_df['no_of_events'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(event_counts.index, event_counts.values)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Events (log-scale)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Number of Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3127793",
   "metadata": {},
   "source": [
    "### Sorting centers\n",
    "We have six sorting centers located in Finland. The Vantaa sorting center is situated near the capital, Helsinki, and adjacent to the Helsinki international airport. In contrast, the other centers are located in smaller, more rural cities. A violin plot depicting the distribution of total daily events at each sorting center reveals significantly higher volumes at Vantaa, while the other centers show comparable levels. This prompts further investigation into the similarities among these sorting centers, specifically regarding the potential for predicting trends between urban and rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "locations[\"KUOPIO\"] = (62.894785, 27.666586)\n",
    "locations[\"LIETO\"] = (60.503899,22.4322587)\n",
    "locations[\"OULU\"] = (65.007693, 25.471332)\n",
    "locations[\"SEINÃ„JOKI\"] = (62.7902205,22.7469712)\n",
    "locations[\"TAMPERE\"] = (61.499625, 23.751267)\n",
    "locations[\"VANTAA\"] = (60.294196, 25.035834)\n",
    "\n",
    "avg_lat = sum(lat for lat, _ in locations.values()) / len(locations)\n",
    "avg_lng = sum(lng for _, lng in locations.values()) / len(locations)\n",
    "m = folium.Map(location=[avg_lat, avg_lng], zoom_start=6, tiles='CartoDB positron')\n",
    "\n",
    "# Step 1: Add markers for each sorting center and track bounds\n",
    "bounds = []\n",
    "\n",
    "# Step 2: Add markers for each sorting center\n",
    "for name, (lat, lng) in locations.items():\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=folium.DivIcon(\n",
    "            html='<div style=\"background-color: #C50000; width: 12px; height: 12px; border: none;\"></div>'\n",
    "        ),\n",
    "        popup=name  # Display the sorting center name on click\n",
    "    ).add_to(m)\n",
    "    # Add the current location to bounds\n",
    "    bounds.append((lat, lng))\n",
    "\n",
    "# Step 2: Fit the map bounds to show all markers\n",
    "m.fit_bounds(bounds)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1523837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(df, period = \"yearday\"):\n",
    "    if period == \"yearday\":\n",
    "        df_period = df.groupby([\"yearday\"])[\"no_of_events\"].sum().reset_index()\n",
    "    else:\n",
    "        df_period = df.groupby([\"yearday\", period])[\"no_of_events\"].sum().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if period == \"yearday\":\n",
    "        sns.violinplot(y=\"no_of_events\", data=df_period)\n",
    "    else:\n",
    "        sns.violinplot(x = period, y=\"no_of_events\", data=df_period)\n",
    "\n",
    "    if period == \"sorting_center_name\":\n",
    "        period = \"sorting center\"\n",
    "    plt.xlabel(\"{}\".format(period).capitalize())\n",
    "    plt.ylabel(\"Number of Events\")\n",
    "    plt.title(\"Violin Plot of Number of Events by {}\".format(period).capitalize())\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc61ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly per sorting center\n",
    "plot_violin(df, \"sorting_center_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c072b9",
   "metadata": {},
   "source": [
    "### Outlier analysis\n",
    "The violin plots indicate that all sorting centers have extreme outliers in the number of events compared to their mean. In the following analysis, we will further explore the variability of the number of events across these centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(filtered_df, \n",
    "                x='no_of_events', \n",
    "                y='sorting_center_name', \n",
    "                color='sorting_center_name',\n",
    "                title='Spread of Number of Events by Sorting Center',\n",
    "                labels={'no_of_events': 'Number of Events', 'sorting_center_name': 'Sorting Center'},\n",
    "                stripmode='overlay')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74969f",
   "metadata": {},
   "source": [
    "We observed that most outliers occur at VANTAA. Specifically, there are 284 instances where the number of events exceeds 5000 and 234 instances where it surpasses 10,000. However, no particular day stands out, as the most frequent day occurs only 4 times. Interestingly, output belt 109 accounts for 254 days with over 5000 events and 233 days with over 10,000 events. This means only one other instance of 10,000+ events happens on a different belt, suggesting that the identified outliers reflect consistently high event volumes on belt 109 rather than sporadic spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bac994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier analysis\n",
    "df_VANTAA_oultiers = df[(df[\"sorting_center_name\"] == \"VANTAA\") & (df[\"no_of_events\"] > 5000)]\n",
    "print(df_VANTAA_oultiers.shape[0])\n",
    "\n",
    "df_VANTAA_oultiers[\"yearday\"].value_counts()\n",
    "df_VANTAA_oultiers[\"output_belt\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3fa96",
   "metadata": {},
   "source": [
    "## Data visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dc58f",
   "metadata": {},
   "source": [
    "### Aggregate demand planning horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15255b0",
   "metadata": {},
   "source": [
    "We will focus on the sorting center VANTAA when creating data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e6bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting center is VANTAA\n",
    "df_VANTAA = df[df[\"sorting_center_name\"] == \"VANTAA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb001792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:28.415162Z",
     "start_time": "2024-10-06T20:21:27.433040300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate demand planning horizon (Mats)\n",
    "def total_orders_per_period(df, period):\n",
    "    totals = df.groupby(period)['no_of_events'].sum()\n",
    "\n",
    "    timeframe = \"\"\n",
    "\n",
    "    if period == \"day\":\n",
    "        timeframe = \"month\"\n",
    "        \n",
    "        valid_months = {day: 12 for day in range(1, 29)}\n",
    "        valid_months[29] = 11\n",
    "        valid_months[30] = 11\n",
    "        valid_months[31] = 7\n",
    "\n",
    "        year = df.iloc[0][\"scanning_date\"].year\n",
    "        if (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)):\n",
    "            valid_months[29] = 12\n",
    "\n",
    "        for index in range(len(totals)):\n",
    "            totals[index+1] = totals[index+1] / valid_months[index+1]\n",
    "    elif period == \"weekday\":\n",
    "        timeframe = \"week\"\n",
    "    elif period == \"week\":\n",
    "        timeframe = \"year\"\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(totals.index, totals.values, marker='o')\n",
    "    plt.xlabel('{}'.format(period).capitalize())\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.ylim(0, 1.2 * totals.max())\n",
    "    plt.title('Number of events in a {} over the {}'.format(period, timeframe))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dd5b9",
   "metadata": {},
   "source": [
    "### Demand over the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d18bd7",
   "metadata": {},
   "source": [
    "Below we show the total number of events for each week over the planning horizon of a year. We observe that the demand is relatively stable over the year except of a sharp increase in the last weeks of the year. This is probably due to Black Friday at the end of November and Christmas in December. Furthermore we observe that there are 5 days for which all sorting centers are closed. These are April 9 (Easter Sunday), June 24 (Midsummer Holiday), December 24 (Christmas Eve), December 25 (Christmas), December 31 (New Years')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a610bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orders_per_period(df_VANTAA, \"week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35729f",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc94f9e",
   "metadata": {},
   "source": [
    "Below, we present violin plots showing demand distribution over a year, broken down by month and weekday. Overall, daily demand typically ranges between 50,000 and 160,000, with a high density around 150,000.\n",
    "\n",
    "The monthly plots reveal some interesting patterns. December's average demand is lower than most months, likely due to the reduced activity between Christmas and New Year, which is also evident in the planning horizon plot. Notably, demand in November and December fluctuates significantly, as shown by the peaks in the violin plots, while March and August show a relatively narrow range.\n",
    "\n",
    "Looking at the weekday patterns, demand during the workweek is noticeably higher than on weekends, with Sunday showing almost no demand. Tuesday consistently has the highest demand during the week, while Friday has the lowest. This weekday pattern suggests a cyclical trend, resembling a sine or cosine wave. Incorporating such patterns could maybe enhance the accuracy of demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly\n",
    "plot_violin(df_VANTAA)\n",
    "\n",
    "# Monthly\n",
    "plot_violin(df_VANTAA, \"month\")\n",
    "\n",
    "# Weekday\n",
    "plot_violin(df_VANTAA, \"weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f569b",
   "metadata": {},
   "source": [
    "### Day of the month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b17cad",
   "metadata": {},
   "source": [
    "Below we present the demand distribution over a month by day of the month. We normalized the values since not all day of the month occur with the same frequency. We clearly see peaks at the 13th, 20th and 27th of the month. Interstingly those have a week interval in between, indicating that is maybe has to due with the fact that they occur more often at weekdays instead of weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orders_per_period(df_VANTAA, \"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04fe36",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "Before trying to estimate a linear regression, it might be interesting to study the correlations of the variables with the number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#determine correlations\n",
    "target_covariance_matrix = adf.corr()[['no_of_events']]\n",
    "print(target_covariance_matrix)\n",
    "sns.heatmap(target_covariance_matrix,annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3cec1",
   "metadata": {},
   "source": [
    "This colormap shows that the orders correlate with the day of the week. It was also attempted to put the day of the year in a trigonometric function with a period of 7 days, for which higher correlation was found with the cosine function. The reason is that the beginning of the week is more busy than the end, and the cosine function has its peak at 0.\n",
    "\n",
    "For the linear regression model, lag data has to be created. Lags are the datapoints of x steps back in time. The best result was found by using 2 lags when estimating the model only on aggregate data of the large Vantaa sorting center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a453d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vantaa = adf[adf['sorting_center_name']=='VANTAA']\n",
    "vantaa\n",
    "# Create lag features (e.g., lag 1, lag 2)\n",
    "def create_lag_features(series, lags):\n",
    "    lagged_data = pd.DataFrame()\n",
    "    for lag in range(1, lags+1):\n",
    "        lagged_data[f'lag_{lag}'] = series.shift(lag)\n",
    "    return lagged_data\n",
    "\n",
    "# Create features and target\n",
    "lags = 2  # You can change the number of lags\n",
    "X = create_lag_features(vantaa['no_of_events'], lags)\n",
    "X.dropna(inplace=True)\n",
    "y = vantaa['no_of_events'][lags:]  # Target variable is the actual series shifted by the number of lags\n",
    "dates = vantaa['scanning_date'][lags:]  # Corresponding dates for the target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test, dates_train, dates_test = train_test_split(\n",
    "    X, y, dates, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4aaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(dates_test, y_test, label='Actual Values', color='blue', marker='o')\n",
    "\n",
    "# Plot predicted values\n",
    "plt.plot(dates_test, y_pred, label='Predicted Values', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652e540",
   "metadata": {},
   "source": [
    "## (Artificial) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992d12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7362a836",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da54c1",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "We need to develop a model for each output belt at every sorting center. To ensure the accuracy of our models, we will train the hyperparameters p, d an q using data from the first nine months. These hyperparameters will be saved for future forecasts.\n",
    "\n",
    "In the context of ARIMA:\n",
    "- p represents the number of lag observations included in the model (the autoregressive term).\n",
    "- d denotes the degree of differencing needed to make the time series stationary.\n",
    "- q indicates the size of the moving average window, which reflects the number of lagged forecast errors in the prediction equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567d763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(df_output_belt):\n",
    "    # Perform Box-Cox transformation\n",
    "    #df_output_belt[\"no_of_events_boxcox\"], lam = boxcox(df_output_belt[\"no_of_events\"])\n",
    "    #df_output_belt[\"no_of_events_diff\"] = df_output_belt[\"no_of_events_boxcox\"].diff()\n",
    "    #df_output_belt.dropna(inplace=True)\n",
    "\n",
    "    # Tune the model\n",
    "    tuning_model = auto_arima(df_output_belt[\"no_of_events\"], \n",
    "                               seasonal=False, \n",
    "                               stepwise=True,  \n",
    "                               suppress_warnings=True, \n",
    "                               trace=False)\n",
    "\n",
    "    \n",
    "    p, d, q = tuning_model.order\n",
    "    return (df_output_belt[\"output_belt\"].iloc[0], p, d, q)\n",
    "\n",
    "def train_ARIMA(df):\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        df_sorting_center = df[df[\"sorting_center_name\"] == sorting_center_name]\n",
    "        df_sorting_center.drop([\"sorting_center_name\"], axis=1, inplace=True)\n",
    "        \n",
    "        hyperparameterList = []\n",
    "        output_belts = df_sorting_center[\"output_belt\"].unique()\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(tune_hyperparameters)(\n",
    "            df_sorting_center[df_sorting_center[\"output_belt\"] == output_belt]) \n",
    "            for output_belt in output_belts)\n",
    "        \n",
    "        hyperparameterList.extend(results)\n",
    "\n",
    "        hyperparameter_df = pd.DataFrame(hyperparameterList, columns=[\"output belt\", \"p\", \"d\", \"q\"])\n",
    "        hyperparameter_df.to_csv(f'Data/hyperparameters ARIMA/hyperparameters_ARIMA_{sorting_center_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99101ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "start = datetime.now()\n",
    "train = df[df[\"month\"] <= 9] # Train on first 9 months\n",
    "train_ARIMA(train)\n",
    "end = datetime.now()\n",
    "print(\"Running time training ARIMA is: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ad3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(starting_date, planning_horizon, train, forecasts, model, test = pd.DataFrame([])):\n",
    "    fig = go.Figure()\n",
    "    if test.shape[0] == 0:\n",
    "        scanning_dates = pd.date_range(start=starting_date, periods=planning_horizon).strftime('%Y-%m-%d').tolist()\n",
    "        fig.add_trace(go.Scatter(x=scanning_dates, y=forecasts, name=\"Forecast\"))\n",
    "    if test.shape[0] > 0:\n",
    "        fig.add_trace(go.Scatter(x=train[\"scanning_date\"], y=train[\"no_of_events\"], name=\"Train\"))\n",
    "        fig.add_trace(go.Scatter(x=test[\"scanning_date\"], y=test[\"no_of_events\"], name=\"Test\"))\n",
    "        fig.add_trace(go.Scatter(x=test[\"scanning_date\"], y=forecasts, name=\"Test\"))\n",
    "    fig.update_layout(template=\"simple_white\", font=dict(size=18), title_text=model,\n",
    "                      width=650, title_x=0.5, height=400, xaxis_title=\"scanning_date\",\n",
    "                      yaxis_title=\"no_of_events\")\n",
    "    \n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb65a19",
   "metadata": {},
   "source": [
    "### Test ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d567dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ARIMA per output belt\n",
    "def forecast_ARIMA(starting_date, planning_horizon, train_df, test_df = pd.DataFrame([])):\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    forecast_dict = {}\n",
    "    if test_df.shape[0] > 0:\n",
    "        daily_errors = {}\n",
    "        MSE_dict = {}\n",
    "        VSE_dict = {}\n",
    "        MAE_dict = {}\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        hyperparameters = pd.read_csv(\"Data/hyperparameters ARIMA/hyperparameters_ARIMA_{}.csv\".format(sorting_center_name))\n",
    "\n",
    "        train_sorting_center = train_df[train_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        if test_df.shape[0] > 0:\n",
    "            test_sorting_center = test_df[test_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        output_belts = train_sorting_center['output_belt'].unique()\n",
    "\n",
    "        for output_belt in output_belts:\n",
    "            train_output_belt = train_sorting_center[train_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            if test_df.shape[0] == 0:\n",
    "                test_output_belt = pd.Dataframe([])\n",
    "            elif test_df.shape[0] > 0:\n",
    "                test_output_belt = test_sorting_center[test_sorting_center[\"output_belt\"] == output_belt]\n",
    "            \n",
    "            train_output_belt[\"no_of_events_boxcox\"], lam = boxcox(train_output_belt[\"no_of_events\"])\n",
    "            train_output_belt[\"no_of_events_diff\"] = train_output_belt[\"no_of_events_boxcox\"].diff()\n",
    "            #train_output_belt.dropna(inplace=True)\n",
    "            train_output_belt[\"no_of_events_diff\"].fillna(0, inplace=True)\n",
    "\n",
    "            p, d, q = hyperparameters[hyperparameters.iloc[:, 0] == output_belt].iloc[0, 1:4]\n",
    "\n",
    "            arima_model = ARIMA(train_output_belt[\"no_of_events\"], order=(p, d, q)).fit()\n",
    "\n",
    "            # Forecast for the planning horizon\n",
    "            forecasts = arima_model.forecast(steps=planning_horizon)\n",
    "\n",
    "            '''arima_model = ARIMA(train_output_belt[\"no_of_events_boxcox\"], order=(p, d, q)).fit()\n",
    "\n",
    "            boxcox_forecast = arima_model.forecast(planning_horizon)\n",
    "\n",
    "            def safe_inv_boxcox(y_series, lambda_value):\n",
    "                # Ensure the input is a Pandas Series or DataFrame column\n",
    "                if not isinstance(y_series, (pd.Series, pd.DataFrame)):\n",
    "                    raise ValueError(\"Input must be a Pandas Series or DataFrame column.\")\n",
    "                \n",
    "                # Apply the inverse Box-Cox transformation safely\n",
    "                def transform_value(y):\n",
    "                    base = lambda_value * y + 1\n",
    "                    return base ** (1 / lambda_value) if base > 0 else np.nan\n",
    "                \n",
    "                # Apply the transformation element-wise\n",
    "                return y_series.apply(transform_value)\n",
    "\n",
    "            # Example usage\n",
    "            forecasts = safe_inv_boxcox(boxcox_forecast, lam)\n",
    "\n",
    "            if round(lam,4) == -0.2721:\n",
    "                print(\"Hoi\")\n",
    "            #forecasts = inv_boxcox(boxcox_forecast, lam)'''\n",
    "\n",
    "            if sorting_center_name == \"VANTAA\" and output_belt in [109]:\n",
    "                plot_forecast(starting_date, planning_horizon, train_output_belt, forecasts, \"ARIMA\", test_output_belt)\n",
    "\n",
    "            if test_df.shape[0] == 0: \n",
    "                if sorting_center_name not in forecast_dict:\n",
    "                    forecast_dict[sorting_center_name] = {}\n",
    "                \n",
    "                forecast_dict[sorting_center_name][output_belt] = forecasts\n",
    "                \n",
    "            elif test_df.shape[0] > 0: \n",
    "                for day in range(planning_horizon):\n",
    "                    actual = test_output_belt.iloc[day][\"no_of_events\"]\n",
    "                    forecast = forecasts.iloc[day]\n",
    "\n",
    "                    squared_difference = (actual - forecast) ** 2\n",
    "                    absolute_difference = abs(actual - forecast)\n",
    "\n",
    "                    if math.isnan(squared_difference):\n",
    "                        print(\"The number is NaN\")\n",
    "\n",
    "                    if sorting_center_name not in daily_errors:\n",
    "                        daily_errors[sorting_center_name] = {}\n",
    "                    if day not in daily_errors[sorting_center_name]:\n",
    "                        daily_errors[sorting_center_name][day] = {}\n",
    "                        daily_errors[sorting_center_name][day][\"mse\"] = []\n",
    "                        daily_errors[sorting_center_name][day][\"mae\"] = []\n",
    "                    \n",
    "                    daily_errors[sorting_center_name][day][\"mse\"].append(squared_difference)\n",
    "                    daily_errors[sorting_center_name][day][\"mae\"].append(absolute_difference)\n",
    "\n",
    "    if test_df.shape[0] > 0: \n",
    "        for sorting_center_name in sorting_center_names:\n",
    "            mse = {}\n",
    "            mae = {}\n",
    "            for day in range(planning_horizon):\n",
    "                mse[day] = sum(daily_errors[sorting_center_name][day][\"mse\"]) / len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae[day] = sum(daily_errors[sorting_center_name][day][\"mae\"]) / len(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "\n",
    "            MSE_dict[sorting_center_name] = sum(mse.values()) / len(mse)\n",
    "            VSE_dict[sorting_center_name] = np.var(list(mse.values()), ddof=1)\n",
    "            MAE_dict[sorting_center_name] = sum(mae.values()) / len(mae)\n",
    "\n",
    "        daily_mse = {}\n",
    "        daily_mae = {}\n",
    "\n",
    "        for day in range(planning_horizon):\n",
    "            mse = 0\n",
    "            mae = 0\n",
    "            n_output_belts = 0\n",
    "            for sorting_center_name in sorting_center_names:\n",
    "                mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "                n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "            daily_mse[day] = mse / n_output_belts\n",
    "            daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "        MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "        VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "        MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "        with open(\"Results/results_ARIMA.csv\", mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "            \n",
    "            for key in MSE_dict.keys():\n",
    "                writer.writerow([key, MSE_dict[key], VSE_dict[key], MAE_dict[key]])\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ce856",
   "metadata": {},
   "source": [
    "### Predictions ARIMA model\n",
    "Make forecasts for all sorting centers and report the important KPIs to evaluate the performance. These are the mean squared error (MSE), variance squared error (VSE) and mean absolute error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon = 14\n",
    "\n",
    "train = df[df[\"month\"] <= 9] # Train on first 9 months\n",
    "test = df[(df[\"month\"] == 10) & (df[\"day\"] <= planning_horizon)] # Test on first two weeks of 10th month\n",
    "\n",
    "starting_date = train.iloc[0][\"scanning_date\"]\n",
    "\n",
    "start = datetime.now()\n",
    "forecast_ARIMA(starting_date, planning_horizon, train, test)\n",
    "end = datetime.now()\n",
    "\n",
    "print(\"Running time predicting ARIMA is: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b396b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "MSE_dict = {}\n",
    "VSE_dict = {}\n",
    "MAE_dict = {}\n",
    "daily_errors = {}\n",
    "\n",
    "for sorting_center_name in sorting_center_names:\n",
    "    hyperparameters_df = pd.read_csv(\"Data/hyperparameters ARIMA/hyperparameters_ARIMA_{}.csv\".format(sorting_center_name))\n",
    "    hyperparameterList = [tuple(row) for row in hyperparameters_df.to_numpy()]\n",
    "    daily_errors_sorting_center, mse, mae = predict_ARIMA(df, sorting_center_name, hyperparameterList)\n",
    "\n",
    "    daily_errors[sorting_center_name] = daily_errors_sorting_center\n",
    "    MSE_dict[sorting_center_name] = sum(mse.values()) / len(mse)\n",
    "    VSE_dict[sorting_center_name] = np.var(list(mse.values()), ddof=1)\n",
    "    MAE_dict[sorting_center_name] = sum(mae.values()) / len(mae)\n",
    "    \n",
    "daily_mse = {}\n",
    "daily_mae = {}\n",
    "\n",
    "for day in range(len(daily_errors)):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    n_output_belts = 0\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "        mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "        n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "    daily_mse[day] = mse / n_output_belts\n",
    "    daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "with open(\"Results/results_ARIMA.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "    \n",
    "    for key in MSE_dict.keys():\n",
    "        writer.writerow([key, MSE_dict[key], VSE_dict[key], MAE_dict[key]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11164e7d",
   "metadata": {},
   "source": [
    "## LSTM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e9e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa56ac3e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Write about pro and cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cc71b",
   "metadata": {},
   "source": [
    "## Dashboard forecast (Tom)\n",
    "Standardize input + output\n",
    "Make accessible for forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62352c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training functions for all models such that the training can be saved and used for prediction every time when needed\n",
    "\n",
    "def train_model(data, model):\n",
    "    data = prepare_data(data) # Ensure that data is ready for training\n",
    "\n",
    "    if model == \"Linear Regression\":\n",
    "        #train_LR(data)\n",
    "        print(\"Training is finished\")\n",
    "    elif model == \"ARIMA\":\n",
    "        train_ARIMA(data)\n",
    "        print(\"Training is finished\")\n",
    "    elif model == \"Neural Network\":\n",
    "        #train_NN(data)\n",
    "        print(\"Training is finished\")\n",
    "    '''elif model == \"LSTM\":\n",
    "        #train_LSTM(data)\n",
    "        print(\"Training is finished\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast(start_date, planning_horizon, data, model):\n",
    "    if model == \"Linear Regression\":\n",
    "        #forecast = forecast_LR(start_date, planning_horizon)             # If data is needed for forecast, then add to the function\n",
    "        print(\"Forecast finished\")\n",
    "    elif model == \"ARIMA\":\n",
    "        forecast = forecast_ARIMA(start_date, planning_horizon, data)\n",
    "        print(\"Forecast finished\")\n",
    "    elif model == \"Neural Network\":\n",
    "        #forecast = forecast_NN(start_date, planning_horizon)             # If data is needed for forecast, then add to the function\n",
    "        print(\"Forecast finished\")\n",
    "    '''elif model == \"LSTM\":\n",
    "        #forecast = forecast_LSTM(start_date, planning_horizon)           # If data is needed for forecast, then add to the function\n",
    "        print(\"Forecast finished\")'''\n",
    "        \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddaece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forecast(start_date, planning_horizon, model, forecast):\n",
    "    for sorting_center_name, output_belts in forecast.items():\n",
    "        # Define the file name\n",
    "        file_name = f\"Results/forecast_{model}_{sorting_center_name}.csv\"\n",
    "        \n",
    "        # Open the file for writing\n",
    "        with open(file_name, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # Construct the list of dates for the forecast period\n",
    "            dates = [(start_date + timedelta(days=day)).strftime(\"%Y-%m-%d\") for day in range(planning_horizon)]\n",
    "            \n",
    "            # Write the header row (Dates)\n",
    "            header = ['Output Belt'] + dates\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Write the forecast data\n",
    "            for output_belt, forecasts in output_belts.items():\n",
    "                row = [output_belt] + forecasts.tolist()  # Convert forecasts to list if it's a NumPy array\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(\"Forecasts successfully written to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(start_date, planning_horizon, data, model, train_indicator = False):\n",
    "    if train_indicator == True:\n",
    "        train_model(data, model)\n",
    "\n",
    "    forecast = make_forecast(start_date, planning_horizon, data, model)\n",
    "\n",
    "    save_forecast(start_date, planning_horizon, forecast)\n",
    "    #plot_forecast(start_date, planning_horizon, data, prediction, model)   # Currently written to plot one output belt, so process it in the prediction function of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "directory_path = os.getcwd() + \"\\\\Data\\\\sorting_event_volumes_2023.csv\"\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Input\n",
    "start_date = \"01-10-2023\"       # When does the planning horizon start\n",
    "planning_horizon = 14           # How long is the planning horizon\n",
    "data = df[df[\"month\"] <= 9]     # Always needed since some models need it for prediction\n",
    "model = \"ARIMA\"                 # Which model do you want to use, options are Linear Regression, ARIMA and Neural Network \n",
    "train_indicator = False         # Optional, if you want to train the model\n",
    "\n",
    "main(start_date, planning_horizon, data, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
