{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ce3068",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook describes the process of creating a model to predict parcel volumes for the output belts of parcel sorting centers in Finland. The goal is to plan for a two-week period based on historic data. Models were constructed using varying methods to decide which is the most useful, looking for a balance in accuracy, simplicity, running time and generalizability. The research question aimed to be answered is how can time-series forecasting methods be used to predict future parcel volume in sorting centers? Two sub-questions  are explored to obtain an answer to the research question:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0ef35",
   "metadata": {},
   "source": [
    "\n",
    "1. What does the data look like?\n",
    "   - a.) What does the raw data look like (variables, structure, etc.)?\n",
    "   - b.) What correlations or relationships exist between variables in the data (e.g., between time of day, distribution center, and package volume)?\n",
    "   - c.) How should the data be prepared and cleaned (e.g., handling missing values, duplicates, and inconsistencies)?\n",
    "   - d.) What are the daily frequencies of package deliveries, and are there seasonal patterns (e.g., daily, weekly, or monthly trends)?\n",
    "   - e.) How do the distribution centers differ from each other in terms of package volume and other key metrics \\[‘VANTAA’, ‘KUOPIO’, ‘OULU’, ‘SEINÄJOKI’, ‘TAMPERE’\\]?\n",
    "   - f.) How many outliers are present, and how do we identify them (e.g., through statistical or machine learning methods)?\n",
    "   - g.) What is the statistical distribution of the data points (e.g., normal, skewed, or multimodal)?\n",
    "   - h.) How do holidays and special events impact the volume of packages?\n",
    "\n",
    "2. What models can potentially yield effective modeling results?\n",
    "   - a.) Linear Regression Model (with benchmarks MSE, VSE, MAE, and running time)\n",
    "   - b.) (Artificial) Neural Network (with benchmarks MSE, VSE, MAE, and running time)\n",
    "   - c.) ARIMA (with benchmarks MSE, VSE, MAE, and running time)\n",
    "   - d.) LSTM/GRU (with benchmarks MSE, VSE, MAE, and running time)\n",
    "\n",
    "After each sub-question is explored, a final results and conclusion section will summarize the results. A dashboard will then be created to streamline testing all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075eb04",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8551110e7f3394cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:18.271259200Z",
     "start_time": "2024-10-06T20:21:14.977256400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and packages\n",
    "import CEEMDAN_LSTM as cl\n",
    "import csv\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import folium\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "#from EMD import EMD\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pmdarima as pm\n",
    "from pmdarima import auto_arima\n",
    "# import PyEMD\n",
    "# from PyEMD import EMD\n",
    "# import pywt\n",
    "import random\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9f920",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859069b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_events(df):\n",
    "    df_filled_list = []\n",
    "    \n",
    "    for center in df['sorting_center_name'].unique():\n",
    "        df_center = df[df['sorting_center_name'] == center]\n",
    "        output_belts = df_center['output_belt'].unique()\n",
    "        \n",
    "        min_date = df_center['scanning_date'].min()\n",
    "        max_date = df_center['scanning_date'].max()\n",
    "\n",
    "        all_dates = pd.date_range(start=pd.Timestamp(year=min_date.year, month=1, day=1), end=pd.Timestamp(year=max_date.year, month=max_date.month, day=1) + pd.offsets.MonthEnd(0))\n",
    "        \n",
    "        all_combinations = pd.MultiIndex.from_product(\n",
    "            [[center], all_dates, output_belts],\n",
    "            names=['sorting_center_name', 'scanning_date', 'output_belt']\n",
    "        )\n",
    "        \n",
    "        all_combinations_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "        df_filled_center = pd.merge(all_combinations_df, df_center, \n",
    "                                    on=['sorting_center_name', 'scanning_date', 'output_belt'], \n",
    "                                    how='left')\n",
    "        \n",
    "        df_filled_center['no_of_events'] = df_filled_center['no_of_events'].fillna(0.0001)\n",
    "        df_filled_list.append(df_filled_center)\n",
    "    \n",
    "    df_filled = pd.concat(df_filled_list, ignore_index=True)\n",
    "    \n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ef5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    # Data cleaning\n",
    "    print(\"Number of rows original dataset is: \" + str(df.shape[0]))\n",
    "\n",
    "    df = df.loc[df[\"event_type\"] == \"LAJ\", :]\n",
    "    df.drop(['event_location', 'input_belt', 'position'], axis=1, inplace = True)\n",
    "    df.dropna(inplace = True)\n",
    "    df['output_belt'] = df['output_belt'].astype(int)\n",
    "    df = df.groupby(['sorting_center_name', 'scanning_date', 'output_belt'], as_index = False)['no_of_events'].sum()\n",
    "    df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "\n",
    "    # We add 0 events for all dates without orders \n",
    "    df = fill_missing_events(df)\n",
    "\n",
    "    print(\"Number of rows cleaned dataset is: \" + str(df.shape[0]))\n",
    "\n",
    "    # Data preparation\n",
    "    df['day'] = df['scanning_date'].dt.day\n",
    "    df['month'] = df['scanning_date'].dt.month\n",
    "    df['weekday'] = df['scanning_date'].dt.dayofweek + 1\n",
    "    df['week'] = df['scanning_date'].dt.isocalendar().week\n",
    "    df['week_of_month'] = (df['day'] - 1) // 7 + 1\n",
    "    df['yearday'] = df['scanning_date'].dt.day_of_year\n",
    "    df['weekday_sin'] = np.sin(df['weekday'] / 7 * 2 * np.pi)\n",
    "    df['weekday_cos'] = np.cos(df['weekday'] / 7 * 2 * np.pi)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeeef21",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b921e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows original dataset is: 8949721\n",
      "Number of rows cleaned dataset is: 243090\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "directory_path = os.getcwd() + \"\\\\Data\\\\sorting_event_volumes_2023.csv\"\n",
    "\n",
    "df = pd.read_csv(directory_path)\n",
    "df = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c4b96",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25388a4a",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "We construct a correlation matrix to investigate which features correlate most with the number of events. This could give an indication with features would add most to prediction the number of events in the future. We observe that the correlation with number of events the day before is the strongest. Next the correlation is the strongest with output belt, weekday and yearday cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "# Add no_of_events previous day\n",
    "df_prev_yearday = df.copy()\n",
    "df_prev_yearday['yearday_yesterday'] = df_prev_yearday['yearday'] + 1  # Adjusting to match yearday - 1\n",
    "\n",
    "df_correlations = pd.merge(\n",
    "    df, \n",
    "    df_prev_yearday[['sorting_center_name', 'output_belt', 'yearday_yesterday', 'no_of_events']],  # only keep necessary columns from right\n",
    "    left_on=['sorting_center_name', 'output_belt', 'yearday'],  # match on the current 'yearday' from the left df\n",
    "    right_on=['sorting_center_name', 'output_belt', 'yearday_yesterday'],  # match on the 'yearday_yesterday' from the right df\n",
    "    how='left',  # Perform a left join to keep all rows from the left dataframe\n",
    "    suffixes=('', '_yesterday')\n",
    ")\n",
    "\n",
    "df_correlations = df_correlations.drop(columns = [\"yearday_yesterday\"])\n",
    "\n",
    "# Create correlation matrix\n",
    "target_covariance_matrix = df_correlations.drop(columns = [\"sorting_center_name\"]).corr().round(2)\n",
    "sns.heatmap(target_covariance_matrix,annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac13f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analysis exclude the null events\n",
    "filtered_df = df[df['no_of_events'] >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596e7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:24.351015800Z",
     "start_time": "2024-10-06T20:21:21.728507200Z"
    }
   },
   "outputs": [],
   "source": [
    "event_counts = filtered_df['no_of_events'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(event_counts.index, event_counts.values)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Events (log-scale)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Number of Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3127793",
   "metadata": {},
   "source": [
    "### Sorting centers\n",
    "We have six sorting centers located in Finland. The Vantaa sorting center is situated near the capital, Helsinki, and adjacent to the Helsinki international airport. In contrast, the other centers are located in smaller, more rural cities. A violin plot depicting the distribution of total daily events at each sorting center reveals significantly higher volumes at Vantaa, while the other centers show comparable levels. This prompts further investigation into the similarities among these sorting centers, specifically regarding the potential for predicting trends between urban and rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "locations[\"KUOPIO\"] = (62.894785, 27.666586)\n",
    "locations[\"LIETO\"] = (60.503899,22.4322587)\n",
    "locations[\"OULU\"] = (65.007693, 25.471332)\n",
    "locations[\"SEINÄJOKI\"] = (62.7902205,22.7469712)\n",
    "locations[\"TAMPERE\"] = (61.499625, 23.751267)\n",
    "locations[\"VANTAA\"] = (60.294196, 25.035834)\n",
    "\n",
    "avg_lat = sum(lat for lat, _ in locations.values()) / len(locations)\n",
    "avg_lng = sum(lng for _, lng in locations.values()) / len(locations)\n",
    "m = folium.Map(location=[avg_lat, avg_lng], zoom_start=6, tiles='CartoDB positron')\n",
    "\n",
    "# Step 1: Add markers for each sorting center and track bounds\n",
    "bounds = []\n",
    "\n",
    "# Step 2: Add markers for each sorting center\n",
    "for name, (lat, lng) in locations.items():\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=folium.DivIcon(\n",
    "            html='<div style=\"background-color: #C50000; width: 12px; height: 12px; border: none;\"></div>'\n",
    "        ),\n",
    "        popup=name  # Display the sorting center name on click\n",
    "    ).add_to(m)\n",
    "    # Add the current location to bounds\n",
    "    bounds.append((lat, lng))\n",
    "\n",
    "# Step 2: Fit the map bounds to show all markers\n",
    "m.fit_bounds(bounds)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1523837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(df, period = \"yearday\"):\n",
    "    if period == \"yearday\":\n",
    "        df_period = df.groupby([\"yearday\"])[\"no_of_events\"].sum().reset_index()\n",
    "    else:\n",
    "        df_period = df.groupby([\"yearday\", period])[\"no_of_events\"].sum().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if period == \"yearday\":\n",
    "        sns.violinplot(y=\"no_of_events\", data=df_period)\n",
    "    else:\n",
    "        sns.violinplot(x = period, y=\"no_of_events\", data=df_period)\n",
    "\n",
    "    if period == \"sorting_center_name\":\n",
    "        period = \"sorting center\"\n",
    "    plt.xlabel(\"{}\".format(period).capitalize())\n",
    "    plt.ylabel(\"Number of Events\")\n",
    "    plt.title(\"Violin Plot of Number of Events by {}\".format(period).capitalize())\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc61ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly per sorting center\n",
    "plot_violin(df, \"sorting_center_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c072b9",
   "metadata": {},
   "source": [
    "### Outlier analysis\n",
    "The violin plots indicate that all sorting centers have extreme outliers in the number of events compared to their mean. In the following analysis, we will further explore the variability of the number of events across these centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(filtered_df, \n",
    "                x='no_of_events', \n",
    "                y='sorting_center_name', \n",
    "                color='sorting_center_name',\n",
    "                title='Spread of Number of Events by Sorting Center',\n",
    "                labels={'no_of_events': 'Number of Events', 'sorting_center_name': 'Sorting Center'},\n",
    "                stripmode='overlay')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74969f",
   "metadata": {},
   "source": [
    "We observed that most outliers occur at VANTAA. Specifically, there are 284 instances where the number of events exceeds 5000 and 234 instances where it surpasses 10,000. However, no particular day stands out, as the most frequent day occurs only 4 times. Interestingly, output belt 109 accounts for 254 days with over 5000 events and 233 days with over 10,000 events. This means only one other instance of 10,000+ events happens on a different belt, suggesting that the identified outliers reflect consistently high event volumes on belt 109 rather than sporadic spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bac994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier analysis\n",
    "df_VANTAA_oultiers = df[(df[\"sorting_center_name\"] == \"VANTAA\") & (df[\"no_of_events\"] > 5000)]\n",
    "df_VANTAA_oultiers[\"output_belt\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3fa96",
   "metadata": {},
   "source": [
    "## Data visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dc58f",
   "metadata": {},
   "source": [
    "### Aggregate demand planning horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15255b0",
   "metadata": {},
   "source": [
    "We will focus on the sorting center VANTAA when creating data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e6bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting center is VANTAA\n",
    "df_VANTAA = df[df[\"sorting_center_name\"] == \"VANTAA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb001792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:28.415162Z",
     "start_time": "2024-10-06T20:21:27.433040300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate demand planning horizon (Mats)\n",
    "def total_orders_per_period(df, period):\n",
    "    totals = df.groupby(period)['no_of_events'].sum()\n",
    "\n",
    "    timeframe = \"\"\n",
    "\n",
    "    if period == \"day\":\n",
    "        timeframe = \"month\"\n",
    "        \n",
    "        valid_months = {day: 12 for day in range(1, 29)}\n",
    "        valid_months[29] = 11\n",
    "        valid_months[30] = 11\n",
    "        valid_months[31] = 7\n",
    "\n",
    "        year = df.iloc[0][\"scanning_date\"].year\n",
    "        if (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)):\n",
    "            valid_months[29] = 12\n",
    "\n",
    "        for index in range(len(totals)):\n",
    "            totals[index+1] = totals[index+1] / valid_months[index+1]\n",
    "    elif period == \"weekday\":\n",
    "        timeframe = \"week\"\n",
    "    elif period == \"week\":\n",
    "        timeframe = \"year\"\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(totals.index, totals.values, marker='o')\n",
    "    plt.xlabel('{}'.format(period).capitalize())\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.ylim(0, 1.2 * totals.max())\n",
    "    plt.title('Number of events in a {} over the {}'.format(period, timeframe))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dd5b9",
   "metadata": {},
   "source": [
    "### Demand over the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d18bd7",
   "metadata": {},
   "source": [
    "Below we show the total number of events for each week over the planning horizon of a year. We observe that the demand is relatively stable over the year except of a sharp increase in the last weeks of the year. This is probably due to Black Friday at the end of November and Christmas in December. Furthermore we observe that there are 5 days for which all sorting centers are closed. These are April 9 (Easter Sunday), June 24 (Midsummer Holiday), December 24 (Christmas Eve), December 25 (Christmas), December 31 (New Years')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a610bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orders_per_period(df_VANTAA, \"week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35729f",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc94f9e",
   "metadata": {},
   "source": [
    "Below, we present violin plots showing demand distribution over a year, broken down by month and weekday. Overall, daily demand typically ranges between 50,000 and 160,000, with a high density around 150,000.\n",
    "\n",
    "The monthly plots reveal some interesting patterns. December's average demand is lower than most months, likely due to the reduced activity between Christmas and New Year, which is also evident in the planning horizon plot. Notably, demand in November and December fluctuates significantly, as shown by the peaks in the violin plots, while March and August show a relatively narrow range.\n",
    "\n",
    "Looking at the weekday patterns, demand during the workweek is noticeably higher than on weekends, with Sunday showing almost no demand. Tuesday consistently has the highest demand during the week, while Friday has the lowest. This weekday pattern suggests a cyclical trend, resembling a sine or cosine wave. Incorporating such patterns could maybe enhance the accuracy of demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly\n",
    "plot_violin(df_VANTAA)\n",
    "\n",
    "# Monthly\n",
    "plot_violin(df_VANTAA, \"month\")\n",
    "\n",
    "# Weekday\n",
    "plot_violin(df_VANTAA, \"weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f569b",
   "metadata": {},
   "source": [
    "### Day of the month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b17cad",
   "metadata": {},
   "source": [
    "Below we present the demand distribution over a month by day of the month. We normalized the values since not all day of the month occur with the same frequency. We clearly see peaks at the 13th, 20th and 27th of the month. Interstingly those have a week interval in between, indicating that is maybe has to due with the fact that they occur more often at weekdays instead of weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orders_per_period(df_VANTAA, \"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83d3fd",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e6e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare result file\n",
    "with open(\"Results/results_general.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Method', 'MSE', 'VSE', 'MAE', 'Running time training', 'Running time forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b32cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(starting_date, planning_horizon, output_belt, forecasts, model, train = pd.DataFrame([]), test = pd.DataFrame([])):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    scanning_dates = pd.date_range(start=starting_date, periods=planning_horizon).strftime('%Y-%m-%d').tolist()\n",
    "    if test.shape[0] == 0:\n",
    "        fig.add_trace(go.Scatter(x=scanning_dates, y=forecasts, name=\"Forecast\"))\n",
    "    if test.shape[0] > 0:\n",
    "        #fig.add_trace(go.Scatter(x=train[\"scanning_date\"], y=train[\"no_of_events\"], name=\"Train\"))\n",
    "        fig.add_trace(go.Scatter(x=scanning_dates, y=test[\"no_of_events\"], name=\"Test\"))\n",
    "        fig.add_trace(go.Scatter(x=scanning_dates, y=forecasts[\"forecasts\"], name=\"Forecast\"))\n",
    "    fig.update_layout(template=\"simple_white\", font=dict(size=18), title_text=f\"Forecast for output belt {output_belt} with {model}\",\n",
    "                      width=650, title_x=0.5, height=400, xaxis_title=\"scanning_date\",\n",
    "                      yaxis_title=\"no_of_events\")\n",
    "    \n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04fe36",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3cec1",
   "metadata": {},
   "source": [
    "The first step in finding a suitable prediction model was to implement a relatively simple one, the linear regression model. To check which variables were most useful, different combinations of features were used in a model to predict aggregate demand for sorting center Vantaa, and those yielding the best results were used in this model. The chosen features are 7 lag variables that represent the values of the last 7 days, the day of the week, and a sine and cosine of this day of the week (with period 7). Increasing the number of lag features did not improve the results.\n",
    "\n",
    "One important note is that for the lag variables, the predictions of the model have to be used, because in real life that would be the only data available. For this reason, the model decreases in accuracy as time progresses in the planning horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e43931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(series, lags):\n",
    "    lagged_data = pd.DataFrame()\n",
    "    for lag in range(1, lags+1):\n",
    "        lagged_data[f'lag_{lag}'] = series.shift(lag)\n",
    "    return lagged_data\n",
    "\n",
    "def linreg(df_train, planning_horizon, df_test=pd.DataFrame([])):\n",
    "    lags = 7 \n",
    "    X_train = create_lag_features(df_train['no_of_events'], lags)\n",
    "    y_train = df_train['no_of_events'][lags:]  # Target variable is the actual series shifted by the number of lags\n",
    "\n",
    "    # Add additional features\n",
    "    X_train['weekday'] = df_train['weekday']\n",
    "    X_train['weekday_cos'] = df_train['weekday_cos']\n",
    "    X_train['weekday_sin'] = df_train['weekday_sin']\n",
    "\n",
    "    # Drop rows with NaN values in X_train\n",
    "    X_train.dropna(inplace=True)\n",
    "\n",
    "    # Train the model\n",
    "    features = ['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'weekday', 'weekday_cos', 'weekday_sin'] \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[features], y_train)\n",
    "\n",
    "    # Prepare for predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Start with the last available data point\n",
    "    last_row = X_train.iloc[-1].copy()\n",
    "\n",
    "    # Recursive forecasting loop for the prediction horizon\n",
    "    for i in range(planning_horizon):\n",
    "        # Make the prediction for the current step\n",
    "        pred = model.predict(last_row[features].values.reshape(1, -1))[0]\n",
    "        predictions.append(pred)\n",
    "\n",
    "        # Update the last_row for the next prediction\n",
    "        last_row['lag_1'] = pred  # Update lag_1 with the new prediction\n",
    "\n",
    "        # Shift the lag features\n",
    "        for lag in range(2, 8):\n",
    "            last_row[f'lag_{lag}'] = last_row[f'lag_{lag - 1}']\n",
    "\n",
    "        # Update the additional features (assuming they follow a similar pattern)\n",
    "        # You may need to adjust this logic based on how your features are structured\n",
    "        last_row['weekday'] = (last_row['weekday'] + 1) % 7  # Example: Increment weekday cyclically\n",
    "        last_row['weekday_cos'] = np.cos(2 * np.pi * last_row['weekday'] / 7)\n",
    "        last_row['weekday_sin'] = np.sin(2 * np.pi * last_row['weekday'] / 7)\n",
    "\n",
    "    # Convert predictions list to a NumPy array or DataFrame for analysis\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['forecasts'])\n",
    "\n",
    "    # Set the index based on df_test if it exists\n",
    "    if df_test.shape[0] > 0:\n",
    "        predictions_df.index = df_test.index[:planning_horizon]\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bc8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_LR(starting_date, planning_horizon, train_df, test_df = pd.DataFrame([])):\n",
    "    start = datetime.now()\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    forecast_dict = {}\n",
    "    if test_df.shape[0] > 0:\n",
    "        daily_errors = {}\n",
    "        MSE_dict = {}\n",
    "        VSE_dict = {}\n",
    "        MAE_dict = {}\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        train_sorting_center = train_df[train_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        if test_df.shape[0] > 0:\n",
    "            test_sorting_center = test_df[test_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        output_belts = train_sorting_center['output_belt'].unique()\n",
    "\n",
    "        for output_belt in output_belts:\n",
    "            train_output_belt = train_sorting_center[train_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            if test_df.shape[0] == 0:\n",
    "                test_output_belt = pd.Dataframe([])\n",
    "            elif test_df.shape[0] > 0:\n",
    "                test_output_belt = test_sorting_center[test_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            forecasts = linreg(train_output_belt, planning_horizon, test_output_belt)  \n",
    "\n",
    "            if sorting_center_name == \"VANTAA\" and output_belt in [66, 109, 333]:\n",
    "                plot_forecast(starting_date, planning_horizon, output_belt, forecasts, \"Linear Regression\", train_output_belt, test_output_belt)  \n",
    "\n",
    "            if test_df.shape[0] == 0: \n",
    "                if sorting_center_name not in forecast_dict:\n",
    "                    forecast_dict[sorting_center_name] = {}\n",
    "                \n",
    "                forecast_dict[sorting_center_name][output_belt] = forecasts\n",
    "                \n",
    "            elif test_df.shape[0] > 0: \n",
    "                for day in range(planning_horizon):\n",
    "                    actual = test_output_belt.iloc[day][\"no_of_events\"]\n",
    "                    forecast = forecasts.iloc[day]\n",
    "\n",
    "                    squared_difference = (actual - forecast) ** 2\n",
    "                    absolute_difference = abs(actual - forecast)\n",
    "\n",
    "                    if sorting_center_name not in daily_errors:\n",
    "                        daily_errors[sorting_center_name] = {}\n",
    "                    if day not in daily_errors[sorting_center_name]:\n",
    "                        daily_errors[sorting_center_name][day] = {}\n",
    "                        daily_errors[sorting_center_name][day][\"mse\"] = []\n",
    "                        daily_errors[sorting_center_name][day][\"mae\"] = []\n",
    "                    \n",
    "                    daily_errors[sorting_center_name][day][\"mse\"].append(squared_difference)\n",
    "                    daily_errors[sorting_center_name][day][\"mae\"].append(absolute_difference)\n",
    "\n",
    "    if test_df.shape[0] > 0: \n",
    "        for sorting_center_name in sorting_center_names:\n",
    "            mse = {}\n",
    "            mae = {}\n",
    "            for day in range(planning_horizon):\n",
    "                mse[day] = sum(daily_errors[sorting_center_name][day][\"mse\"]) / len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae[day] = sum(daily_errors[sorting_center_name][day][\"mae\"]) / len(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "\n",
    "            MSE_dict[sorting_center_name] = sum(mse.values()) / len(mse)\n",
    "            VSE_dict[sorting_center_name] = np.var(list(mse.values()), ddof=1)\n",
    "            MAE_dict[sorting_center_name] = sum(mae.values()) / len(mae)\n",
    "\n",
    "        daily_mse = {}\n",
    "        daily_mae = {}\n",
    "\n",
    "        for day in range(planning_horizon):\n",
    "            mse = 0\n",
    "            mae = 0\n",
    "            n_output_belts = 0\n",
    "            for sorting_center_name in sorting_center_names:\n",
    "                mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "                n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "            daily_mse[day] = mse / n_output_belts\n",
    "            daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "        MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "        VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "        MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "        with open(\"Results/results_LR.csv\", mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "            \n",
    "            for key in MSE_dict.keys():\n",
    "                # Extract the first value from each Series\n",
    "                mse_value = MSE_dict[key].values[0] if isinstance(MSE_dict[key], pd.Series) else MSE_dict[key]\n",
    "                vse_value = VSE_dict[key].values[0] if isinstance(VSE_dict[key], pd.Series) else VSE_dict[key]\n",
    "                mae_value = MAE_dict[key].values[0] if isinstance(MAE_dict[key], pd.Series) else MAE_dict[key]\n",
    "                \n",
    "                writer.writerow([key, mse_value, vse_value, mae_value])\n",
    "\n",
    "        with open(\"Results/results_general.csv\", mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            end = datetime.now()\n",
    "\n",
    "            mse_value = MSE_dict[\"total\"].values[0] if isinstance(MSE_dict[\"total\"], pd.Series) else MSE_dict[\"total\"]\n",
    "            vse_value = VSE_dict[\"total\"].values[0] if isinstance(VSE_dict[\"total\"], pd.Series) else VSE_dict[\"total\"]\n",
    "            mae_value = MAE_dict[\"total\"].values[0] if isinstance(MAE_dict[\"total\"], pd.Series) else MAE_dict[\"total\"]\n",
    "            running_time = end - start\n",
    "\n",
    "            writer.writerow(['Linear Regression', mse_value, vse_value, mae_value, \"-\", running_time])\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c51324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Test",
         "type": "scatter",
         "x": [
          "2023-10-01",
          "2023-10-02",
          "2023-10-03",
          "2023-10-04",
          "2023-10-05",
          "2023-10-06",
          "2023-10-07",
          "2023-10-08",
          "2023-10-09",
          "2023-10-10",
          "2023-10-11",
          "2023-10-12",
          "2023-10-13",
          "2023-10-14"
         ],
         "y": [
          0.0001,
          2332,
          2252,
          1969,
          2288,
          2277,
          581,
          1,
          2234,
          1644,
          1903,
          2008,
          1959,
          467
         ]
        },
        {
         "name": "Forecast",
         "type": "scatter",
         "x": [
          "2023-10-01",
          "2023-10-02",
          "2023-10-03",
          "2023-10-04",
          "2023-10-05",
          "2023-10-06",
          "2023-10-07",
          "2023-10-08",
          "2023-10-09",
          "2023-10-10",
          "2023-10-11",
          "2023-10-12",
          "2023-10-13",
          "2023-10-14"
         ],
         "y": [
          702.6734879670807,
          1990.884950598463,
          2515.8107561574598,
          2561.3136930140913,
          2403.8615258439595,
          2184.4582409075856,
          1865.1594418302172,
          1306.705584146351,
          2463.5599248293493,
          2885.694463790538,
          2850.759855030623,
          2630.362663842739,
          2361.7028281473085,
          2003.859158676448
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 18
        },
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Forecast for output belt 66 with Linear Regression",
         "x": 0.5
        },
        "width": 650,
        "xaxis": {
         "title": {
          "text": "scanning_date"
         }
        },
        "yaxis": {
         "title": {
          "text": "no_of_events"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Test",
         "type": "scatter",
         "x": [
          "2023-10-01",
          "2023-10-02",
          "2023-10-03",
          "2023-10-04",
          "2023-10-05",
          "2023-10-06",
          "2023-10-07",
          "2023-10-08",
          "2023-10-09",
          "2023-10-10",
          "2023-10-11",
          "2023-10-12",
          "2023-10-13",
          "2023-10-14"
         ],
         "y": [
          0.0001,
          18818,
          13789,
          16112,
          12949,
          13218,
          2433,
          0.0001,
          16401,
          14169,
          13025,
          14881,
          14489,
          3738
         ]
        },
        {
         "name": "Forecast",
         "type": "scatter",
         "x": [
          "2023-10-01",
          "2023-10-02",
          "2023-10-03",
          "2023-10-04",
          "2023-10-05",
          "2023-10-06",
          "2023-10-07",
          "2023-10-08",
          "2023-10-09",
          "2023-10-10",
          "2023-10-11",
          "2023-10-12",
          "2023-10-13",
          "2023-10-14"
         ],
         "y": [
          2807.436978467409,
          19718.02055681201,
          19714.76487012214,
          16712.791456523446,
          13739.754621723598,
          10989.914389816298,
          7888.106501132346,
          3995.5867677156657,
          19972.144877818148,
          19769.11758819794,
          16724.416545969943,
          13742.241023681465,
          10990.446187418764,
          7888.22024327653
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 18
        },
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Forecast for output belt 109 with Linear Regression",
         "x": 0.5
        },
        "width": 650,
        "xaxis": {
         "title": {
          "text": "scanning_date"
         }
        },
        "yaxis": {
         "title": {
          "text": "no_of_events"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Test",
         "type": "scatter",
         "x": [
          "2023-10-01",
          "2023-10-02",
          "2023-10-03",
          "2023-10-04",
          "2023-10-05",
          "2023-10-06",
          "2023-10-07",
          "2023-10-08",
          "2023-10-09",
          "2023-10-10",
          "2023-10-11",
          "2023-10-12",
          "2023-10-13",
          "2023-10-14"
         ],
         "y": [
          0.0001,
          167,
          210,
          165,
          213,
          176,
          38,
          0.0001,
          171,
          245,
          200,
          243,
          187,
          50
         ]
        },
        {
         "name": "Forecast",
         "type": "scatter",
         "x": [
          "2023-10-01",
          "2023-10-02",
          "2023-10-03",
          "2023-10-04",
          "2023-10-05",
          "2023-10-06",
          "2023-10-07",
          "2023-10-08",
          "2023-10-09",
          "2023-10-10",
          "2023-10-11",
          "2023-10-12",
          "2023-10-13",
          "2023-10-14"
         ],
         "y": [
          55.22234217013062,
          197.91790751099668,
          256.92644960008647,
          275.4923789323792,
          264.150450727011,
          222.01733702292807,
          153.15974763702985,
          71.96966594185228,
          206.91662043913334,
          261.76166001739176,
          278.09044600778475,
          265.54645047588394,
          222.76743901887028,
          153.56279427423067
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 18
        },
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Forecast for output belt 333 with Linear Regression",
         "x": 0.5
        },
        "width": 650,
        "xaxis": {
         "title": {
          "text": "scanning_date"
         }
        },
        "yaxis": {
         "title": {
          "text": "no_of_events"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planning_horizon = 14\n",
    "\n",
    "df_linear = df.copy()\n",
    "\n",
    "train = df_linear[df_linear[\"month\"] <= 9] # Train on first 9 months\n",
    "test = df_linear[(df_linear[\"month\"] == 10) & (df_linear[\"day\"] <= planning_horizon)] # Test on first two weeks of 10th month\n",
    "\n",
    "starting_date = test.iloc[0][\"scanning_date\"]\n",
    "\n",
    "forecast_LR(starting_date, planning_horizon, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652e540",
   "metadata": {},
   "source": [
    "# (Artificial) Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f68de3-0808-42cf-8014-0448dc3a15cc",
   "metadata": {},
   "source": [
    "## Grouping and Merging Event Data by Sorting Center\n",
    "The dataset is processed to group and merge event data by sorting center:\n",
    "\n",
    "1. **Group by `scanning_date` and `output_belt`:** The data is grouped based on `scanning_date` and `output_belt` for each sorting center. The total number of events (`no_of_events`) is summed for each group.\n",
    "2. **Merge with Additional Features:** The grouped data is merged with additional features such as `day`, `month`, `weekday`, `week`, `week_of_month`, and `yearday`. These features are extracted from the original dataset to retain relevant date-related information.\n",
    "3. **Split by Sorting Center:** After processing, the dataset is split into individual dataframes for each sorting center (VANTAA, LIETO, TAMPERE, SEINÄJOKI, KUOPIO, OULU) for further analysis.\n",
    "\n",
    "This process ensures that each sorting center has its own summarized dataset with date-related features intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6bca0-952b-43d6-b75a-98b7b67c8481",
   "metadata": {},
   "source": [
    "### Code Explanation: `fill_missing_events`\n",
    "\n",
    "The `fill_missing_events` function fills missing event data by:\n",
    "\n",
    "1. **Creating All Date and Output Belt Combinations**: Generates all dates from the minimum to the maximum `scanning_date` and combines them with unique `output_belt` values.\n",
    "2. **Merging with Original Data**: Merges these combinations with the original dataframe to add missing entries, setting `no_of_events` to `0` for missing values.\n",
    "3. **Adding Date Components**: Adds columns for day, month, weekday, week, week of month, and yearday for each `scanning_date`.\n",
    "\n",
    "Finally, it applies this function to all dataframes in `dfs`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d2e7b-69d9-4042-a5b3-844010dee792",
   "metadata": {},
   "source": [
    "### Code Explanation: `EventDataset` and `create_dataloaders_for_df`\n",
    "\n",
    "This code defines a PyTorch dataset and functions to split data and create data loaders.\n",
    "\n",
    "1. **`EventDataset` Class**:\n",
    "   - Initializes by loading data from a dataframe (`df`).\n",
    "   - Stores input features (`output_belt`, `day`, `weekday`, `week_of_month`, `mult`) as integer tensors and `no_of_events` as the target.\n",
    "   - `__getitem__` returns input data, targets, `output_belt`, and `yearday` for each index.\n",
    "   - `__len__` gives the dataset length.\n",
    "\n",
    "2. **`create_dataloaders_for_df` Function**:\n",
    "   - Splits `df` into training and testing sets using `split_dataset`.\n",
    "   - Creates and returns data loaders with specified `batch_size`.\n",
    "\n",
    "3. **`split_dataset` Function**:\n",
    "   - Splits `df` based on `method` (`random` or `sequential`) and `test_size` ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a9123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df  \n",
    "        one_hot_columns = [col for col in self.data.columns if col.startswith('output_belt_')]\n",
    "        feature_columns = one_hot_columns + ['day', 'weekday', 'week_of_month', 'mult']\n",
    "        self.inputs = torch.tensor(self.data[feature_columns].values.astype(int), dtype=torch.long)\n",
    "        self.targets = torch.tensor(self.data['no_of_events'].values.astype(float), dtype=torch.float32)\n",
    "        self.yearday = torch.tensor(self.data['yearday'].values.astype(int), dtype=torch.long)\n",
    "        self.output_belt = torch.tensor(self.data['output_belt'].values.astype(int), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.output_belt[idx], self.yearday[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def create_dataloaders_for_df(df, planning_horizon = 14, batch_size=128):\n",
    "    train_df = df[df[\"month\"] <= 9]\n",
    "    test_df = df[(df[\"month\"] == 10) & (df[\"day\"] <= planning_horizon)]\n",
    "    train_loader = DataLoader(EventDataset(train_df), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(EventDataset(test_df), batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d9036-35b3-4d07-bd7f-c046542d94bf",
   "metadata": {},
   "source": [
    "### Code Explanation: `SimpleNN` Neural Network\n",
    "\n",
    "The `SimpleNN` class defines a neural network with multiple fully connected layers and a custom activation function (`swish`). Here’s a breakdown of its structure:\n",
    "\n",
    "1. **Network Architecture**:\n",
    "   - `fc1` to `fc6`: Six fully connected layers with varying output dimensions.\n",
    "   - Skip connections are implemented in the `forward` method for `fc2` and `fc4`, allowing information from earlier layers to be added directly to later ones, which helps retain important features and reduces the risk of gradient vanishing.\n",
    "\n",
    "2. **Swish Activation Function**:\n",
    "   - `swish`: A custom activation function, defined as `x * sigmoid(x)`, is used to introduce non-linearity. Swish often outperforms ReLU by allowing small negative values, which can improve model performance.\n",
    "\n",
    "3. **Forward Pass**:\n",
    "   - The input passes through each layer with the Swish activation applied.\n",
    "   - After `fc1`, `fc2` and `fc4` layers use skip connections, enhancing learning by preserving important features.\n",
    "   - Finally, `fc6` outputs a single value (presumably for regression).\n",
    "\n",
    "Overall, this structure with Swish activation and skip connections is designed to improve learning efficiency and capture complex patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61558ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 2*128)\n",
    "        self.fc2 = nn.Linear(2*128, 2*128)\n",
    "        self.fc3 = nn.Linear(2*128, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 1)\n",
    "\n",
    "    def swish(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.swish(self.fc1(x))\n",
    "        x2 = self.swish(self.fc2(x1)) + x1  # Skip connection\n",
    "        x3 = self.swish(self.fc3(x2))\n",
    "        x4 = self.swish(self.fc4(x3)) + x3  # Skip connection\n",
    "        x5 = self.swish(self.fc5(x4))\n",
    "        output = self.fc6(x5)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052caf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, test_losses, sorting_center_name):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Test Loss for sorting center {}'.format(sorting_center_name))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        errors = targets - outputs\n",
    "        loss = torch.max(self.tau * errors, (self.tau - 1) * errors)\n",
    "        return loss.mean()\n",
    "tau = 0.7  # Kies een waarde voor tau (0.5 is de mediaan)\n",
    "criterion = QuantileLoss(tau=tau)\n",
    "\n",
    "def train_model(sorting_center_name, model, train_loader, test_loader, criterion, optimizer, epochs=100, patience=20, min_delta=0.1, training_time=timedelta(seconds = 0), prediction_time=timedelta(seconds = 0)):\n",
    "    # Initialize dictionaries to store daily errors for each sorting center and output belt\n",
    "    daily_errors = {}\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        start_trainning = datetime.now()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, _, _ in train_loader:\n",
    "            inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        end_training = datetime.now()\n",
    "\n",
    "        training_time += (end_training - start_trainning)\n",
    "\n",
    "        start_prediction = datetime.now()\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_mae = 0.0\n",
    "\n",
    "        # Reset daily errors for each epoch\n",
    "        daily_errors = {}\n",
    "        tests = {}\n",
    "        forecasts = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, output_belts, days in test_loader:\n",
    "                inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                mse = nn.MSELoss()(outputs, targets)\n",
    "                mae = nn.L1Loss()(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                total_mse += mse.item()\n",
    "                total_mae += mae.item()\n",
    "                \n",
    "                # Calculate and record individual errors\n",
    "                for i in range(len(targets)):\n",
    "                    actual = targets[i].item()\n",
    "                    forecast = outputs[i].item()\n",
    "                    day = days[i].item()\n",
    "                    output_belt = output_belts[i].item()\n",
    "\n",
    "                    squared_difference = (actual - forecast) ** 2\n",
    "                    absolute_difference = abs(actual - forecast)\n",
    "\n",
    "                    if day not in daily_errors:\n",
    "                        daily_errors[day] = {'mse': [], 'mae': []}\n",
    "\n",
    "                    daily_errors[day]['mse'].append(squared_difference)\n",
    "                    daily_errors[day]['mae'].append(absolute_difference)\n",
    "                \n",
    "                    if sorting_center_name == \"VANTAA\" and output_belt in [66, 109, 333]:\n",
    "                        if output_belt not in tests:\n",
    "                            tests[output_belt] = []\n",
    "                            forecasts[output_belt] = []\n",
    "                        tests[output_belt].append(actual)\n",
    "                        forecasts[output_belt].append(forecast)\n",
    "        \n",
    "        end_prediction = datetime.now()\n",
    "\n",
    "        avg_test_loss = total_loss / len(test_loader)\n",
    "        avg_test_mse = total_mse / len(test_loader)\n",
    "        avg_test_mae = total_mae / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if sorting_center_name == \"VANTAA\":\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, '\n",
    "                f'MSE: {avg_test_mse:.4f}, MAE: {avg_test_mae:.4f}, LR: {current_lr:.6f}')\n",
    "\n",
    "        mse = {}\n",
    "        mae = {}\n",
    "        for day, errors in daily_errors.items():\n",
    "            mse[day] = sum(errors[\"mse\"]) / len(errors[\"mse\"])\n",
    "            mae[day] = sum(errors[\"mae\"]) / len(errors[\"mae\"])\n",
    "\n",
    "        # Calculate MSE, VSE, and MAE metrics across days\n",
    "        MSE_avg = sum(mse.values()) / len(mse)\n",
    "        VSE = np.var(list(mse.values()), ddof=1)\n",
    "        MAE_avg = sum(mae.values()) / len(mae)\n",
    "        \n",
    "        # Early stopping op basis van de agressievere criteria\n",
    "        if epoch >= patience:\n",
    "            if avg_test_loss > min(test_losses[-patience:]) - min_delta:\n",
    "                early_stopping_counter += 1\n",
    "            else:\n",
    "                early_stopping_counter = 0    \n",
    "            if early_stopping_counter >= patience:\n",
    "                if sorting_center_name == \"VANTAA\":\n",
    "                    print(f'Early stopping na {epoch + 1} epochs.')\n",
    "                break\n",
    "    \n",
    "    prediction_time += (end_prediction - start_prediction)            \n",
    "\n",
    "    if sorting_center_name == \"VANTAA\":\n",
    "        plot_losses(train_losses, test_losses, sorting_center_name)  \n",
    "    for output_belt in tests.keys():\n",
    "        tests_outputbelt = pd.DataFrame(tests[output_belt])  # Convert Series to DataFrame\n",
    "        tests_outputbelt.columns = ['no_of_events']  # Rename the column to 'forecast\n",
    "\n",
    "        forecasts_outputbelt = pd.DataFrame(forecasts[output_belt])  # Convert Series to DataFrame\n",
    "        forecasts_outputbelt.columns = ['forecasts']  # Rename the column to 'forecast\n",
    "\n",
    "        forecasts_outputbelt['forecasts'] = forecasts_outputbelt['forecasts'].round()\n",
    "        plot_forecast(starting_date, planning_horizon, output_belt, forecasts_outputbelt, \"Neural Network\", pd.DataFrame([]), tests_outputbelt)\n",
    "\n",
    "    if sorting_center_name == \"KUOPIO\":\n",
    "        mode = \"w\"\n",
    "    else:\n",
    "        mode = \"a\"\n",
    "\n",
    "    with open(\"Results/results_NN.csv\", mode=mode, newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if mode == \"w\":\n",
    "            writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "\n",
    "        writer.writerow([sorting_center_name, MSE_avg, VSE, MAE_avg])\n",
    "\n",
    "    return model, daily_errors, training_time, prediction_time\n",
    "\n",
    "def train_NN(df):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    df_NN = df.copy()\n",
    "    df_NN['mult'] = df_NN['day'] * df_NN['weekday'] * df_NN['week_of_month']\n",
    "    df_NN['sum'] = df_NN['day'] + df_NN['weekday'] + df_NN['week_of_month']\n",
    "\n",
    "    daily_errors = {}\n",
    "    planning_horizon = 14\n",
    "\n",
    "    sorting_center_names = df_NN[\"sorting_center_name\"].unique()\n",
    "    training_time = timedelta(seconds=0)\n",
    "    prediction_time = timedelta(seconds=0)\n",
    "\n",
    "    # Directory to save loaders (make sure it exists or create it)\n",
    "    save_dir_load = \"Data/loaders NN/\"\n",
    "    save_dir_models = \"Data/models NN/\"\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        df_sorting_center = df_NN[df_NN[\"sorting_center_name\"] == sorting_center_name]\n",
    "        one_hot_belt = pd.get_dummies(df_sorting_center['output_belt'], prefix='output_belt').astype(int)\n",
    "        df_sorting_center = pd.concat([df_sorting_center, one_hot_belt], axis=1)\n",
    "\n",
    "        train_loader, test_loader = create_dataloaders_for_df(df_sorting_center, planning_horizon)\n",
    "        input_dim = next(iter(train_loader))[0].shape[1]\n",
    "        model = SimpleNN(input_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        if sorting_center_name == \"VANTAA\":\n",
    "            print(f'Training model for {sorting_center_name}')\n",
    "        model, daily_errors_sorting_center, training_time, prediction_time = train_model(\n",
    "            sorting_center_name,\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epochs=100,\n",
    "            patience=20,\n",
    "            min_delta=0.1,\n",
    "            training_time=training_time,\n",
    "            prediction_time=prediction_time\n",
    "        )\n",
    "\n",
    "        daily_errors[sorting_center_name] = daily_errors_sorting_center\n",
    "\n",
    "        # Save train and test loaders\n",
    "        torch.save(train_loader, f\"{save_dir_load}{sorting_center_name}_train_loader.pth\")\n",
    "        torch.save(test_loader, f\"{save_dir_load}{sorting_center_name}_test_loader.pth\")\n",
    "\n",
    "        # Save the model's state_dict\n",
    "        torch.save(model.state_dict(), f\"{save_dir_models}{sorting_center_name}_model.pth\")\n",
    "\n",
    "    daily_mse = {}\n",
    "    daily_mae = {}\n",
    "\n",
    "    for day in daily_errors[sorting_center_name].keys():\n",
    "        mse = 0\n",
    "        mae = 0\n",
    "        n_output_belts = 0\n",
    "        for sorting_center_name in sorting_center_names:\n",
    "            mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "            mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "            n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "        daily_mse[day] = mse / n_output_belts\n",
    "        daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "    MSE = sum(daily_mse.values()) / len(daily_mse)\n",
    "    VSE = np.var(list(daily_mse.values()), ddof=1)\n",
    "    MAE = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "    with open(\"Results/results_NN.csv\", mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"total\", MSE, VSE, MAE])\n",
    "            \n",
    "    with open(\"Results/results_general.csv\", mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        end = datetime.now()\n",
    "        running_time = end - start\n",
    "\n",
    "        writer.writerow(['Neural Network', MSE, VSE, MAE, training_time, prediction_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NN(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5427646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_data(start_date, planning_horizon, df_sorting_center, output_belt):\n",
    "    # Get unique output_belts and the index of the given output_belt for one-hot encoding\n",
    "    output_belts = sorted(df_sorting_center[\"output_belt\"].unique())\n",
    "    belt_index = np.where(output_belts == output_belt)[0][0]\n",
    "    \n",
    "    # Initialize one-hot encoded array for output_belt\n",
    "    one_hot_output_belt = np.zeros(len(output_belts))\n",
    "    one_hot_output_belt[belt_index] = 1  # Set the corresponding position to 1\n",
    "\n",
    "    x_data = []\n",
    "    current_date = datetime.strptime(start_date, \"%d-%m-%Y\")\n",
    "    end_date = current_date + timedelta(days=planning_horizon - 1)  # End date based on planning horizon\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        day = current_date.day  # Dag van de maand\n",
    "        weekday = current_date.isoweekday()  # Weekdag (1=maandag, 7=zondag)\n",
    "        \n",
    "        week_of_month = (current_date.day - 1) // 7 + 1\n",
    "        \n",
    "        # Calculate 'mult'\n",
    "        mult = day * weekday * week_of_month\n",
    "        \n",
    "        # Concatenate one-hot output belt with other features into a single flat array\n",
    "        x_data.append(np.concatenate((one_hot_output_belt, [day, weekday, week_of_month, mult])))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return np.array(x_data)\n",
    "\n",
    "def forecast_NN(starting_date, planning_horizon, df):\n",
    "    forecast_dict = {}\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # paths\n",
    "    save_dir_loaders = \"Data/loaders NN/\"\n",
    "    save_dir_models = \"Data/models NN/\"    \n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        # load loaders\n",
    "        train_loader = torch.load(f\"{save_dir_loaders}{sorting_center_name}_train_loader.pth\")\n",
    "        test_loader = torch.load(f\"{save_dir_loaders}{sorting_center_name}_test_loader.pth\")\n",
    "        input_dim = next(iter(train_loader))[0].shape[1]\n",
    "\n",
    "        # load model\n",
    "        model = SimpleNN(input_dim)  # initialize the model architecture\n",
    "        model.load_state_dict(torch.load(f\"{save_dir_models}{sorting_center_name}_model.pth\"))\n",
    "        model.eval()\n",
    "\n",
    "        df_sorting_center = df[df[\"sorting_center_name\"] == sorting_center_name]\n",
    "        output_belts = df_sorting_center[\"output_belt\"].unique()\n",
    "\n",
    "        for output_belt in output_belts:\n",
    "            # Genereer predictie data\n",
    "            x_data = generate_prediction_data(starting_date, planning_horizon, df_sorting_center, output_belt)\n",
    "            x_tensor = torch.tensor(x_data).float().to(device)\n",
    "            \n",
    "            # Maak voorspellingen met het model\n",
    "            with torch.no_grad():\n",
    "                predictions = model(x_tensor).cpu().numpy().flatten()\n",
    "\n",
    "            forecasts = pd.DataFrame(predictions)  # Convert Series to DataFrame\n",
    "            forecasts.columns = ['forecasts']  # Rename the column to 'forecast\n",
    "\n",
    "            # Round forecasts\n",
    "            forecasts['forecasts'] = forecasts['forecasts'].round()\n",
    "\n",
    "            if sorting_center_name == \"VANTAA\" and output_belt in [109]:\n",
    "                plot_forecast(starting_date, planning_horizon, output_belt, forecasts, \"NN\")\n",
    "\n",
    "            if sorting_center_name not in forecast_dict:\n",
    "                forecast_dict[sorting_center_name] = {}\n",
    "\n",
    "            forecast_dict[sorting_center_name][output_belt] = forecasts\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f4e226-e171-48ef-be35-b850383a411c",
   "metadata": {},
   "source": [
    "## Select en choose your sorting center, output belt, and days to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a00015-ba15-4904-a029-498a8a9661f5",
   "metadata": {},
   "source": [
    "# Multi warehouse model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b258d",
   "metadata": {},
   "source": [
    "#### Rural vs capital\n",
    "Model for rural sorting centers together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362a836",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da54c1",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "We need to develop a model for each output belt at every sorting center. To ensure the accuracy of our models, we will train the hyperparameters p, d an q using data from the first nine months. These hyperparameters will be saved for future forecasts.\n",
    "\n",
    "In the context of ARIMA:\n",
    "- p represents the number of lag observations included in the model (the autoregressive term).\n",
    "- d denotes the degree of differencing needed to make the time series stationary.\n",
    "- q indicates the size of the moving average window, which reflects the number of lagged forecast errors in the prediction equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "567d763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(df_output_belt):\n",
    "    # Perform Box-Cox transformation to fasten the process\n",
    "    df_output_belt[\"no_of_events_boxcox\"], lam = boxcox(df_output_belt[\"no_of_events\"])\n",
    "    df_output_belt[\"no_of_events_diff\"] = df_output_belt[\"no_of_events_boxcox\"].diff()\n",
    "    df_output_belt.dropna(inplace=True)\n",
    "\n",
    "    # Tune the model\n",
    "    tuning_model = auto_arima(df_output_belt[\"no_of_events_boxcox\"], \n",
    "                               seasonal=False, \n",
    "                               stepwise=True,  \n",
    "                               suppress_warnings=True, \n",
    "                               trace=False)\n",
    "\n",
    "    \n",
    "    p, d, q = tuning_model.order\n",
    "    return (df_output_belt[\"output_belt\"].iloc[0], p, d, q)\n",
    "\n",
    "def train_ARIMA(df):\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        df_sorting_center = df[df[\"sorting_center_name\"] == sorting_center_name]\n",
    "        df_sorting_center.drop([\"sorting_center_name\"], axis=1, inplace=True)\n",
    "        \n",
    "        hyperparameterList = []\n",
    "        output_belts = df_sorting_center[\"output_belt\"].unique()\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(tune_hyperparameters)(\n",
    "            df_sorting_center[df_sorting_center[\"output_belt\"] == output_belt]) \n",
    "            for output_belt in output_belts)\n",
    "        \n",
    "        hyperparameterList.extend(results)\n",
    "\n",
    "        hyperparameter_df = pd.DataFrame(hyperparameterList, columns=[\"output belt\", \"p\", \"d\", \"q\"])\n",
    "        hyperparameter_df.to_csv(f'Data/hyperparameters ARIMA/hyperparameters_ARIMA_{sorting_center_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb65a19",
   "metadata": {},
   "source": [
    "#### Forecast ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d567dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ARIMA per output belt\n",
    "def forecast_ARIMA(starting_date, planning_horizon, train_df, test_df = pd.DataFrame([]), running_time_training = timedelta(seconds=0)):\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "    \n",
    "    start = datetime.now()\n",
    "\n",
    "    forecast_dict = {}\n",
    "    if test_df.shape[0] > 0:\n",
    "        daily_errors = {}\n",
    "        MSE_dict = {}\n",
    "        VSE_dict = {}\n",
    "        MAE_dict = {}\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        hyperparameters = pd.read_csv(\"Data/hyperparameters ARIMA/hyperparameters_ARIMA_{}.csv\".format(sorting_center_name))\n",
    "\n",
    "        train_sorting_center = train_df[train_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        if test_df.shape[0] > 0:\n",
    "            test_sorting_center = test_df[test_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        output_belts = train_sorting_center['output_belt'].unique()\n",
    "\n",
    "        for output_belt in output_belts:\n",
    "            train_output_belt = train_sorting_center[train_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            if test_df.shape[0] == 0:\n",
    "                test_output_belt = pd.Dataframe([])\n",
    "            elif test_df.shape[0] > 0:\n",
    "                test_output_belt = test_sorting_center[test_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            p, d, q = hyperparameters[hyperparameters.iloc[:, 0] == output_belt].iloc[0, 1:4]\n",
    "\n",
    "            arima_model = ARIMA(train_output_belt[\"no_of_events\"], order=(p, d, q)).fit()\n",
    "\n",
    "            # Forecast for the planning horizon\n",
    "            forecasts = arima_model.forecast(steps=planning_horizon)\n",
    "\n",
    "            # Assuming `predictions` is your Series with the forecast values\n",
    "            forecasts = pd.DataFrame(forecasts)  # Convert Series to DataFrame\n",
    "            forecasts.columns = ['forecasts']  # Rename the column to 'forecast\n",
    "\n",
    "            if sorting_center_name == \"VANTAA\" and output_belt in [66, 109, 333]:\n",
    "                plot_forecast(starting_date, planning_horizon, output_belt, forecasts, \"ARIMA\", train_output_belt, test_output_belt)\n",
    "\n",
    "            if test_df.shape[0] == 0: \n",
    "                if sorting_center_name not in forecast_dict:\n",
    "                    forecast_dict[sorting_center_name] = {}\n",
    "                \n",
    "                forecast_dict[sorting_center_name][output_belt] = forecasts\n",
    "                \n",
    "            elif test_df.shape[0] > 0: \n",
    "                for day in range(planning_horizon):\n",
    "                    actual = test_output_belt.iloc[day][\"no_of_events\"]\n",
    "                    forecast = forecasts.iloc[day]\n",
    "\n",
    "                    squared_difference = (actual - forecast) ** 2\n",
    "                    absolute_difference = abs(actual - forecast)\n",
    "\n",
    "                    if sorting_center_name not in daily_errors:\n",
    "                        daily_errors[sorting_center_name] = {}\n",
    "                    if day not in daily_errors[sorting_center_name]:\n",
    "                        daily_errors[sorting_center_name][day] = {}\n",
    "                        daily_errors[sorting_center_name][day][\"mse\"] = []\n",
    "                        daily_errors[sorting_center_name][day][\"mae\"] = []\n",
    "                    \n",
    "                    daily_errors[sorting_center_name][day][\"mse\"].append(squared_difference)\n",
    "                    daily_errors[sorting_center_name][day][\"mae\"].append(absolute_difference)\n",
    "\n",
    "    if test_df.shape[0] > 0: \n",
    "        for sorting_center_name in sorting_center_names:\n",
    "            mse = {}\n",
    "            mae = {}\n",
    "            for day in range(planning_horizon):\n",
    "                mse[day] = sum(daily_errors[sorting_center_name][day][\"mse\"]) / len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae[day] = sum(daily_errors[sorting_center_name][day][\"mae\"]) / len(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "\n",
    "            MSE_dict[sorting_center_name] = sum(mse.values()) / len(mse)\n",
    "            VSE_dict[sorting_center_name] = np.var(list(mse.values()), ddof=1)\n",
    "            MAE_dict[sorting_center_name] = sum(mae.values()) / len(mae)\n",
    "\n",
    "        daily_mse = {}\n",
    "        daily_mae = {}\n",
    "\n",
    "        for day in range(planning_horizon):\n",
    "            mse = 0\n",
    "            mae = 0\n",
    "            n_output_belts = 0\n",
    "            for sorting_center_name in sorting_center_names:\n",
    "                mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "                n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "            daily_mse[day] = mse / n_output_belts\n",
    "            daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "        MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "        VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "        MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "        with open(\"Results/results_ARIMA.csv\", mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "            \n",
    "            for key in MSE_dict.keys():\n",
    "                # Extract the first value from each Series\n",
    "                mse_value = MSE_dict[key].values[0] if isinstance(MSE_dict[key], pd.Series) else MSE_dict[key]\n",
    "                vse_value = VSE_dict[key].values[0] if isinstance(VSE_dict[key], pd.Series) else VSE_dict[key]\n",
    "                mae_value = MAE_dict[key].values[0] if isinstance(MAE_dict[key], pd.Series) else MAE_dict[key]\n",
    "                \n",
    "                writer.writerow([key, mse_value, vse_value, mae_value])\n",
    "\n",
    "        with open(\"Results/results_general.csv\", mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            end = datetime.now()\n",
    "\n",
    "            mse_value = MSE_dict[\"total\"].values[0] if isinstance(MSE_dict[\"total\"], pd.Series) else MSE_dict[\"total\"]\n",
    "            vse_value = VSE_dict[\"total\"].values[0] if isinstance(VSE_dict[\"total\"], pd.Series) else VSE_dict[\"total\"]\n",
    "            mae_value = MAE_dict[\"total\"].values[0] if isinstance(MAE_dict[\"total\"], pd.Series) else MAE_dict[\"total\"]\n",
    "            running_time = end - start\n",
    "\n",
    "            writer.writerow(['ARIMA', mse_value, vse_value, mae_value, running_time_training, running_time])\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ce856",
   "metadata": {},
   "source": [
    "### Predictions ARIMA model\n",
    "Make forecasts for all sorting centers and report the important KPIs to evaluate the performance. These are the mean squared error (MSE), variance squared error (VSE) and mean absolute error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ARIMA = df.copy()\n",
    "\n",
    "planning_horizon = 14\n",
    "\n",
    "train = df_ARIMA[df_ARIMA[\"month\"] <= 9] # Train on first 9 months\n",
    "test = df_ARIMA[(df_ARIMA[\"month\"] == 10) & (df_ARIMA[\"day\"] <= planning_horizon)] # Test on first two weeks of 10th month\n",
    "\n",
    "starting_date = \"01-10-2023\"\n",
    "\n",
    "start = datetime.now()\n",
    "train_ARIMA(train)\n",
    "end = datetime.now()\n",
    "forecast_ARIMA(starting_date, planning_horizon, train, test, end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11164e7d",
   "metadata": {},
   "source": [
    "## LSTM/GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3068bf0",
   "metadata": {},
   "source": [
    "Because of the sparse data, engineering features to improve the model on can pose a challenge. For this reason, a new method was proposed using wave decomposition. One of the ways to do this, is through CEEMDAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2d788",
   "metadata": {},
   "source": [
    "#### CEEMDAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c37506",
   "metadata": {},
   "source": [
    "CEEMDAN, or Complete Ensemble Empirical Mode Decomposition with Adaptive Noise, is a signal processing technique used to analyze non-linear and non-stationary time series data. It decomposes a signal into components known as Intrinsic Mode Functions (IMFs), which can then be used as features to train various types of Recurrent Neural Networks (RNNs), such as LSTM and GRU.\n",
    "\n",
    "CEEMDAN combines two key techniques:\n",
    "- Empirical Mode Decomposition (EMD): Breaks down signals into IMFs without assuming linearity or stationarity.\n",
    "- Ensemble Empirical Mode Decomposition (EEMD): Improves EMD by adding white noise to the signal to reduce mode mixing.\n",
    "\n",
    "By adding adaptive noise and white noise, CEEMDAN extracts purer frequencies from the signal, enhancing the precision and robustness of the decomposition.\n",
    "\n",
    "This technique shows promise for our goals of feature extraction from limited data. By decomposing the signal, we can detect patterns like seasonality and outliers directly from the data itself, without relying on biased inputs. Since CEEMDAN is often used in forecasting unpredictable time series, such as weather, wind speeds, and stock markets, we are exploring its potential for improving package prediction in our system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fee20f",
   "metadata": {},
   "source": [
    "There are multiple methods of deploying the IMF's as vector/matrix inputs:\n",
    "\n",
    "| Forecast Method     | Description                                                                                      |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------|\n",
    "| Single Method       | Use Keras model to directly forecast with vector input                                           |\n",
    "| Ensemble Method     | Use decomposition-integration Keras model to directly forecast with matrix input                 |\n",
    "| Respective Method   | Use decomposition-integration Keras model to respectively forecast each IMF with vector input    |\n",
    "| Hybrid Method       | Use the ensemble method to forecast high-frequency IMF and the respective method for other IMFs. |\n",
    "| Multiple Method     | Multiple runs of the above method                                                                |\n",
    "| Rolling Method      | Rolling run of the above method to avoid the look-ahead bias, but takes a long time              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3f5bf",
   "metadata": {},
   "source": [
    "The data being used is aggregated, because the method decomposes the signal into multiple Intrinsic Mode Functions (IMFs). The model will than be trained on each IMF. This results in long computation times for making predictions —about two minutes for GRU and LSTM models, even with plotting functions disabled. Applying this approach to each output belt is not feasible. Later, we will try to use a similar method but we will be employing parralel computing to decrease the computation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data\n",
    "adf = df.groupby(['scanning_date', 'sorting_center_name'])['no_of_events'].sum().reset_index()\n",
    "\n",
    "# Extracting day, month, weekday, week, and yearday features\n",
    "adf['day'] = adf['scanning_date'].dt.day\n",
    "adf['month'] = adf['scanning_date'].dt.month\n",
    "adf['weekday'] = adf['scanning_date'].dt.day_of_week + 1\n",
    "adf['week'] = adf['scanning_date'].dt.day_of_year // 7 + 1\n",
    "adf['yearday'] = adf['scanning_date'].dt.day_of_year\n",
    "\n",
    "# Adding sinusoidal and cosinusoidal transformations for yearday\n",
    "adf['yearday_sin'] = np.sin(adf['weekday'] / 7 * 2 * np.pi)\n",
    "adf['yearday_cos'] = np.cos(adf['weekday'] / 7 * 2 * np.pi)\n",
    "\n",
    "# Filtering data for 'VANTAA' sorting center and setting the index\n",
    "data = adf[adf['sorting_center_name'] == 'VANTAA'].set_index('scanning_date')\n",
    "data = data['no_of_events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b13b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM, 14 days prediction\n",
    "cl.statis_tests(data)\n",
    "kr = cl.keras_predictor(\n",
    "    FORECAST_HORIZONS=14,\n",
    "    FORECAST_LENGTH=14,\n",
    "    KERAS_MODEL='LSTM',\n",
    "    DECOM_MODE='CEEMDAN',\n",
    "    DAY_AHEAD=1,\n",
    "    NOR_METHOD='minmax',\n",
    "    FIT_METHOD='add',\n",
    "    REDECOM_LIST={'co-imf0': 'ovmd'}\n",
    ")\n",
    "df_result = kr.hybrid_keras_predict(data=data, show=True, plot=True, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e12656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU model, 14 days prediction using CEEMDAN decomposition and min-max normalization\n",
    "# Assumes 'cl' is a class instance with statistical testing and prediction methods\n",
    "\n",
    "# Perform statistical tests on the data\n",
    "cl.statis_tests(data)\n",
    "\n",
    "# Initialize the keras predictor with specified parameters\n",
    "kr = cl.keras_predictor(\n",
    "    FORECAST_HORIZONS=14,\n",
    "    FORECAST_LENGTH=14,\n",
    "    KERAS_MODEL='GRU',\n",
    "    DECOM_MODE='CEEMDAN',\n",
    "    DAY_AHEAD=1,\n",
    "    NOR_METHOD='minmax',\n",
    "    FIT_METHOD='add',\n",
    "    REDECOM_LIST={'co-imf0': 'ovmd'}\n",
    ")\n",
    "\n",
    "# Run the hybrid Keras prediction and store the results in df_result\n",
    "df_result = kr.hybrid_keras_predict(data=data, show=True, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289319e9",
   "metadata": {},
   "source": [
    "#### Parralel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476482d",
   "metadata": {},
   "source": [
    "We were inspired by the success of CEEMDAN (Complete Ensemble Empirical Mode Decomposition with Adaptive Noise) in improving clustering models. However, while CEEMDAN is well-suited for aggregated time-series analysis, it lacks the granularity required for predicting outputs at the level of each output belt within each sorting center. Therefore, we adapted its core decomposition philosophy to suit our specific needs, allowing us to improve the computational efficiency by enabling parallel processing across belts. This adaptation not only drew upon the strength of CEEMDAN in decomposing non-stationary signals but also introduced modifications that better accommodate the unique characteristics of our problem—specifically the need to process data for multiple output belts concurrently. Our approach ensures each output belt can be analyzed individually while maintaining computational efficiency.\n",
    "\n",
    "To address the challenge of predicting sparse time-series data in this project, we designed a predictive approach combining GRU and signal decomposition techniques. The objective of the project is to predict event counts for different conveyor belts at each sorting center. Due to data sparsity and the presence of outliers, traditional time-series methods struggle to effectively model the behavior. In this context, we chose the GRU (Gated Recurrent Unit) model as it effectively captures both short- and long-term dependencies with fewer parameters, making it more suitable for training on sparse data compared to more complex LSTM models, which may be prone to overfitting in this scenario. To enhance the model's generalization ability and extract latent patterns from the data, we applied signal decomposition techniques—using both wavelet transform and Empirical Mode Decomposition (EMD). This was done to provide clearer learning inputs for the GRU, enhancing its ability to handle the peculiarities of the dataset.\n",
    "\n",
    "In the signal processing step, the wavelet transform was utilized to decompose each conveyor belt's signal into different frequency components, thereby extracting richer information in both time and frequency domains. Empirical Mode Decomposition (EMD) was used to decompose the signal into intrinsic mode functions (IMFs), making it particularly effective at capturing nonlinear and non-stationary signal characteristics. By employing this hybrid signal decomposition strategy, the model can avoid being misled by outliers within the sparse data, extracting the most useful features for predicting event counts. We also employed data augmentation techniques by adding random shifts and introducing synthetic outliers to increase data volume, thereby enhancing the robustness and generalizability of the model. In the final setup, each sorting center's conveyor belts were modeled individually, ensuring the accuracy of the prediction to align with the unique dynamic patterns of each belt.\n",
    "\n",
    "To effectively train these models, we employed a strategy combining GRU with specific optimization techniques. The optimization process leveraged Stochastic Gradient Descent (SGD) with momentum, which helps accelerate convergence and reduce the influence of outliers during training. Learning rates were dynamically adjusted using a cosine annealing scheduler to ensure that the model finds a global optimum effectively during the latter part of training. For each sorting center's conveyor belts, we independently calculated the Mean Squared Error (MSE) and Variance of Squared Error (VSE) to assess the model's overall performance and its consistency across different time periods. With this comprehensive approach, we successfully developed custom predictive models for each sorting center, effectively addressing the challenges posed by the sparse data and achieving accurate event count predictions for each output belt.\n",
    "\n",
    "Despite this, there remain some challenges due to the inherent sparsity of the data and limitations in sampling duration. The average data volume per conveyor belt within each sorting center is relatively low, which makes it difficult for the model to capture complex dynamic features. Furthermore, the current dataset only spans one year, making it challenging to effectively capture seasonality and other cyclical patterns. Thus, while this model utilizes a variety of signal processing and optimization techniques, its predictive performance may be somewhat limited by the available data. This suggests that future improvements—such as expanding data volume and gathering multi-year data—could significantly enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69265f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Define hyperparameters\n",
    "sequence_length = 15\n",
    "hidden_channels = 16\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Create GRU input sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        Y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Wavelet-based decomposition function (GPU-friendly)\n",
    "def decompose_signal_wavelet(signal, wavelets=['db1', 'haar', 'sym5'], level=None):\n",
    "    if level is None:\n",
    "        level = min(1, int(np.log2(len(signal))))  # Dynamic level selection\n",
    "    coeffs_list = []\n",
    "    for wavelet in wavelets:\n",
    "        coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "        coeffs = [torch.tensor(c, dtype=torch.float32).to(device) for c in coeffs]\n",
    "        coeffs_list.extend(coeffs)\n",
    "    return coeffs_list\n",
    "\n",
    "# Empirical Mode Decomposition (EMD) based decomposition\n",
    "def decompose_signal_emd(signal):\n",
    "    emd = EMD()  # Initialize EMD class\n",
    "    imfs = emd.emd(signal)  # Perform EMD decomposition\n",
    "    imfs = [torch.tensor(imf, dtype=torch.float32).to(device) for imf in imfs]\n",
    "    return imfs\n",
    "\n",
    "# GRU model for time series\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Data augmentation for sparse data\n",
    "def augment_data(signal, num_augments=10, noise_level=0.02):\n",
    "    augmented_signals = [signal]\n",
    "    for _ in range(num_augments):\n",
    "        noise = noise_level * np.random.randn(len(signal))\n",
    "        shifted_signal = np.roll(signal, shift=np.random.randint(-5, 5))\n",
    "        augmented_signal = shifted_signal + noise\n",
    "        augmented_signals.append(augmented_signal)\n",
    "    return np.array(augmented_signals)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "'''df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "df = df.sort_values(by='scanning_date').reset_index(drop=True)\n",
    "\n",
    "# Train/Test split\n",
    "train_size = int(0.75 * len(df))\n",
    "test_size = int(0.05 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:train_size + test_size]\n",
    "\n",
    "# Data preprocessing\n",
    "feature_columns = ['day', 'month', 'weekday', 'week', 'weekday_sin', 'weekday_cos']\n",
    "\n",
    "# Fill missing values with median values for robustness\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "train_df[feature_columns] = train_df[feature_columns].fillna(train_df[feature_columns].median(numeric_only=True))\n",
    "test_df[feature_columns] = test_df[feature_columns].fillna(test_df[feature_columns].median(numeric_only=True))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90804063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GRU = df.copy()\n",
    "\n",
    "planning_horizon = 14\n",
    "starting_date = \"01-10-2023\"\n",
    "\n",
    "train_df = df_GRU[df_GRU[\"month\"] <= 9]\n",
    "test_df = df_GRU[(df_GRU[\"month\"] == 10) & (df_GRU[\"day\"] <= planning_horizon)]\n",
    "\n",
    "# Data preprocessing\n",
    "feature_columns = ['day', 'month', 'weekday', 'week', 'weekday_sin', 'weekday_cos']\n",
    "\n",
    "# Fit scalers with valid data only\n",
    "scaler_features = RobustScaler()\n",
    "train_features = scaler_features.fit_transform(train_df[feature_columns])\n",
    "test_features = scaler_features.transform(test_df[feature_columns])\n",
    "\n",
    "scaler_target = StandardScaler()\n",
    "train_targets = scaler_target.fit_transform(train_df['no_of_events'].values.reshape(-1, 1))\n",
    "test_targets = scaler_target.transform(test_df['no_of_events'].values.reshape(-1, 1))\n",
    "\n",
    "# Decompose each output belt's signal using Wavelet and EMD-based method\n",
    "output_belt_ids = train_df['output_belt'].unique()\n",
    "imfs_dict = {}\n",
    "for belt in output_belt_ids:\n",
    "    belt_signal = train_df[train_df['output_belt'] == belt]['no_of_events'].values\n",
    "    if len(belt_signal) == 0:\n",
    "        continue\n",
    "    augmented_signals = augment_data(belt_signal)\n",
    "    imfs = []\n",
    "    for augmented_signal in augmented_signals:\n",
    "        # Apply both Wavelet and EMD decomposition\n",
    "        wavelet_coeffs = decompose_signal_wavelet(augmented_signal)\n",
    "        emd_imfs = decompose_signal_emd(augmented_signal)\n",
    "        imfs.extend(wavelet_coeffs)\n",
    "        imfs.extend(emd_imfs)\n",
    "    imfs_dict[belt] = imfs\n",
    "\n",
    "# Assign sorting centers to belts and create models for each sorting center\n",
    "sorting_center_models = {}\n",
    "sorting_center_belts = train_df.groupby('sorting_center_name')['output_belt'].unique().to_dict()\n",
    "\n",
    "# Create GRU models for each sorting center\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    if sorting_center not in sorting_center_models:\n",
    "        model = GRUModel(input_size=1, hidden_size=hidden_channels, num_layers=2, output_size=1, dropout_rate=dropout_rate).to(device)\n",
    "        sorting_center_models[sorting_center] = model\n",
    "\n",
    "# Training loop for each sorting center model\n",
    "belt_predictions_actuals = {}\n",
    "\n",
    "daily_errors = {}\n",
    "MSE_dict = {}\n",
    "VSE_dict = {}\n",
    "MAE_dict = {}\n",
    "\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    model = sorting_center_models[sorting_center]\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    all_X_train, all_Y_train = [], []\n",
    "    belt_losses = {}\n",
    "    for belt in belts:\n",
    "        imfs = imfs_dict.get(belt, [])\n",
    "        belt_losses[belt] = []  # Ensure initialization of belt_losses before using it\n",
    "        for imf in imfs:\n",
    "            X_train, Y_train = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "            if len(X_train) > 0:\n",
    "                all_X_train.append(X_train)\n",
    "                all_Y_train.append(Y_train)\n",
    "\n",
    "    if len(all_X_train) > 0:\n",
    "        X_train_tensor = torch.tensor(np.concatenate(all_X_train), dtype=torch.float32).unsqueeze(2)\n",
    "        Y_train_tensor = torch.tensor(np.concatenate(all_Y_train), dtype=torch.float32).unsqueeze(1)\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_Y in train_loader:\n",
    "                batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = nn.MSELoss()(outputs.view(-1), batch_Y.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Step scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Sorting Center: {sorting_center}, Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Store predictions and actuals for each belt\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for belt in belts:\n",
    "                imfs = imfs_dict.get(belt, [])\n",
    "                belt_predictions = []\n",
    "                belt_actuals = []\n",
    "\n",
    "                for imf in imfs:\n",
    "                    X_test, Y_test = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "                    if len(X_test) > 0:\n",
    "                        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(2).to(device)\n",
    "                        Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                        predictions = model(X_test_tensor).cpu().numpy().flatten()\n",
    "                        actuals = Y_test_tensor.cpu().numpy().flatten()\n",
    "\n",
    "                        #belt_predictions.extend(predictions)\n",
    "                        #belt_actuals.extend(actuals)\n",
    "\n",
    "                        squared_difference = (actuals - predictions) ** 2\n",
    "                        absolute_difference = abs(actuals - predictions)\n",
    "\n",
    "                        if sorting_center not in daily_errors:\n",
    "                            daily_errors[sorting_center] = {}\n",
    "                        if day not in daily_errors[sorting_center]:\n",
    "                            daily_errors[sorting_center][day] = {}\n",
    "                            daily_errors[sorting_center][day][\"mse\"] = []\n",
    "                            daily_errors[sorting_center][day][\"mae\"] = []\n",
    "                        \n",
    "                        daily_errors[sorting_center][day][\"mse\"].append(squared_difference)\n",
    "                        daily_errors[sorting_center][day][\"mae\"].append(absolute_difference)\n",
    "\n",
    "                # Store for KPI calculation\n",
    "                belt_predictions_actuals[belt] = {'predictions': belt_predictions, 'actuals': belt_actuals}\n",
    "\n",
    "# Calculate KPIs for each sorting center\n",
    "'''sorting_center_metrics = {}\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    daily_errors = {}\n",
    "    for belt in belts:\n",
    "        if belt not in belt_predictions_actuals:\n",
    "            continue\n",
    "        predictions = belt_predictions_actuals[belt]['predictions']\n",
    "        actuals = belt_predictions_actuals[belt]['actuals']\n",
    "        scanning_dates = test_df['scanning_date'][sequence_length:sequence_length + len(predictions)]\n",
    "\n",
    "        # Calculate squared deviation for each day\n",
    "        for date, actual, predicted in zip(scanning_dates, actuals, predictions):\n",
    "            daily_errors.setdefault(date, []).append((actual - predicted) ** 2)\n",
    "\n",
    "    # Calculate overall KPI for sorting center\n",
    "    daily_rmse = {date: np.sqrt(np.mean(errors)) for date, errors in daily_errors.items()}\n",
    "    sorting_center_metrics[sorting_center] = {'daily_rmse': daily_rmse, 'mean_rmse': np.mean(list(daily_rmse.values()))}\n",
    "\n",
    "print(sorting_center_metrics)'''\n",
    "\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    mse = {}\n",
    "    mae = {}\n",
    "    for day in range(planning_horizon):\n",
    "        mse[day] = sum(daily_errors[sorting_center][day][\"mse\"]) / len(daily_errors[sorting_center][day][\"mse\"])\n",
    "        mae[day] = sum(daily_errors[sorting_center][day][\"mae\"]) / len(daily_errors[sorting_center][day][\"mae\"])\n",
    "\n",
    "    MSE_dict[sorting_center] = sum(mse.values()) / len(mse)\n",
    "    VSE_dict[sorting_center] = np.var(list(mse.values()), ddof=1)\n",
    "    MAE_dict[sorting_center] = sum(mae.values()) / len(mae)\n",
    "\n",
    "daily_mse = {}\n",
    "daily_mae = {}\n",
    "\n",
    "for day in range(planning_horizon):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    n_output_belts = 0\n",
    "    for sorting_center, belts in sorting_center_belts.items():\n",
    "        mse += sum(daily_errors[sorting_center][day][\"mse\"])\n",
    "        mae += sum(daily_errors[sorting_center][day][\"mae\"])\n",
    "        n_output_belts += len(daily_errors[sorting_center][day][\"mse\"])\n",
    "    daily_mse[day] = mse / n_output_belts\n",
    "    daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "with open(\"Results/results_CEEMDAN.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "    \n",
    "    for key in MSE_dict.keys():\n",
    "        # Extract the first value from each Series\n",
    "        mse_value = MSE_dict[key].values[0] if isinstance(MSE_dict[key], pd.Series) else MSE_dict[key]\n",
    "        vse_value = VSE_dict[key].values[0] if isinstance(VSE_dict[key], pd.Series) else VSE_dict[key]\n",
    "        mae_value = MAE_dict[key].values[0] if isinstance(MAE_dict[key], pd.Series) else MAE_dict[key]\n",
    "        \n",
    "        writer.writerow([key, mse_value, vse_value, mae_value])\n",
    "\n",
    "with open(\"Results/results_general.csv\", mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    end = datetime.now()\n",
    "\n",
    "    mse_value = MSE_dict[\"total\"].values[0] if isinstance(MSE_dict[\"total\"], pd.Series) else MSE_dict[\"total\"]\n",
    "    vse_value = VSE_dict[\"total\"].values[0] if isinstance(VSE_dict[\"total\"], pd.Series) else VSE_dict[\"total\"]\n",
    "    mae_value = MAE_dict[\"total\"].values[0] if isinstance(MAE_dict[\"total\"], pd.Series) else MAE_dict[\"total\"]\n",
    "    running_time = end - start\n",
    "\n",
    "    writer.writerow(['CEEMDAN', mse_value, vse_value, mae_value, running_time, \"-\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d838e3",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eddb0d",
   "metadata": {},
   "source": [
    "Comparison table with all benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56ac3e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Write about pro and cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cc71b",
   "metadata": {},
   "source": [
    "## Dashboard forecast (Tom)\n",
    "Standardize input + output\n",
    "Make accessible for forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62352c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training functions for all models such that the training can be saved and used for prediction every time when needed\n",
    "\n",
    "def train_model(data, model):\n",
    "    data = prepare_data(data) # Ensure that data is ready for training\n",
    "\n",
    "    if model == \"Linear Regression\":\n",
    "        # Linear Regression model does not need to be trained\n",
    "        print(\"Training is finished\")\n",
    "    elif model == \"ARIMA\":\n",
    "        train_ARIMA(data)\n",
    "        print(\"Training is finished\")\n",
    "    elif model == \"Neural Network\":\n",
    "        train_NN(data)\n",
    "        print(\"Training is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast(start_date, planning_horizon, data, model):\n",
    "    if model == \"Linear Regression\":\n",
    "        forecast = forecast_LR(start_date, planning_horizon, data)        \n",
    "        print(\"Forecast finished\")\n",
    "    elif model == \"ARIMA\":\n",
    "        forecast = forecast_ARIMA(start_date, planning_horizon, data)\n",
    "        print(\"Forecast finished\")\n",
    "    elif model == \"Neural Network\":\n",
    "        forecast = forecast_NN(start_date, planning_horizon, data)\n",
    "        print(\"Forecast finished\")\n",
    "        \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddaece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forecast(start_date, planning_horizon, model, forecast):\n",
    "    for sorting_center_name, output_belts in forecast.items():\n",
    "        # Define the file name\n",
    "        file_name = f\"Results/forecast_{model}_{sorting_center_name}.csv\"\n",
    "        \n",
    "        # Open the file for writing\n",
    "        with open(file_name, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # Construct the list of dates for the forecast period\n",
    "            dates = [(start_date + timedelta(days=day)).strftime(\"%Y-%m-%d\") for day in range(planning_horizon)]\n",
    "            \n",
    "            # Write the header row (Dates)\n",
    "            header = ['Output Belt'] + dates\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Write the forecast data\n",
    "            for output_belt, forecasts in output_belts.items():\n",
    "                row = [output_belt] + forecasts.tolist()  # Convert forecasts to list if it's a NumPy array\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(\"Forecasts successfully written to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(start_date, planning_horizon, data, model, train_indicator = False):\n",
    "    if train_indicator == True:\n",
    "        train_model(data, model)\n",
    "\n",
    "    start = datetime.now()\n",
    "    forecast = make_forecast(start_date, planning_horizon, data, model)\n",
    "    end = datetime.now()\n",
    "    save_forecast(start_date, planning_horizon, forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "directory_path = os.getcwd() + \"\\\\Data\\\\sorting_event_volumes_2023.csv\"\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Input\n",
    "start_date = \"01-10-2023\"       # When does the planning horizon start\n",
    "planning_horizon = 14           # How long is the planning horizon\n",
    "data = df[df[\"month\"] <= 9]     # Always needed since some models need it for prediction\n",
    "model = \"ARIMA\"                 # Which model do you want to use, options are Linear Regression, ARIMA and Neural Network \n",
    "train_indicator = False         # Optional, if you want to train the model\n",
    "\n",
    "main(start_date, planning_horizon, data, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
