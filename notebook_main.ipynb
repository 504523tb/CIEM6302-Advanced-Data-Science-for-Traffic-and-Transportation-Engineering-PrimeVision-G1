{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ce3068",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0ef35",
   "metadata": {},
   "source": [
    "\n",
    "1. How does the data look like?\n",
    "   - a.) What does the raw data look like (variables, structure, etc.)?\n",
    "   - b.) What correlations or relationships exist between variables in the data (e.g., between time of day, distribution center, and package volume)?\n",
    "   - c.) How should the data be prepared and cleaned (e.g., handling missing values, duplicates, and inconsistencies)?\n",
    "   - d.) What are the daily frequencies of package deliveries, and are there seasonal patterns (e.g., daily, weekly, or monthly trends)?\n",
    "   - e.) How do the distribution centers differ from each other in terms of package volume and other key metrics \\[‘VANTAA’, ‘KUOPIO’, ‘OULU’, ‘SEINÄJOKI’, ‘TAMPERE’\\]?\n",
    "   - f.) How many outliers are present, and how do we identify them (e.g., through statistical or machine learning methods)?\n",
    "   - g.) What is the statistical distribution of the data points (e.g., normal, skewed, or multimodal)?\n",
    "   - h.) How do holidays and special events impact the volume of packages?\n",
    "\n",
    "2. What models can potentially yield effective modeling results?\n",
    "   - a.) Linear Regression Model (with benchmarks MSE, VSE, MAE, and running time)\n",
    "   - b.) (Artificial) Neural Network (with benchmarks MSE, VSE, MAE, and running time)\n",
    "   - c.) ARIMA (with benchmarks MSE, VSE, MAE, and running time)\n",
    "   - d.) LSTM/GRU (with benchmarks MSE, VSE, MAE, and running time)\n",
    "\n",
    "After each sub-question is explored, a final results and conclusion section will summarize the results. A dashboard will then be created to streamline testing all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075eb04",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8551110e7f3394cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:18.271259200Z",
     "start_time": "2024-10-06T20:21:14.977256400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and packages\n",
    "import CEEMDAN_LSTM as cl\n",
    "import csv\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import folium\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "#from EMD import EMD\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pmdarima as pm\n",
    "from pmdarima import auto_arima\n",
    "#from PyEMD import EMD\n",
    "import pywt\n",
    "import random\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9f920",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859069b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_events(df):\n",
    "    df_filled_list = []\n",
    "    \n",
    "    for center in df['sorting_center_name'].unique():\n",
    "        df_center = df[df['sorting_center_name'] == center]\n",
    "        output_belts = df_center['output_belt'].unique()\n",
    "        \n",
    "        min_date = df_center['scanning_date'].min()\n",
    "        max_date = df_center['scanning_date'].max()\n",
    "\n",
    "        all_dates = pd.date_range(start=pd.Timestamp(year=min_date.year, month=1, day=1), end=pd.Timestamp(year=max_date.year, month=max_date.month, day=1) + pd.offsets.MonthEnd(0))\n",
    "        \n",
    "        all_combinations = pd.MultiIndex.from_product(\n",
    "            [[center], all_dates, output_belts],\n",
    "            names=['sorting_center_name', 'scanning_date', 'output_belt']\n",
    "        )\n",
    "        \n",
    "        all_combinations_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "        df_filled_center = pd.merge(all_combinations_df, df_center, \n",
    "                                    on=['sorting_center_name', 'scanning_date', 'output_belt'], \n",
    "                                    how='left')\n",
    "        \n",
    "        df_filled_center['no_of_events'] = df_filled_center['no_of_events'].fillna(0.0001)\n",
    "        df_filled_list.append(df_filled_center)\n",
    "    \n",
    "    df_filled = pd.concat(df_filled_list, ignore_index=True)\n",
    "    \n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ef5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    # Data cleaning\n",
    "    print(\"Number of rows original dataset is: \" + str(df.shape[0]))\n",
    "\n",
    "    df = df.loc[df[\"event_type\"] == \"LAJ\", :]\n",
    "    df.drop(['event_location', 'input_belt', 'position'], axis=1, inplace = True)\n",
    "    df.dropna(inplace = True)\n",
    "    df['output_belt'] = df['output_belt'].astype(int)\n",
    "    df = df.groupby(['sorting_center_name', 'scanning_date', 'output_belt'], as_index = False)['no_of_events'].sum()\n",
    "    df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "\n",
    "    # We add 0 events for all dates without orders \n",
    "    df = fill_missing_events(df)\n",
    "\n",
    "    print(\"Number of rows cleaned dataset is: \" + str(df.shape[0]))\n",
    "\n",
    "    # Data preparation\n",
    "    df['day'] = df['scanning_date'].dt.day\n",
    "    df['month'] = df['scanning_date'].dt.month\n",
    "    df['weekday'] = df['scanning_date'].dt.dayofweek + 1\n",
    "    df['week'] = df['scanning_date'].dt.isocalendar().week\n",
    "    df['week_of_month'] = (df['day'] - 1) // 7 + 1\n",
    "    df['yearday'] = df['scanning_date'].dt.day_of_year\n",
    "    df['weekday_sin'] = np.sin(df['weekday'] / 7 * 2 * np.pi)\n",
    "    df['weekday_cos'] = np.cos(df['weekday'] / 7 * 2 * np.pi)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeeef21",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b921e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "directory_path = os.getcwd() + \"\\\\Data\\\\sorting_event_volumes_2023.csv\"\n",
    "\n",
    "df = pd.read_csv(directory_path)\n",
    "df = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c4b96",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25388a4a",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "We construct a correlation matrix to investigate which features correlate most with the number of events. This could give an indication with features would add most to prediction the number of events in the future. We observe that the correlation with number of events the day before is the strongest. Next the correlation is the strongest with output belt, weekday and yearday cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "# Add no_of_events previous day\n",
    "df_prev_yearday = df.copy()\n",
    "df_prev_yearday['yearday_yesterday'] = df_prev_yearday['yearday'] + 1  # Adjusting to match yearday - 1\n",
    "\n",
    "df_correlations = pd.merge(\n",
    "    df, \n",
    "    df_prev_yearday[['sorting_center_name', 'output_belt', 'yearday_yesterday', 'no_of_events']],  # only keep necessary columns from right\n",
    "    left_on=['sorting_center_name', 'output_belt', 'yearday'],  # match on the current 'yearday' from the left df\n",
    "    right_on=['sorting_center_name', 'output_belt', 'yearday_yesterday'],  # match on the 'yearday_yesterday' from the right df\n",
    "    how='left',  # Perform a left join to keep all rows from the left dataframe\n",
    "    suffixes=('', '_yesterday')\n",
    ")\n",
    "\n",
    "df_correlations = df_correlations.drop(columns = [\"yearday_yesterday\"])\n",
    "\n",
    "# Create correlation matrix\n",
    "target_covariance_matrix = df_correlations.drop(columns = [\"sorting_center_name\"]).corr().round(2)\n",
    "#target_covariance_matrix_events = df_correlations.drop(columns = [\"sorting_center_name\"]).corr()[[\"no_of_events\"]]\n",
    "sns.heatmap(target_covariance_matrix,annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac13f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analysis exclude the null events\n",
    "filtered_df = df[df['no_of_events'] >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596e7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:24.351015800Z",
     "start_time": "2024-10-06T20:21:21.728507200Z"
    }
   },
   "outputs": [],
   "source": [
    "event_counts = filtered_df['no_of_events'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(event_counts.index, event_counts.values)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Events (log-scale)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Number of Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3127793",
   "metadata": {},
   "source": [
    "### Sorting centers\n",
    "We have six sorting centers located in Finland. The Vantaa sorting center is situated near the capital, Helsinki, and adjacent to the Helsinki international airport. In contrast, the other centers are located in smaller, more rural cities. A violin plot depicting the distribution of total daily events at each sorting center reveals significantly higher volumes at Vantaa, while the other centers show comparable levels. This prompts further investigation into the similarities among these sorting centers, specifically regarding the potential for predicting trends between urban and rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "locations[\"KUOPIO\"] = (62.894785, 27.666586)\n",
    "locations[\"LIETO\"] = (60.503899,22.4322587)\n",
    "locations[\"OULU\"] = (65.007693, 25.471332)\n",
    "locations[\"SEINÄJOKI\"] = (62.7902205,22.7469712)\n",
    "locations[\"TAMPERE\"] = (61.499625, 23.751267)\n",
    "locations[\"VANTAA\"] = (60.294196, 25.035834)\n",
    "\n",
    "avg_lat = sum(lat for lat, _ in locations.values()) / len(locations)\n",
    "avg_lng = sum(lng for _, lng in locations.values()) / len(locations)\n",
    "m = folium.Map(location=[avg_lat, avg_lng], zoom_start=6, tiles='CartoDB positron')\n",
    "\n",
    "# Step 1: Add markers for each sorting center and track bounds\n",
    "bounds = []\n",
    "\n",
    "# Step 2: Add markers for each sorting center\n",
    "for name, (lat, lng) in locations.items():\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=folium.DivIcon(\n",
    "            html='<div style=\"background-color: #C50000; width: 12px; height: 12px; border: none;\"></div>'\n",
    "        ),\n",
    "        popup=name  # Display the sorting center name on click\n",
    "    ).add_to(m)\n",
    "    # Add the current location to bounds\n",
    "    bounds.append((lat, lng))\n",
    "\n",
    "# Step 2: Fit the map bounds to show all markers\n",
    "m.fit_bounds(bounds)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1523837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(df, period = \"yearday\"):\n",
    "    if period == \"yearday\":\n",
    "        df_period = df.groupby([\"yearday\"])[\"no_of_events\"].sum().reset_index()\n",
    "    else:\n",
    "        df_period = df.groupby([\"yearday\", period])[\"no_of_events\"].sum().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if period == \"yearday\":\n",
    "        sns.violinplot(y=\"no_of_events\", data=df_period)\n",
    "    else:\n",
    "        sns.violinplot(x = period, y=\"no_of_events\", data=df_period)\n",
    "\n",
    "    if period == \"sorting_center_name\":\n",
    "        period = \"sorting center\"\n",
    "    plt.xlabel(\"{}\".format(period).capitalize())\n",
    "    plt.ylabel(\"Number of Events\")\n",
    "    plt.title(\"Violin Plot of Number of Events by {}\".format(period).capitalize())\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc61ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly per sorting center\n",
    "plot_violin(df, \"sorting_center_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c072b9",
   "metadata": {},
   "source": [
    "### Outlier analysis\n",
    "The violin plots indicate that all sorting centers have extreme outliers in the number of events compared to their mean. In the following analysis, we will further explore the variability of the number of events across these centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(filtered_df, \n",
    "                x='no_of_events', \n",
    "                y='sorting_center_name', \n",
    "                color='sorting_center_name',\n",
    "                title='Spread of Number of Events by Sorting Center',\n",
    "                labels={'no_of_events': 'Number of Events', 'sorting_center_name': 'Sorting Center'},\n",
    "                stripmode='overlay')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74969f",
   "metadata": {},
   "source": [
    "We observed that most outliers occur at VANTAA. Specifically, there are 284 instances where the number of events exceeds 5000 and 234 instances where it surpasses 10,000. However, no particular day stands out, as the most frequent day occurs only 4 times. Interestingly, output belt 109 accounts for 254 days with over 5000 events and 233 days with over 10,000 events. This means only one other instance of 10,000+ events happens on a different belt, suggesting that the identified outliers reflect consistently high event volumes on belt 109 rather than sporadic spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bac994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier analysis\n",
    "df_VANTAA_oultiers = df[(df[\"sorting_center_name\"] == \"VANTAA\") & (df[\"no_of_events\"] > 5000)]\n",
    "print(df_VANTAA_oultiers.shape[0])\n",
    "\n",
    "df_VANTAA_oultiers[\"yearday\"].value_counts()\n",
    "df_VANTAA_oultiers[\"output_belt\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3fa96",
   "metadata": {},
   "source": [
    "## Data visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dc58f",
   "metadata": {},
   "source": [
    "### Aggregate demand planning horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15255b0",
   "metadata": {},
   "source": [
    "We will focus on the sorting center VANTAA when creating data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e6bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting center is VANTAA\n",
    "df_VANTAA = df[df[\"sorting_center_name\"] == \"VANTAA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb001792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T20:21:28.415162Z",
     "start_time": "2024-10-06T20:21:27.433040300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate demand planning horizon (Mats)\n",
    "def total_orders_per_period(df, period):\n",
    "    totals = df.groupby(period)['no_of_events'].sum()\n",
    "\n",
    "    timeframe = \"\"\n",
    "\n",
    "    if period == \"day\":\n",
    "        timeframe = \"month\"\n",
    "        \n",
    "        valid_months = {day: 12 for day in range(1, 29)}\n",
    "        valid_months[29] = 11\n",
    "        valid_months[30] = 11\n",
    "        valid_months[31] = 7\n",
    "\n",
    "        year = df.iloc[0][\"scanning_date\"].year\n",
    "        if (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)):\n",
    "            valid_months[29] = 12\n",
    "\n",
    "        for index in range(len(totals)):\n",
    "            totals[index+1] = totals[index+1] / valid_months[index+1]\n",
    "    elif period == \"weekday\":\n",
    "        timeframe = \"week\"\n",
    "    elif period == \"week\":\n",
    "        timeframe = \"year\"\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(totals.index, totals.values, marker='o')\n",
    "    plt.xlabel('{}'.format(period).capitalize())\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.ylim(0, 1.2 * totals.max())\n",
    "    plt.title('Number of events in a {} over the {}'.format(period, timeframe))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dd5b9",
   "metadata": {},
   "source": [
    "### Demand over the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d18bd7",
   "metadata": {},
   "source": [
    "Below we show the total number of events for each week over the planning horizon of a year. We observe that the demand is relatively stable over the year except of a sharp increase in the last weeks of the year. This is probably due to Black Friday at the end of November and Christmas in December. Furthermore we observe that there are 5 days for which all sorting centers are closed. These are April 9 (Easter Sunday), June 24 (Midsummer Holiday), December 24 (Christmas Eve), December 25 (Christmas), December 31 (New Years')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a610bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orders_per_period(df_VANTAA, \"week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35729f",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc94f9e",
   "metadata": {},
   "source": [
    "Below, we present violin plots showing demand distribution over a year, broken down by month and weekday. Overall, daily demand typically ranges between 50,000 and 160,000, with a high density around 150,000.\n",
    "\n",
    "The monthly plots reveal some interesting patterns. December's average demand is lower than most months, likely due to the reduced activity between Christmas and New Year, which is also evident in the planning horizon plot. Notably, demand in November and December fluctuates significantly, as shown by the peaks in the violin plots, while March and August show a relatively narrow range.\n",
    "\n",
    "Looking at the weekday patterns, demand during the workweek is noticeably higher than on weekends, with Sunday showing almost no demand. Tuesday consistently has the highest demand during the week, while Friday has the lowest. This weekday pattern suggests a cyclical trend, resembling a sine or cosine wave. Incorporating such patterns could maybe enhance the accuracy of demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly\n",
    "plot_violin(df_VANTAA)\n",
    "\n",
    "# Monthly\n",
    "plot_violin(df_VANTAA, \"month\")\n",
    "\n",
    "# Weekday\n",
    "plot_violin(df_VANTAA, \"weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f569b",
   "metadata": {},
   "source": [
    "### Day of the month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b17cad",
   "metadata": {},
   "source": [
    "Below we present the demand distribution over a month by day of the month. We normalized the values since not all day of the month occur with the same frequency. We clearly see peaks at the 13th, 20th and 27th of the month. Interstingly those have a week interval in between, indicating that is maybe has to due with the fact that they occur more often at weekdays instead of weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orders_per_period(df_VANTAA, \"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83d3fd",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b32cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(starting_date, planning_horizon, train, forecasts, model, test = pd.DataFrame([])):\n",
    "    fig = go.Figure()\n",
    "    scanning_dates = pd.date_range(start=starting_date, periods=planning_horizon).strftime('%Y-%m-%d').tolist()\n",
    "    if test.shape[0] == 0:\n",
    "        fig.add_trace(go.Scatter(x=scanning_dates, y=forecasts, name=\"Forecast\"))\n",
    "    if test.shape[0] > 0:\n",
    "        fig.add_trace(go.Scatter(x=train[\"scanning_date\"], y=train[\"no_of_events\"], name=\"Train\"))\n",
    "        fig.add_trace(go.Scatter(x=test[\"scanning_date\"], y=test[\"no_of_events\"], name=\"Test\"))\n",
    "        fig.add_trace(go.Scatter(x=scanning_dates, y=forecasts[\"forecasts\"], name=\"Forecast\"))\n",
    "    fig.update_layout(template=\"simple_white\", font=dict(size=18), title_text=model,\n",
    "                      width=650, title_x=0.5, height=400, xaxis_title=\"scanning_date\",\n",
    "                      yaxis_title=\"no_of_events\")\n",
    "    \n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04fe36",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3cec1",
   "metadata": {},
   "source": [
    "This colormap shows that the orders correlate with the day of the week. It was also attempted to put the day of the year in a trigonometric function with a period of 7 days, for which higher correlation was found with the cosine function. The reason is that the beginning of the week is more busy than the end, and the cosine function has its peak at 0.\n",
    "\n",
    "For the linear regression model, lag data has to be created. Lags are the datapoints of x steps back in time. The best result was found by using 2 lags when estimating the model only on aggregate data of the large Vantaa sorting center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e43931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(series, lags):\n",
    "    lagged_data = pd.DataFrame()\n",
    "    for lag in range(1, lags+1):\n",
    "        lagged_data[f'lag_{lag}'] = series.shift(lag)\n",
    "    return lagged_data\n",
    "\n",
    "def linreg(df_train, planning_horizon, df_test=pd.DataFrame([])):\n",
    "    lags = 7 \n",
    "    X_train = create_lag_features(df_train['no_of_events'], lags)\n",
    "    y_train = df_train['no_of_events'][lags:]  # Target variable is the actual series shifted by the number of lags\n",
    "\n",
    "    # Add additional features\n",
    "    X_train['weekday'] = df_train['weekday']\n",
    "    X_train['weekday_cos'] = df_train['weekday_cos']\n",
    "    X_train['weekday_sin'] = df_train['weekday_sin']\n",
    "\n",
    "    # Drop rows with NaN values in X_train\n",
    "    X_train.dropna(inplace=True)\n",
    "\n",
    "    # Train the model\n",
    "    features = ['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'weekday', 'weekday_cos', 'weekday_sin'] \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[features], y_train)\n",
    "\n",
    "    # Prepare for predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Start with the last available data point\n",
    "    last_row = X_train.iloc[-1].copy()\n",
    "\n",
    "    # Recursive forecasting loop for the prediction horizon\n",
    "    for i in range(planning_horizon):\n",
    "        # Make the prediction for the current step\n",
    "        pred = model.predict(last_row[features].values.reshape(1, -1))[0]\n",
    "        predictions.append(pred)\n",
    "\n",
    "        # Update the last_row for the next prediction\n",
    "        last_row['lag_1'] = pred  # Update lag_1 with the new prediction\n",
    "\n",
    "        # Shift the lag features\n",
    "        for lag in range(2, 8):\n",
    "            last_row[f'lag_{lag}'] = last_row[f'lag_{lag - 1}']\n",
    "\n",
    "        # Update the additional features (assuming they follow a similar pattern)\n",
    "        # You may need to adjust this logic based on how your features are structured\n",
    "        last_row['weekday'] = (last_row['weekday'] + 1) % 7  # Example: Increment weekday cyclically\n",
    "        last_row['weekday_cos'] = np.cos(2 * np.pi * last_row['weekday'] / 7)\n",
    "        last_row['weekday_sin'] = np.sin(2 * np.pi * last_row['weekday'] / 7)\n",
    "\n",
    "    # Convert predictions list to a NumPy array or DataFrame for analysis\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['forecasts'])\n",
    "\n",
    "    # Set the index based on df_test if it exists\n",
    "    if df_test.shape[0] > 0:\n",
    "        predictions_df.index = df_test.index[:planning_horizon]\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8bc8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_LR(starting_date, planning_horizon, train_df, test_df = pd.DataFrame([])):\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    forecast_dict = {}\n",
    "    if test_df.shape[0] > 0:\n",
    "        daily_errors = {}\n",
    "        MSE_dict = {}\n",
    "        VSE_dict = {}\n",
    "        MAE_dict = {}\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        train_sorting_center = train_df[train_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        if test_df.shape[0] > 0:\n",
    "            test_sorting_center = test_df[test_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        output_belts = train_sorting_center['output_belt'].unique()\n",
    "\n",
    "        for output_belt in output_belts:\n",
    "            train_output_belt = train_sorting_center[train_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            if test_df.shape[0] == 0:\n",
    "                test_output_belt = pd.Dataframe([])\n",
    "            elif test_df.shape[0] > 0:\n",
    "                test_output_belt = test_sorting_center[test_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            forecasts = linreg(train_output_belt, planning_horizon, test_output_belt)  \n",
    "\n",
    "            if sorting_center_name == \"VANTAA\" and output_belt in [109]:\n",
    "                plot_forecast(starting_date, planning_horizon, train_output_belt, forecasts, \"LR\", test_output_belt)  \n",
    "\n",
    "            if test_df.shape[0] == 0: \n",
    "                if sorting_center_name not in forecast_dict:\n",
    "                    forecast_dict[sorting_center_name] = {}\n",
    "                \n",
    "                forecast_dict[sorting_center_name][output_belt] = forecasts\n",
    "                \n",
    "            elif test_df.shape[0] > 0: \n",
    "                for day in range(planning_horizon):\n",
    "                    actual = test_output_belt.iloc[day][\"no_of_events\"]\n",
    "                    forecast = forecasts.iloc[day]\n",
    "\n",
    "                    squared_difference = (actual - forecast) ** 2\n",
    "                    absolute_difference = abs(actual - forecast)\n",
    "\n",
    "                    if sorting_center_name not in daily_errors:\n",
    "                        daily_errors[sorting_center_name] = {}\n",
    "                    if day not in daily_errors[sorting_center_name]:\n",
    "                        daily_errors[sorting_center_name][day] = {}\n",
    "                        daily_errors[sorting_center_name][day][\"mse\"] = []\n",
    "                        daily_errors[sorting_center_name][day][\"mae\"] = []\n",
    "                    \n",
    "                    daily_errors[sorting_center_name][day][\"mse\"].append(squared_difference)\n",
    "                    daily_errors[sorting_center_name][day][\"mae\"].append(absolute_difference)\n",
    "\n",
    "    if test_df.shape[0] > 0: \n",
    "        for sorting_center_name in sorting_center_names:\n",
    "            mse = {}\n",
    "            mae = {}\n",
    "            for day in range(planning_horizon):\n",
    "                mse[day] = sum(daily_errors[sorting_center_name][day][\"mse\"]) / len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae[day] = sum(daily_errors[sorting_center_name][day][\"mae\"]) / len(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "\n",
    "            MSE_dict[sorting_center_name] = sum(mse.values()) / len(mse)\n",
    "            VSE_dict[sorting_center_name] = np.var(list(mse.values()), ddof=1)\n",
    "            MAE_dict[sorting_center_name] = sum(mae.values()) / len(mae)\n",
    "\n",
    "        daily_mse = {}\n",
    "        daily_mae = {}\n",
    "\n",
    "        for day in range(planning_horizon):\n",
    "            mse = 0\n",
    "            mae = 0\n",
    "            n_output_belts = 0\n",
    "            for sorting_center_name in sorting_center_names:\n",
    "                mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "                n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "            daily_mse[day] = mse / n_output_belts\n",
    "            daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "        MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "        VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "        MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "        with open(\"Results/results_LR.csv\", mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "            \n",
    "            for key in MSE_dict.keys():\n",
    "                # Extract the first value from each Series\n",
    "                mse_value = MSE_dict[key].values[0] if isinstance(MSE_dict[key], pd.Series) else MSE_dict[key]\n",
    "                vse_value = VSE_dict[key].values[0] if isinstance(VSE_dict[key], pd.Series) else VSE_dict[key]\n",
    "                mae_value = MAE_dict[key].values[0] if isinstance(MAE_dict[key], pd.Series) else MAE_dict[key]\n",
    "                \n",
    "                writer.writerow([key, mse_value, vse_value, mae_value])\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c51324",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon = 14\n",
    "\n",
    "train = df[df[\"month\"] <= 9] # Train on first 9 months\n",
    "test = df[(df[\"month\"] == 10) & (df[\"day\"] <= planning_horizon)] # Test on first two weeks of 10th month\n",
    "\n",
    "starting_date = test.iloc[0][\"scanning_date\"]\n",
    "\n",
    "start = datetime.now()\n",
    "forecast_LR(starting_date, planning_horizon, train, test)\n",
    "end = datetime.now()\n",
    "\n",
    "print(\"Running time predicting LR is: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652e540",
   "metadata": {},
   "source": [
    "### (Artificial) Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4aae3",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "Making a vector of all possible combinations, category_comb, and giving this a unique number category_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d992d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_category_combinations(df):\n",
    "    df['category_comb'] = (\n",
    "        df['output_belt'].astype(str) + '_' + \n",
    "        df['day'].astype(str) + '_' + \n",
    "        df['weekday'].astype(str) + '_' + \n",
    "        df['week_of_month'].astype(str)\n",
    "    )\n",
    "    # Encodeer de gecombineerde categorieën\n",
    "    df['category_encoded'] = df['category_comb'].astype('category').cat.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a605440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "        self.inputs = torch.tensor(self.data['category_encoded'].values, dtype=torch.long)\n",
    "        self.targets = torch.tensor(self.data['no_of_events'].values, dtype=torch.float32)\n",
    "        self.output_belt = torch.tensor(self.data['output_belt'].values, dtype=torch.long)\n",
    "        self.yearday = torch.tensor(self.data['yearday'].values, dtype=torch.long)  # Voeg yearday toe\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.output_belt[idx], self.yearday[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Functie voor het maken van de DataLoader\n",
    "def loader(train_df, test_df, batch_size=512):\n",
    "    train_loader = DataLoader(EventDataset(train_df), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(EventDataset(test_df), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a11fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_categories, embedding_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_categories, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # print(x[0])\n",
    "        x = self.embedding(x)\n",
    "        x = self.bn1(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(torch.relu(self.fc2(x)))\n",
    "        x = self.bn3(torch.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e2c7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, test_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Test Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def execute_model(model, train_loader, test_loader, criterion, optimizer, epochs=100, patience=5, min_delta=0.01):\n",
    "    train_losses, test_losses = [], []\n",
    "    mse_losses, mae_losses = [], []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, _, _ in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs).squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_mae = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, _, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.float().to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "\n",
    "                mse_loss = nn.MSELoss()(outputs, targets)\n",
    "                mae_loss = nn.L1Loss()(outputs, targets)\n",
    "                \n",
    "                total_loss += criterion(outputs, targets).item()\n",
    "                total_mse += mse_loss.item()\n",
    "                total_mae += mae_loss.item()\n",
    "\n",
    "        avg_test_loss = total_loss / len(test_loader)\n",
    "        avg_mse_loss = total_mse / len(test_loader)\n",
    "        avg_mae_loss = total_mae / len(test_loader)\n",
    "\n",
    "        test_losses.append(avg_test_loss)\n",
    "        mse_losses.append(avg_mse_loss)\n",
    "        mae_losses.append(avg_mae_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, MSE: {avg_mse_loss:.4f}, MAE: {avg_mae_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        if epoch >= patience:\n",
    "            if avg_test_loss > test_losses[-patience]:\n",
    "                early_stopping_counter += 1\n",
    "            else:\n",
    "                early_stopping_counter = 0\n",
    "        \n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f'Early stopping na {epoch + 1} epochs.')\n",
    "                break\n",
    "\n",
    "    vse = torch.var(torch.tensor(mse_losses)).item()\n",
    "    return train_losses, test_losses, mse_losses, mae_losses, vse\n",
    "\n",
    "def train_NN(train_df, test_df, planning_horizon):\n",
    "    sorting_center_names = train_df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        train_sorting_center = train_df[train_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "        test_sorting_center = test_df[test_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        train_sorting_center = encode_category_combinations(train_sorting_center)\n",
    "        test_sorting_center = encode_category_combinations(train_sorting_center)\n",
    "\n",
    "        print(f\"Training model for sorting center: {sorting_center_name}\")\n",
    "\n",
    "        num_categories = len(train_sorting_center['category_encoded'].unique()) + len(test_sorting_center['category_encoded'].unique())\n",
    "        embedding_dim = 4\n",
    "\n",
    "        model = SimpleNN(num_categories=num_categories, embedding_dim=embedding_dim).to(device)\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "        train_loader, test_loader = loader(train_sorting_center, test_sorting_center)\n",
    "        train_losses, test_losses, mse_losses, mae_losses, vse = execute_model(model, train_loader, test_loader, criterion, optimizer, epochs=200)\n",
    "        plot_losses(train_losses, test_losses)\n",
    "\n",
    "        print(f\"MSE for {sorting_center_name}: {mse_losses[-1]:.4f}, MAE: {mae_losses[-1]:.4f}, VSE: {vse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/200], Train Loss: 31.8605, Test Loss: 25.3322, MSE: 3433.6504, MAE: 25.3322, LR: 0.030000\n"
     ]
    }
   ],
   "source": [
    "planning_horizon = 14\n",
    "\n",
    "train_df = df[df[\"month\"] <= 9] # Train on first 9 months\n",
    "test_df = df[(df[\"month\"] == 10) & (df[\"day\"] <= planning_horizon)] # Test on first two weeks of 10th month\n",
    "\n",
    "starting_date = test_df.iloc[0][\"scanning_date\"]\n",
    "\n",
    "train_NN(train_df, test_df, planning_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b258d",
   "metadata": {},
   "source": [
    "#### Rural vs capital\n",
    "Model for rural sorting centers together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a00fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "belts_per_center = {}\n",
    "\n",
    "for name in loaders:\n",
    "    unique_belts = np.unique([belt for _, _, belts, _ in loaders[name]['test_loader'] for belt in belts.numpy()])\n",
    "    belts_per_center[name] = np.random.choice(unique_belts, size=3, replace=False)\n",
    "\n",
    "def plot_for_all_centers(models, loaders, belts_per_center):\n",
    "    for name, model in models.items():\n",
    "        print(f\"Plotting for Sorting Center: {name}\")\n",
    "        test_loader = loaders[name]['test_loader']\n",
    "\n",
    "        belts = belts_per_center.get(name, [])\n",
    "        if belts.size > 0:  # Pas deze regel aan\n",
    "            plot_output_belts_for_center(model, test_loader, name, belts)\n",
    "\n",
    "plot_for_all_centers(trained_models, loaders, belts_per_center=belts_per_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362a836",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da54c1",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "We need to develop a model for each output belt at every sorting center. To ensure the accuracy of our models, we will train the hyperparameters p, d an q using data from the first nine months. These hyperparameters will be saved for future forecasts.\n",
    "\n",
    "In the context of ARIMA:\n",
    "- p represents the number of lag observations included in the model (the autoregressive term).\n",
    "- d denotes the degree of differencing needed to make the time series stationary.\n",
    "- q indicates the size of the moving average window, which reflects the number of lagged forecast errors in the prediction equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567d763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(df_output_belt):\n",
    "    # Perform Box-Cox transformation to fasten the process\n",
    "    df_output_belt[\"no_of_events_boxcox\"], lam = boxcox(df_output_belt[\"no_of_events\"])\n",
    "    df_output_belt[\"no_of_events_diff\"] = df_output_belt[\"no_of_events_boxcox\"].diff()\n",
    "    df_output_belt.dropna(inplace=True)\n",
    "\n",
    "    # Tune the model\n",
    "    tuning_model = auto_arima(df_output_belt[\"no_of_events_boxcox\"], \n",
    "                               seasonal=False, \n",
    "                               stepwise=True,  \n",
    "                               suppress_warnings=True, \n",
    "                               trace=False)\n",
    "\n",
    "    \n",
    "    p, d, q = tuning_model.order\n",
    "    return (df_output_belt[\"output_belt\"].iloc[0], p, d, q)\n",
    "\n",
    "def train_ARIMA(df):\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        df_sorting_center = df[df[\"sorting_center_name\"] == sorting_center_name]\n",
    "        df_sorting_center.drop([\"sorting_center_name\"], axis=1, inplace=True)\n",
    "        \n",
    "        hyperparameterList = []\n",
    "        output_belts = df_sorting_center[\"output_belt\"].unique()\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(tune_hyperparameters)(\n",
    "            df_sorting_center[df_sorting_center[\"output_belt\"] == output_belt]) \n",
    "            for output_belt in output_belts)\n",
    "        \n",
    "        hyperparameterList.extend(results)\n",
    "\n",
    "        hyperparameter_df = pd.DataFrame(hyperparameterList, columns=[\"output belt\", \"p\", \"d\", \"q\"])\n",
    "        hyperparameter_df.to_csv(f'Data/hyperparameters ARIMA/hyperparameters_ARIMA_{sorting_center_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99101ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "start = datetime.now()\n",
    "train = df[df[\"month\"] <= 9] # Train on first 9 months\n",
    "train_ARIMA(train)\n",
    "end = datetime.now()\n",
    "print(\"Running time training ARIMA is: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb65a19",
   "metadata": {},
   "source": [
    "#### Test ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d567dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ARIMA per output belt\n",
    "def forecast_ARIMA(starting_date, planning_horizon, train_df, test_df = pd.DataFrame([])):\n",
    "    sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "\n",
    "    forecast_dict = {}\n",
    "    if test_df.shape[0] > 0:\n",
    "        daily_errors = {}\n",
    "        MSE_dict = {}\n",
    "        VSE_dict = {}\n",
    "        MAE_dict = {}\n",
    "\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        hyperparameters = pd.read_csv(\"Data/hyperparameters ARIMA/hyperparameters_ARIMA_{}.csv\".format(sorting_center_name))\n",
    "\n",
    "        train_sorting_center = train_df[train_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        if test_df.shape[0] > 0:\n",
    "            test_sorting_center = test_df[test_df[\"sorting_center_name\"] == sorting_center_name]\n",
    "\n",
    "        output_belts = train_sorting_center['output_belt'].unique()\n",
    "\n",
    "        for output_belt in output_belts:\n",
    "            train_output_belt = train_sorting_center[train_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            if test_df.shape[0] == 0:\n",
    "                test_output_belt = pd.Dataframe([])\n",
    "            elif test_df.shape[0] > 0:\n",
    "                test_output_belt = test_sorting_center[test_sorting_center[\"output_belt\"] == output_belt]\n",
    "\n",
    "            p, d, q = hyperparameters[hyperparameters.iloc[:, 0] == output_belt].iloc[0, 1:4]\n",
    "\n",
    "            arima_model = ARIMA(train_output_belt[\"no_of_events\"], order=(p, d, q)).fit()\n",
    "\n",
    "            # Forecast for the planning horizon\n",
    "            forecasts = arima_model.forecast(steps=planning_horizon)\n",
    "\n",
    "            # Assuming `predictions` is your Series with the forecast values\n",
    "            forecasts = pd.DataFrame(forecasts)  # Convert Series to DataFrame\n",
    "            forecasts.columns = ['forecasts']  # Rename the column to 'forecast\n",
    "\n",
    "            if sorting_center_name == \"VANTAA\" and output_belt in [109]:\n",
    "                plot_forecast(starting_date, planning_horizon, train_output_belt, forecasts, \"ARIMA\", test_output_belt)\n",
    "\n",
    "            if test_df.shape[0] == 0: \n",
    "                if sorting_center_name not in forecast_dict:\n",
    "                    forecast_dict[sorting_center_name] = {}\n",
    "                \n",
    "                forecast_dict[sorting_center_name][output_belt] = forecasts\n",
    "                \n",
    "            elif test_df.shape[0] > 0: \n",
    "                for day in range(planning_horizon):\n",
    "                    actual = test_output_belt.iloc[day][\"no_of_events\"]\n",
    "                    forecast = forecasts.iloc[day]\n",
    "\n",
    "                    squared_difference = (actual - forecast) ** 2\n",
    "                    absolute_difference = abs(actual - forecast)\n",
    "\n",
    "                    if sorting_center_name not in daily_errors:\n",
    "                        daily_errors[sorting_center_name] = {}\n",
    "                    if day not in daily_errors[sorting_center_name]:\n",
    "                        daily_errors[sorting_center_name][day] = {}\n",
    "                        daily_errors[sorting_center_name][day][\"mse\"] = []\n",
    "                        daily_errors[sorting_center_name][day][\"mae\"] = []\n",
    "                    \n",
    "                    daily_errors[sorting_center_name][day][\"mse\"].append(squared_difference)\n",
    "                    daily_errors[sorting_center_name][day][\"mae\"].append(absolute_difference)\n",
    "\n",
    "    if test_df.shape[0] > 0: \n",
    "        for sorting_center_name in sorting_center_names:\n",
    "            mse = {}\n",
    "            mae = {}\n",
    "            for day in range(planning_horizon):\n",
    "                mse[day] = sum(daily_errors[sorting_center_name][day][\"mse\"]) / len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae[day] = sum(daily_errors[sorting_center_name][day][\"mae\"]) / len(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "\n",
    "            MSE_dict[sorting_center_name] = sum(mse.values()) / len(mse)\n",
    "            VSE_dict[sorting_center_name] = np.var(list(mse.values()), ddof=1)\n",
    "            MAE_dict[sorting_center_name] = sum(mae.values()) / len(mae)\n",
    "\n",
    "        daily_mse = {}\n",
    "        daily_mae = {}\n",
    "\n",
    "        for day in range(planning_horizon):\n",
    "            mse = 0\n",
    "            mae = 0\n",
    "            n_output_belts = 0\n",
    "            for sorting_center_name in sorting_center_names:\n",
    "                mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "                mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "                n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "            daily_mse[day] = mse / n_output_belts\n",
    "            daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "        MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "        VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "        MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "        with open(\"Results/results_ARIMA.csv\", mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "            \n",
    "            for key in MSE_dict.keys():\n",
    "                # Extract the first value from each Series\n",
    "                mse_value = MSE_dict[key].values[0] if isinstance(MSE_dict[key], pd.Series) else MSE_dict[key]\n",
    "                vse_value = VSE_dict[key].values[0] if isinstance(VSE_dict[key], pd.Series) else VSE_dict[key]\n",
    "                mae_value = MAE_dict[key].values[0] if isinstance(MAE_dict[key], pd.Series) else MAE_dict[key]\n",
    "                \n",
    "                writer.writerow([key, mse_value, vse_value, mae_value])\n",
    "\n",
    "    return forecast_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ce856",
   "metadata": {},
   "source": [
    "### Predictions ARIMA model\n",
    "Make forecasts for all sorting centers and report the important KPIs to evaluate the performance. These are the mean squared error (MSE), variance squared error (VSE) and mean absolute error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon = 14\n",
    "\n",
    "train = df[df[\"month\"] <= 9] # Train on first 9 months\n",
    "test = df[(df[\"month\"] == 10) & (df[\"day\"] <= planning_horizon)] # Test on first two weeks of 10th month\n",
    "\n",
    "starting_date = test.iloc[0][\"scanning_date\"]\n",
    "\n",
    "start = datetime.now()\n",
    "forecast_ARIMA(starting_date, planning_horizon, train, test)\n",
    "end = datetime.now()\n",
    "\n",
    "print(\"Running time predicting ARIMA is: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b396b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_center_names = df[\"sorting_center_name\"].unique()\n",
    "MSE_dict = {}\n",
    "VSE_dict = {}\n",
    "MAE_dict = {}\n",
    "daily_errors = {}\n",
    "\n",
    "for sorting_center_name in sorting_center_names:\n",
    "    hyperparameters_df = pd.read_csv(\"Data/hyperparameters ARIMA/hyperparameters_ARIMA_{}.csv\".format(sorting_center_name))\n",
    "    hyperparameterList = [tuple(row) for row in hyperparameters_df.to_numpy()]\n",
    "    daily_errors_sorting_center, mse, mae = predict_ARIMA(df, sorting_center_name, hyperparameterList)\n",
    "\n",
    "    daily_errors[sorting_center_name] = daily_errors_sorting_center\n",
    "    MSE_dict[sorting_center_name] = sum(mse.values()) / len(mse)\n",
    "    VSE_dict[sorting_center_name] = np.var(list(mse.values()), ddof=1)\n",
    "    MAE_dict[sorting_center_name] = sum(mae.values()) / len(mae)\n",
    "    \n",
    "daily_mse = {}\n",
    "daily_mae = {}\n",
    "\n",
    "for day in range(len(daily_errors)):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    n_output_belts = 0\n",
    "    for sorting_center_name in sorting_center_names:\n",
    "        mse += sum(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "        mae += sum(daily_errors[sorting_center_name][day][\"mae\"])\n",
    "        n_output_belts += len(daily_errors[sorting_center_name][day][\"mse\"])\n",
    "    daily_mse[day] = mse / n_output_belts\n",
    "    daily_mae[day] = mae / n_output_belts\n",
    "\n",
    "MSE_dict[\"total\"] = sum(daily_mse.values()) / len(daily_mse)\n",
    "VSE_dict[\"total\"] = np.var(list(daily_mse.values()), ddof=1)\n",
    "MAE_dict[\"total\"] = sum(daily_mae.values()) / len(daily_mae)\n",
    "\n",
    "with open(\"Results/results_ARIMA.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Sorting center', 'MSE', 'VSE', 'MAE'])\n",
    "    \n",
    "    for key in MSE_dict.keys():\n",
    "        writer.writerow([key, MSE_dict[key], VSE_dict[key], MAE_dict[key]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11164e7d",
   "metadata": {},
   "source": [
    "## LSTM/GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3068bf0",
   "metadata": {},
   "source": [
    "Because of the sparse data, engineering features to improve the model on can pose a challenge. For this reason, a new method was proposed using wave decomposition. One of the ways to do this, is through CEEMDAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2d788",
   "metadata": {},
   "source": [
    "#### CEEMDAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c37506",
   "metadata": {},
   "source": [
    "CEEMDAN, or Complete Ensemble Empirical Mode Decomposition with Adaptive Noise, is a signal processing technique used to analyze non-linear and non-stationary time series data. It decomposes a signal into components known as Intrinsic Mode Functions (IMFs), which can then be used as features to train various types of Recurrent Neural Networks (RNNs), such as LSTM and GRU.\n",
    "\n",
    "CEEMDAN combines two key techniques:\n",
    "- Empirical Mode Decomposition (EMD): Breaks down signals into IMFs without assuming linearity or stationarity.\n",
    "- Ensemble Empirical Mode Decomposition (EEMD): Improves EMD by adding white noise to the signal to reduce mode mixing.\n",
    "\n",
    "By adding adaptive noise and white noise, CEEMDAN extracts purer frequencies from the signal, enhancing the precision and robustness of the decomposition.\n",
    "\n",
    "This technique shows promise for our goals of feature extraction from limited data. By decomposing the signal, we can detect patterns like seasonality and outliers directly from the data itself, without relying on biased inputs. Since CEEMDAN is often used in forecasting unpredictable time series, such as weather, wind speeds, and stock markets, we are exploring its potential for improving package prediction in our system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fee20f",
   "metadata": {},
   "source": [
    "There are multiple methods of deploying the IMF's as vector/matrix inputs:\n",
    "\n",
    "| Forecast Method     | Description                                                                                      |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------|\n",
    "| Single Method       | Use Keras model to directly forecast with vector input                                           |\n",
    "| Ensemble Method     | Use decomposition-integration Keras model to directly forecast with matrix input                 |\n",
    "| Respective Method   | Use decomposition-integration Keras model to respectively forecast each IMF with vector input    |\n",
    "| Hybrid Method       | Use the ensemble method to forecast high-frequency IMF and the respective method for other IMFs. |\n",
    "| Multiple Method     | Multiple runs of the above method                                                                |\n",
    "| Rolling Method      | Rolling run of the above method to avoid the look-ahead bias, but takes a long time              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3f5bf",
   "metadata": {},
   "source": [
    "The data being used is aggregated, because the method decomposes the signal into multiple Intrinsic Mode Functions (IMFs). The model will than be trained on each IMF. This results in long computation times for making predictions —about two minutes for GRU and LSTM models, even with plotting functions disabled. Applying this approach to each output belt is not feasible. Later, we will try to use a similar method but we will be employing parralel computing to decrease the computation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data\n",
    "adf = df.groupby(['scanning_date', 'sorting_center_name'])['no_of_events'].sum().reset_index()\n",
    "\n",
    "# Extracting day, month, weekday, week, and yearday features\n",
    "adf['day'] = adf['scanning_date'].dt.day\n",
    "adf['month'] = adf['scanning_date'].dt.month\n",
    "adf['weekday'] = adf['scanning_date'].dt.day_of_week + 1\n",
    "adf['week'] = adf['scanning_date'].dt.day_of_year // 7 + 1\n",
    "adf['yearday'] = adf['scanning_date'].dt.day_of_year\n",
    "\n",
    "# Adding sinusoidal and cosinusoidal transformations for yearday\n",
    "adf['yearday_sin'] = np.sin(adf['yearday'] / 7 * 2 * np.pi)\n",
    "adf['yearday_cos'] = np.cos(adf['yearday'] / 7 * 2 * np.pi)\n",
    "\n",
    "# Filtering data for 'VANTAA' sorting center and setting the index\n",
    "data = adf[adf['sorting_center_name'] == 'VANTAA'].set_index('scanning_date')\n",
    "data = data['no_of_events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b13b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM, 14 days prediction\n",
    "cl.statis_tests(data)\n",
    "kr = cl.keras_predictor(\n",
    "    FORECAST_HORIZONS=14,\n",
    "    FORECAST_LENGTH=14,\n",
    "    KERAS_MODEL='LSTM',\n",
    "    DECOM_MODE='CEEMDAN',\n",
    "    DAY_AHEAD=1,\n",
    "    NOR_METHOD='minmax',\n",
    "    FIT_METHOD='add',\n",
    "    REDECOM_LIST={'co-imf0': 'ovmd'}\n",
    ")\n",
    "df_result = kr.hybrid_keras_predict(data=data, show=True, plot=True, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e12656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU model, 14 days prediction using CEEMDAN decomposition and min-max normalization\n",
    "# Assumes 'cl' is a class instance with statistical testing and prediction methods\n",
    "\n",
    "# Perform statistical tests on the data\n",
    "cl.statis_tests(data)\n",
    "\n",
    "# Initialize the keras predictor with specified parameters\n",
    "kr = cl.keras_predictor(\n",
    "    FORECAST_HORIZONS=14,\n",
    "    FORECAST_LENGTH=14,\n",
    "    KERAS_MODEL='GRU',\n",
    "    DECOM_MODE='CEEMDAN',\n",
    "    DAY_AHEAD=1,\n",
    "    NOR_METHOD='minmax',\n",
    "    FIT_METHOD='add',\n",
    "    REDECOM_LIST={'co-imf0': 'ovmd'}\n",
    ")\n",
    "\n",
    "# Run the hybrid Keras prediction and store the results in df_result\n",
    "df_result = kr.hybrid_keras_predict(data=data, show=True, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289319e9",
   "metadata": {},
   "source": [
    "#### Parralel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476482d",
   "metadata": {},
   "source": [
    "We were inspired by the success of CEEMDAN (Complete Ensemble Empirical Mode Decomposition with Adaptive Noise) in improving clustering models. However, while CEEMDAN is well-suited for aggregated time-series analysis, it lacks the granularity required for predicting outputs at the level of each output belt within each sorting center. Therefore, we adapted its core decomposition philosophy to suit our specific needs, allowing us to improve the computational efficiency by enabling parallel processing across belts. This adaptation not only drew upon the strength of CEEMDAN in decomposing non-stationary signals but also introduced modifications that better accommodate the unique characteristics of our problem—specifically the need to process data for multiple output belts concurrently. Our approach ensures each output belt can be analyzed individually while maintaining computational efficiency.\n",
    "\n",
    "To address the challenge of predicting sparse time-series data in this project, we designed a predictive approach combining GRU and signal decomposition techniques. The objective of the project is to predict event counts for different conveyor belts at each sorting center. Due to data sparsity and the presence of outliers, traditional time-series methods struggle to effectively model the behavior. In this context, we chose the GRU (Gated Recurrent Unit) model as it effectively captures both short- and long-term dependencies with fewer parameters, making it more suitable for training on sparse data compared to more complex LSTM models, which may be prone to overfitting in this scenario. To enhance the model's generalization ability and extract latent patterns from the data, we applied signal decomposition techniques—using both wavelet transform and Empirical Mode Decomposition (EMD). This was done to provide clearer learning inputs for the GRU, enhancing its ability to handle the peculiarities of the dataset.\n",
    "\n",
    "In the signal processing step, the wavelet transform was utilized to decompose each conveyor belt's signal into different frequency components, thereby extracting richer information in both time and frequency domains. Empirical Mode Decomposition (EMD) was used to decompose the signal into intrinsic mode functions (IMFs), making it particularly effective at capturing nonlinear and non-stationary signal characteristics. By employing this hybrid signal decomposition strategy, the model can avoid being misled by outliers within the sparse data, extracting the most useful features for predicting event counts. We also employed data augmentation techniques by adding random shifts and introducing synthetic outliers to increase data volume, thereby enhancing the robustness and generalizability of the model. In the final setup, each sorting center's conveyor belts were modeled individually, ensuring the accuracy of the prediction to align with the unique dynamic patterns of each belt.\n",
    "\n",
    "To effectively train these models, we employed a strategy combining GRU with specific optimization techniques. The optimization process leveraged Stochastic Gradient Descent (SGD) with momentum, which helps accelerate convergence and reduce the influence of outliers during training. Learning rates were dynamically adjusted using a cosine annealing scheduler to ensure that the model finds a global optimum effectively during the latter part of training. For each sorting center's conveyor belts, we independently calculated the Mean Squared Error (MSE) and Variance of Squared Error (VSE) to assess the model's overall performance and its consistency across different time periods. With this comprehensive approach, we successfully developed custom predictive models for each sorting center, effectively addressing the challenges posed by the sparse data and achieving accurate event count predictions for each output belt.\n",
    "\n",
    "Despite this, there remain some challenges due to the inherent sparsity of the data and limitations in sampling duration. The average data volume per conveyor belt within each sorting center is relatively low, which makes it difficult for the model to capture complex dynamic features. Furthermore, the current dataset only spans one year, making it challenging to effectively capture seasonality and other cyclical patterns. Thus, while this model utilizes a variety of signal processing and optimization techniques, its predictive performance may be somewhat limited by the available data. This suggests that future improvements—such as expanding data volume and gathering multi-year data—could significantly enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69265f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Define hyperparameters\n",
    "sequence_length = 15\n",
    "hidden_channels = 16\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Create GRU input sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        Y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Wavelet-based decomposition function (GPU-friendly)\n",
    "def decompose_signal_wavelet(signal, wavelets=['db1', 'haar', 'sym5'], level=None):\n",
    "    if level is None:\n",
    "        level = min(1, int(np.log2(len(signal))))  # Dynamic level selection\n",
    "    coeffs_list = []\n",
    "    for wavelet in wavelets:\n",
    "        coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "        coeffs = [torch.tensor(c, dtype=torch.float32).to(device) for c in coeffs]\n",
    "        coeffs_list.extend(coeffs)\n",
    "    return coeffs_list\n",
    "\n",
    "# Empirical Mode Decomposition (EMD) based decomposition\n",
    "def decompose_signal_emd(signal):\n",
    "    emd = EMD()  # Initialize EMD class\n",
    "    imfs = emd.emd(signal)  # Perform EMD decomposition\n",
    "    imfs = [torch.tensor(imf, dtype=torch.float32).to(device) for imf in imfs]\n",
    "    return imfs\n",
    "\n",
    "# GRU model for time series\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Data augmentation for sparse data\n",
    "def augment_data(signal, num_augments=10, noise_level=0.02):\n",
    "    augmented_signals = [signal]\n",
    "    for _ in range(num_augments):\n",
    "        noise = noise_level * np.random.randn(len(signal))\n",
    "        shifted_signal = np.roll(signal, shift=np.random.randint(-5, 5))\n",
    "        augmented_signal = shifted_signal + noise\n",
    "        augmented_signals.append(augmented_signal)\n",
    "    return np.array(augmented_signals)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df['scanning_date'] = pd.to_datetime(df['scanning_date'])\n",
    "df = df.sort_values(by='scanning_date').reset_index(drop=True)\n",
    "\n",
    "# Train/Test split\n",
    "train_size = int(0.75 * len(df))\n",
    "test_size = int(0.05 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:train_size + test_size]\n",
    "\n",
    "# Data preprocessing\n",
    "feature_columns = ['day', 'month', 'weekday', 'week', 'yearday_sin', 'yearday_cos']\n",
    "\n",
    "# Fill missing values with median values for robustness\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "train_df[feature_columns] = train_df[feature_columns].fillna(train_df[feature_columns].median(numeric_only=True))\n",
    "test_df[feature_columns] = test_df[feature_columns].fillna(test_df[feature_columns].median(numeric_only=True))\n",
    "\n",
    "# Fit scalers with valid data only\n",
    "scaler_features = RobustScaler()\n",
    "train_features = scaler_features.fit_transform(train_df[feature_columns])\n",
    "test_features = scaler_features.transform(test_df[feature_columns])\n",
    "\n",
    "scaler_target = StandardScaler()\n",
    "train_targets = scaler_target.fit_transform(train_df['no_of_events'].values.reshape(-1, 1))\n",
    "test_targets = scaler_target.transform(test_df['no_of_events'].values.reshape(-1, 1))\n",
    "\n",
    "# Decompose each output belt's signal using Wavelet and EMD-based method\n",
    "output_belt_ids = train_df['output_belt'].unique()\n",
    "imfs_dict = {}\n",
    "for belt in output_belt_ids:\n",
    "    belt_signal = train_df[train_df['output_belt'] == belt]['no_of_events'].values\n",
    "    if len(belt_signal) == 0:\n",
    "        continue\n",
    "    augmented_signals = augment_data(belt_signal)\n",
    "    imfs = []\n",
    "    for augmented_signal in augmented_signals:\n",
    "        # Apply both Wavelet and EMD decomposition\n",
    "        wavelet_coeffs = decompose_signal_wavelet(augmented_signal)\n",
    "        emd_imfs = decompose_signal_emd(augmented_signal)\n",
    "        imfs.extend(wavelet_coeffs)\n",
    "        imfs.extend(emd_imfs)\n",
    "    imfs_dict[belt] = imfs\n",
    "\n",
    "# Assign sorting centers to belts and create models for each sorting center\n",
    "sorting_center_models = {}\n",
    "sorting_center_belts = train_df.groupby('sorting_center_name')['output_belt'].unique().to_dict()\n",
    "\n",
    "# Create GRU models for each sorting center\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    if sorting_center not in sorting_center_models:\n",
    "        model = GRUModel(input_size=1, hidden_size=hidden_channels, num_layers=2, output_size=1, dropout_rate=dropout_rate).to(device)\n",
    "        sorting_center_models[sorting_center] = model\n",
    "\n",
    "# Training loop for each sorting center model\n",
    "belt_predictions_actuals = {}\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    model = sorting_center_models[sorting_center]\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    all_X_train, all_Y_train = [], []\n",
    "    belt_losses = {}\n",
    "    for belt in belts:\n",
    "        imfs = imfs_dict.get(belt, [])\n",
    "        belt_losses[belt] = []  # Ensure initialization of belt_losses before using it\n",
    "        for imf in imfs:\n",
    "            X_train, Y_train = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "            if len(X_train) > 0:\n",
    "                all_X_train.append(X_train)\n",
    "                all_Y_train.append(Y_train)\n",
    "\n",
    "    if len(all_X_train) > 0:\n",
    "        X_train_tensor = torch.tensor(np.concatenate(all_X_train), dtype=torch.float32).unsqueeze(2)\n",
    "        Y_train_tensor = torch.tensor(np.concatenate(all_Y_train), dtype=torch.float32).unsqueeze(1)\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_Y in train_loader:\n",
    "                batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = nn.MSELoss()(outputs.view(-1), batch_Y.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Step scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Sorting Center: {sorting_center}, Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Store predictions and actuals for each belt\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for belt in belts:\n",
    "                imfs = imfs_dict.get(belt, [])\n",
    "                belt_predictions = []\n",
    "                belt_actuals = []\n",
    "\n",
    "                for imf in imfs:\n",
    "                    X_test, Y_test = create_sequences(imf.cpu().numpy(), sequence_length=sequence_length)\n",
    "                    if len(X_test) > 0:\n",
    "                        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(2).to(device)\n",
    "                        Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                        predictions = model(X_test_tensor).cpu().numpy().flatten()\n",
    "                        actuals = Y_test_tensor.cpu().numpy().flatten()\n",
    "\n",
    "                        belt_predictions.extend(predictions)\n",
    "                        belt_actuals.extend(actuals)\n",
    "\n",
    "                # Store for KPI calculation\n",
    "                belt_predictions_actuals[belt] = {'predictions': belt_predictions, 'actuals': belt_actuals}\n",
    "\n",
    "# Calculate KPIs for each sorting center\n",
    "sorting_center_metrics = {}\n",
    "for sorting_center, belts in sorting_center_belts.items():\n",
    "    daily_errors = {}\n",
    "    for belt in belts:\n",
    "        if belt not in belt_predictions_actuals:\n",
    "            continue\n",
    "        predictions = belt_predictions_actuals[belt]['predictions']\n",
    "        actuals = belt_predictions_actuals[belt]['actuals']\n",
    "        scanning_dates = test_df['scanning_date'][sequence_length:sequence_length + len(predictions)]\n",
    "\n",
    "        # Calculate squared deviation for each day\n",
    "        for date, actual, predicted in zip(scanning_dates, actuals, predictions):\n",
    "            daily_errors.setdefault(date, []).append((actual - predicted) ** 2)\n",
    "\n",
    "    # Calculate overall KPI for sorting center\n",
    "    daily_rmse = {date: np.sqrt(np.mean(errors)) for date, errors in daily_errors.items()}\n",
    "    sorting_center_metrics[sorting_center] = {'daily_rmse': daily_rmse, 'mean_rmse': np.mean(list(daily_rmse.values()))}\n",
    "\n",
    "print(sorting_center_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d838e3",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eddb0d",
   "metadata": {},
   "source": [
    "Comparison table with all benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56ac3e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Write about pro and cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cc71b",
   "metadata": {},
   "source": [
    "## Dashboard forecast (Tom)\n",
    "Standardize input + output\n",
    "Make accessible for forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62352c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training functions for all models such that the training can be saved and used for prediction every time when needed\n",
    "\n",
    "def train_model(data, model):\n",
    "    data = prepare_data(data) # Ensure that data is ready for training\n",
    "\n",
    "    if model == \"Linear Regression\":\n",
    "        #train_LR(data)\n",
    "        print(\"Training is finished\")\n",
    "    elif model == \"ARIMA\":\n",
    "        train_ARIMA(data)\n",
    "        print(\"Training is finished\")\n",
    "    elif model == \"Neural Network\":\n",
    "        #train_NN(data)\n",
    "        print(\"Training is finished\")\n",
    "    '''elif model == \"LSTM\":\n",
    "        #train_LSTM(data)\n",
    "        print(\"Training is finished\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast(start_date, planning_horizon, data, model):\n",
    "    if model == \"Linear Regression\":\n",
    "        forecast = forecast_LR(start_date, planning_horizon, data)        \n",
    "        print(\"Forecast finished\")\n",
    "    elif model == \"ARIMA\":\n",
    "        forecast = forecast_ARIMA(start_date, planning_horizon, data)\n",
    "        print(\"Forecast finished\")\n",
    "    elif model == \"Neural Network\":\n",
    "        #forecast = forecast_NN(start_date, planning_horizon)             # If data is needed for forecast, then add to the function\n",
    "        print(\"Forecast finished\")\n",
    "    '''elif model == \"LSTM\":\n",
    "        #forecast = forecast_LSTM(start_date, planning_horizon)           # If data is needed for forecast, then add to the function\n",
    "        print(\"Forecast finished\")'''\n",
    "        \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddaece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forecast(start_date, planning_horizon, model, forecast):\n",
    "    for sorting_center_name, output_belts in forecast.items():\n",
    "        # Define the file name\n",
    "        file_name = f\"Results/forecast_{model}_{sorting_center_name}.csv\"\n",
    "        \n",
    "        # Open the file for writing\n",
    "        with open(file_name, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # Construct the list of dates for the forecast period\n",
    "            dates = [(start_date + timedelta(days=day)).strftime(\"%Y-%m-%d\") for day in range(planning_horizon)]\n",
    "            \n",
    "            # Write the header row (Dates)\n",
    "            header = ['Output Belt'] + dates\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Write the forecast data\n",
    "            for output_belt, forecasts in output_belts.items():\n",
    "                row = [output_belt] + forecasts.tolist()  # Convert forecasts to list if it's a NumPy array\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(\"Forecasts successfully written to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(start_date, planning_horizon, data, model, train_indicator = False):\n",
    "    if train_indicator == True:\n",
    "        train_model(data, model)\n",
    "\n",
    "    forecast = make_forecast(start_date, planning_horizon, data, model)\n",
    "\n",
    "    save_forecast(start_date, planning_horizon, forecast)\n",
    "    #plot_forecast(start_date, planning_horizon, data, prediction, model)   # Currently written to plot one output belt, so process it in the prediction function of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "directory_path = os.getcwd() + \"\\\\Data\\\\sorting_event_volumes_2023.csv\"\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Input\n",
    "start_date = \"01-10-2023\"       # When does the planning horizon start\n",
    "planning_horizon = 14           # How long is the planning horizon\n",
    "data = df[df[\"month\"] <= 9]     # Always needed since some models need it for prediction\n",
    "model = \"ARIMA\"                 # Which model do you want to use, options are Linear Regression, ARIMA and Neural Network \n",
    "train_indicator = False         # Optional, if you want to train the model\n",
    "\n",
    "main(start_date, planning_horizon, data, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
